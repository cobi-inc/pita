[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "pita"
version = "0.0.1"
description = "A library to allow probabilistic inference time algorithms from any model"

# Core dependencies - minimal set needed for the library to work
dependencies = [
    # Core ML/Data
    "torch>=2.0.0",
    "transformers>=4.30.0",
    "numpy>=1.20.0",
    "scipy>=1.10.0",
    "pandas>=2.0.0",
    
    # Tokenization
    "tokenizers>=0.13.0",
    "sentencepiece>=0.1.99",
    "tiktoken>=0.5.0",
    
    # API/Web
    "fastapi>=0.100.0",
    "uvicorn>=0.23.0",
    "pydantic>=2.0.0",
    "starlette>=0.27.0",
    
    # HTTP/Networking
    "httpx>=0.24.0",
    "requests>=2.28.0",
    "urllib3>=2.0.0",
    "aiohttp>=3.8.0",
    
    # Utilities
    "tqdm>=4.65.0",
    "regex>=2023.0.0",
    "packaging>=23.0",
    "filelock>=3.12.0",
    "fsspec>=2023.0.0",
    "huggingface-hub>=0.16.0",
    "safetensors>=0.3.0",
    
    # Serialization
    "PyYAML>=6.0.0",
    "Jinja2>=3.1.0",
    "MarkupSafe>=2.1.0",
    
    # Documentation (can be moved to dev if needed)
    "mkdocs>=1.5.0",
    "mkdocs-material>=9.0.0",
    "mkdocstrings[python]>=0.24.0",
]

[project.optional-dependencies]
# vLLM backend dependencies
vllm = [
    "vllm>=0.4.0",
    "ray>=2.9.0",
    "redis>=4.0.0",
    "cupy-cuda12x>=13.0.0",
    "xformers>=0.0.23",
    "triton>=2.1.0",
    "outlines_core>=0.2.0",
    "lm-format-enforcer>=0.10.0",
    "xgrammar>=0.1.0",
    "compressed-tensors>=0.5.0",
    "mistral_common>=1.0.0",
    "msgspec>=0.18.0",
    "pyzmq>=25.0.0",
    "cloudpickle>=3.0.0",
]

# llama.cpp backend dependencies
llama_cpp = [
    "llama-cpp-python>=0.2.0",
    "gguf>=0.6.0",
    "diskcache>=5.6.0",
]

# Development/testing dependencies
dev = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.21.0",
]

# Install all backends and development dependencies
all = [
    # vLLM backend dependencies
    "vllm>=0.4.0",
    "ray>=2.9.0",
    "redis>=4.0.0",
    "cupy-cuda12x>=13.0.0",
    "xformers>=0.0.23",
    "triton>=2.1.0",
    "outlines_core>=0.2.0",
    "lm-format-enforcer>=0.10.0",
    "xgrammar>=0.1.0",
    "compressed-tensors>=0.5.0",
    "mistral_common>=1.0.0",
    "msgspec>=0.18.0",
    "pyzmq>=25.0.0",
    "cloudpickle>=3.0.0",

    # llama.cpp backend dependencies
    "llama-cpp-python>=0.2.0",
    "gguf>=0.6.0",
    "diskcache>=5.6.0",

    # Development/testing dependencies
    "pytest>=7.0.0",
    "pytest-asyncio>=0.21.0",
]

[tool.setuptools]
packages = ["pita"]

[tool.pytest.ini_options]
norecursedirs = ["pita/old_tests"]