{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"PITA","text":"<p>PITA (Probabilistic Inference Time Algorithms) is a library designed to consolidate and simplify the usage of probabilistic inference time algorithms with LLMs. It is built on top of existing inference frameworks and provides a unified interface for different inference backends.</p>"},{"location":"#introduction","title":"Introduction","text":"<p><code>pita</code> splits probabilistic inference time scaling methods into two categories:</p> <ul> <li>Chain Scaling: Methods that curate the multiple responses to a prompt. For example, Best-of-N creates N sequences and returns the best one based on a decision metric.</li> <li>Token Scaling: Methods that curate the tokens generated for a prompt. For example, Power Sampling generates iteratively improves the prompt through Metropolis-Hastings Sampling combined with a decision metric.</li> </ul> <p>Chain scaling methods can be combined with token scaling methods to create a hybrid scaling method. For example, Power Best-of-N creates N chains with each chains being generated with Power Sampling. (WIP) See this flow chart for specifics on how to create custom hybrid scaling methods.</p> <p>Both chain and token scaling methods have shared decision metrics. Decision metrics can be based on token probabiliites, or external graders/process reward models. </p> <p>This library can also be used to generate non-probabilistic, non-test-time scaled outputs while taking advantage of the unified interface for different inference backends. Different models run better on different engines and hardware. Develop on your CPU before deploying on your GPU. Swap between ROCm, CUDA, and CPU. <code>pita</code> provides a unified interface for the most popular inference backends while your source code remains the same.</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#sampling-methodologies","title":"Sampling Methodologies","text":"<ul> <li>Power Sampling: Leverage Metropolis-Hastings MCMC Sampling to generate diverse and high-quality outputs.</li> <li>Sequential Monte Carlo (SMC): Sequential Monte Carlo/Particle Filtering generates diverse and high-quality token sequences, parsing and extending sequences.</li> <li>Best-of-N: Generate N sequences and select the best based on decision metrics</li> <li>(WIP) Beam Search: Maintain multiple candidate sequences during generation</li> <li>Hybrid Strategies: Combine chain and token-level scaling methods</li> </ul>"},{"location":"#decision-metrics","title":"Decision Metrics","text":"<ul> <li>Log Probabilities: Standard model confidence scoring based on token probabilities</li> <li>Power Distribution: Temperature-scaled confidence metrics using logits and normalization constants</li> <li>Entropy: Model uncertainty quantification at each token position</li> <li>Likelihood Confidence: Combined metric multiplying probability by confidence (exp(-entropy))</li> <li>(WIP) Entropy Minimization Inference: Advanced entropy-based sampling</li> <li>(WIP) Process Reward Models (PRM): External graders for decision-making</li> <li>(WIP) Verifiers: External verification models for quality assessment</li> </ul>"},{"location":"#inference-backends","title":"Inference Backends","text":"<ul> <li>vLLM: High-throughput GPU inference (primary backend, fully supported)</li> <li>llama.cpp: CPU/GPU inference with GGUF model support (fully supported)</li> <li>TensorRT: NVIDIA-optimized inference (supported, requires Valkey)</li> <li>(WIP) Transformers: HuggingFace integration for flexibility (basic support)</li> <li>(WIP) DeepSpeed: Distributed inference support</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation: Set up your environment and install the library.</li> <li>Usage: Learn the basics of running inference and using the library.</li> <li>API Reference: Dive into the technical details of modules and classes.</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! Please see the repository for more details on how to contribute.</p>"},{"location":"#license","title":"License","text":"<p>PITA is dual-licensed under: - GNU Affero General Public License v3.0 or later (AGPLv3+) - for open source use - Commercial License - for proprietary use</p>"},{"location":"#for-open-source-users","title":"For Open Source Users","text":"<p>Use PITA freely under AGPLv3+. Key requirements: - Make source code available to all users (including network users) - License derivatives under AGPLv3+ - See LICENSE for full terms</p>"},{"location":"#for-commercial-users","title":"For Commercial Users","text":"<p>Use PITA in proprietary software without AGPLv3 obligations. - Contact: sales@cobi-inc.com - Contact: sales@cobi-inc.com</p>"},{"location":"#dependencies-special-cases","title":"Dependencies &amp; Special Cases","text":"<p>All dependencies use permissive licenses (MIT, BSD, Apache 2.0). See NOTICE.</p> <p>TensorRT backend (optional): Requires NVIDIA's proprietary TensorRT with separate licensing. See TENSORRT-LICENSE-NOTICE.md.</p> <p>HuggingFace models: Individual models have their own licenses - users must verify before use.</p> <p>Complete guide: See LICENSING-GUIDE.md</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use PITA in your research, please cite it as follows:</p> <pre><code>@misc{pita2026,\n  author = {COBI, Inc. Engineering Team},\n  title = {PITA: Probabilistic Inference Time Algorithms},\n  year = {2026},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/cobi-inc-MC/pita}}\n}\n</code></pre>"},{"location":"LICENSE/","title":"LICENSE","text":"<p>GNU AFFERO GENERAL PUBLIC LICENSE                        Version 3, 19 November 2007</p> <p>Copyright (C) 2007 Free Software Foundation, Inc. http://fsf.org/  Everyone is permitted to copy and distribute verbatim copies  of this license document, but changing it is not allowed.</p> <pre><code>                        Preamble\n</code></pre> <p>The GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software.</p> <p>The licenses for most software and other practical works are designed to take away your freedom to share and change the works.  By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users.</p> <p>When we speak of free software, we are referring to freedom, not price.  Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.</p> <p>Developers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software.</p> <p>A secondary benefit of defending all users' freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate.  Many developers of free software are heartened and encouraged by the resulting cooperation.  However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public.</p> <p>The GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community.  It requires the operator of a network server to provide the source code of the modified version running there to the users of that server.  Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version.</p> <p>An older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals.  This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license.</p> <p>The precise terms and conditions for copying, distribution and modification follow.</p> <pre><code>                   TERMS AND CONDITIONS\n</code></pre> <ol> <li>Definitions.</li> </ol> <p>\"This License\" refers to version 3 of the GNU Affero General Public License.</p> <p>\"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.</p> <p>\"The Program\" refers to any copyrightable work licensed under this License.  Each licensee is addressed as \"you\".  \"Licensees\" and \"recipients\" may be individuals or organizations.</p> <p>To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy.  The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work.</p> <p>A \"covered work\" means either the unmodified Program or a work based on the Program.</p> <p>To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy.  Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.</p> <p>To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies.  Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.</p> <p>An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License.  If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.</p> <ol> <li>Source Code.</li> </ol> <p>The \"source code\" for a work means the preferred form of the work for making modifications to it.  \"Object code\" means any non-source form of a work.</p> <p>A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.</p> <p>The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form.  A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.</p> <p>The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities.  However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work.  For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.</p> <p>The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.</p> <p>The Corresponding Source for a work in source code form is that same work.</p> <ol> <li>Basic Permissions.</li> </ol> <p>All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met.  This License explicitly affirms your unlimited permission to run the unmodified Program.  The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work.  This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.</p> <p>You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force.  You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright.  Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.</p> <p>Conveying under any other circumstances is permitted solely under the conditions stated below.  Sublicensing is not allowed; section 10 makes it unnecessary.</p> <ol> <li>Protecting Users' Legal Rights From Anti-Circumvention Law.</li> </ol> <p>No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.</p> <p>When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures.</p> <ol> <li>Conveying Verbatim Copies.</li> </ol> <p>You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.</p> <p>You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.</p> <ol> <li>Conveying Modified Source Versions.</li> </ol> <p>You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:</p> <pre><code>a) The work must carry prominent notices stating that you modified\nit, and giving a relevant date.\n\nb) The work must carry prominent notices stating that it is\nreleased under this License and any conditions added under section\n7.  This requirement modifies the requirement in section 4 to\n\"keep intact all notices\".\n\nc) You must license the entire work, as a whole, under this\nLicense to anyone who comes into possession of a copy.  This\nLicense will therefore apply, along with any applicable section 7\nadditional terms, to the whole of the work, and all its parts,\nregardless of how they are packaged.  This License gives no\npermission to license the work in any other way, but it does not\ninvalidate such permission if you have separately received it.\n\nd) If the work has interactive user interfaces, each must display\nAppropriate Legal Notices; however, if the Program has interactive\ninterfaces that do not display Appropriate Legal Notices, your\nwork need not make them do so.\n</code></pre> <p>A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit.  Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.</p> <ol> <li>Conveying Non-Source Forms.</li> </ol> <p>You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:</p> <pre><code>a) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by the\nCorresponding Source fixed on a durable physical medium\ncustomarily used for software interchange.\n\nb) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by a\nwritten offer, valid for at least three years and valid for as\nlong as you offer spare parts or customer support for that product\nmodel, to give anyone who possesses the object code either (1) a\ncopy of the Corresponding Source for all the software in the\nproduct that is covered by this License, on a durable physical\nmedium customarily used for software interchange, for a price no\nmore than your reasonable cost of physically performing this\nconveying of source, or (2) access to copy the\nCorresponding Source from a network server at no charge.\n\nc) Convey individual copies of the object code with a copy of the\nwritten offer to provide the Corresponding Source.  This\nalternative is allowed only occasionally and noncommercially, and\nonly if you received the object code with such an offer, in accord\nwith subsection 6b.\n\nd) Convey the object code by offering access from a designated\nplace (gratis or for a charge), and offer equivalent access to the\nCorresponding Source in the same way through the same place at no\nfurther charge.  You need not require recipients to copy the\nCorresponding Source along with the object code.  If the place to\ncopy the object code is a network server, the Corresponding Source\nmay be on a different server (operated by you or a third party)\nthat supports equivalent copying facilities, provided you maintain\nclear directions next to the object code saying where to find the\nCorresponding Source.  Regardless of what server hosts the\nCorresponding Source, you remain obligated to ensure that it is\navailable for as long as needed to satisfy these requirements.\n\ne) Convey the object code using peer-to-peer transmission, provided\nyou inform other peers where the object code and Corresponding\nSource of the work are being offered to the general public at no\ncharge under subsection 6d.\n</code></pre> <p>A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.</p> <p>A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling.  In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage.  For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product.  A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.</p> <p>\"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source.  The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.</p> <p>If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information.  But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).</p> <p>The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed.  Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.</p> <p>Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.</p> <ol> <li>Additional Terms.</li> </ol> <p>\"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law.  If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.</p> <p>When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it.  (Additional permissions may be written to require their own removal in certain cases when you modify the work.)  You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.</p> <p>Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:</p> <pre><code>a) Disclaiming warranty or limiting liability differently from the\nterms of sections 15 and 16 of this License; or\n\nb) Requiring preservation of specified reasonable legal notices or\nauthor attributions in that material or in the Appropriate Legal\nNotices displayed by works containing it; or\n\nc) Prohibiting misrepresentation of the origin of that material, or\nrequiring that modified versions of such material be marked in\nreasonable ways as different from the original version; or\n\nd) Limiting the use for publicity purposes of names of licensors or\nauthors of the material; or\n\ne) Declining to grant rights under trademark law for use of some\ntrade names, trademarks, or service marks; or\n\nf) Requiring indemnification of licensors and authors of that\nmaterial by anyone who conveys the material (or modified versions of\nit) with contractual assumptions of liability to the recipient, for\nany liability that these contractual assumptions directly impose on\nthose licensors and authors.\n</code></pre> <p>All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10.  If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term.  If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.</p> <p>If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.</p> <p>Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.</p> <ol> <li>Termination.</li> </ol> <p>You may not propagate or modify a covered work except as expressly provided under this License.  Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).</p> <p>However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.</p> <p>Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.</p> <p>Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License.  If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.</p> <ol> <li>Acceptance Not Required for Having Copies.</li> </ol> <p>You are not required to accept this License in order to receive or run a copy of the Program.  Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance.  However, nothing other than this License grants you permission to propagate or modify any covered work.  These actions infringe copyright if you do not accept this License.  Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.</p> <ol> <li>Automatic Licensing of Downstream Recipients.</li> </ol> <p>Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License.  You are not responsible for enforcing compliance by third parties with this License.</p> <p>An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations.  If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.</p> <p>You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License.  For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.</p> <ol> <li>Patents.</li> </ol> <p>A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based.  The work thus licensed is called the contributor's \"contributor version\".</p> <p>A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version.  For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.</p> <p>Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.</p> <p>In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement).  To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.</p> <p>If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients.  \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.</p> <p>If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.</p> <p>A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License.  You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.</p> <p>Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.</p> <ol> <li>No Surrender of Others' Freedom.</li> </ol> <p>If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License.  If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all.  For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.</p> <ol> <li>Remote Network Interaction; Use with the GNU General Public License.</li> </ol> <p>Notwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software.  This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph.</p> <p>Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work.  The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License.</p> <ol> <li>Revised Versions of this License.</li> </ol> <p>The Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time.  Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.</p> <p>Each version is given a distinguishing version number.  If the Program specifies that a certain numbered version of the GNU Affero General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation.  If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation.</p> <p>If the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program.</p> <p>Later license versions may give you additional or different permissions.  However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.</p> <ol> <li>Disclaimer of Warranty.</li> </ol> <p>THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.</p> <ol> <li>Limitation of Liability.</li> </ol> <p>IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.</p> <ol> <li>Interpretation of Sections 15 and 16.</li> </ol> <p>If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.</p> <pre><code>                 END OF TERMS AND CONDITIONS\n\n        How to Apply These Terms to Your New Programs\n</code></pre> <p>If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.</p> <p>To do so, attach the following notices to the program.  It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found.</p> <pre><code>&lt;one line to give the program's name and a brief idea of what it does.&gt;\nCopyright (C) &lt;year&gt;  &lt;name of author&gt;\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Affero General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Affero General Public License for more details.\n\nYou should have received a copy of the GNU Affero General Public License\nalong with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.\n</code></pre> <p>Also add information on how to contact you by electronic and paper mail.</p> <p>If your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source.  For example, if your program is a web application, its interface could display a \"Source\" link that leads users to an archive of the code.  There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements.</p> <p>You should also get your employer (if you work as a programmer) or school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see http://www.gnu.org/licenses/.</p>"},{"location":"LICENSING-GUIDE/","title":"PITA Licensing Guide","text":"<p>Complete guide to licensing for the PITA (Probabilistic Inference Time Algorithms) library.</p>"},{"location":"LICENSING-GUIDE/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Dual Licensing Model</li> <li>Open Source License (AGPLv3+)</li> <li>Commercial License</li> <li>Dependency Licenses</li> <li>TensorRT Special Case</li> <li>HuggingFace Models</li> <li>Choosing a License</li> <li>FAQ</li> </ul>"},{"location":"LICENSING-GUIDE/#overview","title":"Overview","text":"<p>PITA is dual-licensed to support both open source and commercial use cases:</p> License Type Best For Source Code Disclosure Cost AGPLv3+ Open source projects, research, evaluation Required (including for network services) Free Commercial Proprietary software, SaaS without source disclosure Not required Paid"},{"location":"LICENSING-GUIDE/#dual-licensing-model","title":"Dual Licensing Model","text":""},{"location":"LICENSING-GUIDE/#what-is-dual-licensing","title":"What is Dual Licensing?","text":"<p>Dual licensing means the same software is available under two different license options. You choose which license to use based on your needs.</p>"},{"location":"LICENSING-GUIDE/#how-it-works","title":"How It Works","text":"<ol> <li>COBI, Inc. owns the copyright to PITA</li> <li>All dependencies use permissive licenses (MIT, BSD, Apache 2.0) that allow dual licensing</li> <li>You choose between AGPLv3+ (free, open source) or Commercial (paid, proprietary)</li> <li>Your choice applies to your use of PITA - it doesn't affect others</li> </ol>"},{"location":"LICENSING-GUIDE/#legal-basis","title":"Legal Basis","text":"<p>This dual licensing model is legally sound because: - All third-party dependencies use permissive licenses compatible with both AGPLv3 and proprietary use - No copyleft dependencies (GPL/LGPL) restrict our licensing options - COBI, Inc. owns the copyright and can license under multiple terms</p>"},{"location":"LICENSING-GUIDE/#open-source-license-agplv3","title":"Open Source License (AGPLv3+)","text":""},{"location":"LICENSING-GUIDE/#what-is-agplv3","title":"What is AGPLv3?","text":"<p>The GNU Affero General Public License v3.0 is a strong copyleft license designed for network services. It's based on GPLv3 with an additional requirement for network use.</p>"},{"location":"LICENSING-GUIDE/#key-requirements","title":"Key Requirements","text":"<p>If you use PITA under AGPLv3+, you must:</p> <ol> <li>Provide source code to users, including:</li> <li>The PITA library code</li> <li>Any modifications you make</li> <li> <p>Your application code that uses PITA</p> </li> <li> <p>Network use clause (the \"Affero\" part):</p> </li> <li>If users interact with your software over a network (e.g., web API, SaaS)</li> <li>You must offer them the source code</li> <li> <p>Even if you don't distribute the software</p> </li> <li> <p>License derivatives under AGPLv3+:</p> </li> <li>Any software you create using PITA must also be AGPLv3+</li> <li> <p>You cannot create proprietary derivatives</p> </li> <li> <p>Include license notices:</p> </li> <li>Keep copyright notices intact</li> <li>Include the full AGPLv3 license text</li> </ol>"},{"location":"LICENSING-GUIDE/#when-to-use-agplv3","title":"When to Use AGPLv3+","text":"<p>AGPLv3+ is perfect for: - \u2705 Open source projects - \u2705 Academic research - \u2705 Internal tools (no external users) - \u2705 Evaluation and testing - \u2705 Contributing back to the community</p> <p>AGPLv3+ is NOT suitable for: - \u274c Proprietary SaaS products - \u274c Closed-source software - \u274c Products where you can't disclose source code - \u274c Integration into proprietary systems</p>"},{"location":"LICENSING-GUIDE/#compliance-checklist","title":"Compliance Checklist","text":"<p>To comply with AGPLv3+ when using PITA:</p> <ul> <li>[ ] Keep all copyright and license notices</li> <li>[ ] Include LICENSE file in distributions</li> <li>[ ] Make source code available to users</li> <li>[ ] For network services: provide download link for complete source code</li> <li>[ ] License your application under AGPLv3+ or compatible license</li> <li>[ ] Document any modifications you make</li> </ul>"},{"location":"LICENSING-GUIDE/#full-license-text","title":"Full License Text","text":"<p>The complete AGPLv3 license is available at: - In this repository: LICENSE - Official source: https://www.gnu.org/licenses/agpl-3.0.txt</p>"},{"location":"LICENSING-GUIDE/#commercial-license","title":"Commercial License","text":""},{"location":"LICENSING-GUIDE/#what-is-the-commercial-license","title":"What is the Commercial License?","text":"<p>A proprietary license that allows you to use PITA in closed-source software without the source code disclosure requirements of AGPLv3.</p>"},{"location":"LICENSING-GUIDE/#what-you-get","title":"What You Get","text":"<p>With a commercial license:</p> <ol> <li>No source code disclosure required</li> <li>Keep your application code proprietary</li> <li> <p>No AGPLv3 network use obligations</p> </li> <li> <p>Flexible terms</p> </li> <li>Negotiated based on your use case</li> <li> <p>Can include support and maintenance</p> </li> <li> <p>Legal certainty</p> </li> <li>Clear proprietary use rights</li> <li>No copyleft concerns</li> </ol>"},{"location":"LICENSING-GUIDE/#when-you-need-commercial-license","title":"When You Need Commercial License","text":"<p>You need a commercial license if: - \u2705 Building proprietary SaaS products - \u2705 Embedding in closed-source software - \u2705 Cannot disclose source code (IP protection, customer requirements, etc.) - \u2705 Want to avoid AGPLv3 compliance complexity</p>"},{"location":"LICENSING-GUIDE/#how-to-obtain","title":"How to Obtain","text":"<ol> <li>Contact us: sales@cobi-inc.com</li> <li>Discuss your use case: We'll determine appropriate terms</li> <li>Review agreement: Contact us for template</li> <li>Execute license: Sign agreement and pay licensing fee</li> <li>Receive license: Written authorization to use under commercial terms</li> </ol>"},{"location":"LICENSING-GUIDE/#pricing","title":"Pricing","text":"<p>Pricing is customized based on: - Number of users/deployments - Commercial vs. internal use - Support and maintenance requirements - Organization size</p> <p>Contact sales@cobi-inc.com for a quote.</p>"},{"location":"LICENSING-GUIDE/#dependency-licenses","title":"Dependency Licenses","text":"<p>PITA depends on various third-party libraries. All use permissive licenses that are compatible with both open source and commercial use.</p>"},{"location":"LICENSING-GUIDE/#license-summary","title":"License Summary","text":"License Count Compatibility MIT 5 packages \u2705 Fully compatible with dual licensing BSD 3-Clause 7 packages \u2705 Fully compatible with dual licensing Apache 2.0 7 packages \u2705 Fully compatible with dual licensing"},{"location":"LICENSING-GUIDE/#complete-list","title":"Complete List","text":"<p>See NOTICE for complete attribution information including: - All dependency names and versions - Copyright notices - License texts</p>"},{"location":"LICENSING-GUIDE/#what-this-means-for-you","title":"What This Means for You","text":"<p>Good news: All dependencies are permissively licensed, so: - \u2705 No additional copyleft obligations - \u2705 Can use in proprietary software (with commercial PITA license) - \u2705 Simple compliance - just maintain attribution notices - \u2705 No conflicts with dual licensing model</p> <p>Your obligations: - Include the NOTICE file in distributions - Maintain copyright notices - For Apache 2.0 components: document any modifications</p>"},{"location":"LICENSING-GUIDE/#tensorrt-special-case","title":"TensorRT Special Case","text":""},{"location":"LICENSING-GUIDE/#the-situation","title":"The Situation","text":"<p>The optional TensorRT backend uses NVIDIA's proprietary TensorRT library, which has separate licensing requirements.</p>"},{"location":"LICENSING-GUIDE/#nvidia-tensorrt-license","title":"NVIDIA TensorRT License","text":"<p>TensorRT is proprietary software with restrictions: - Cannot redistribute TensorRT itself - Cannot create derivative works of TensorRT - License states it cannot be \"subjected to open source licenses\" - Users must obtain TensorRT directly from NVIDIA</p>"},{"location":"LICENSING-GUIDE/#potential-agplv3-interaction","title":"Potential AGPLv3 Interaction","text":"<p>There's a theoretical question: Does using AGPLv3-licensed PITA with proprietary TensorRT create a conflict?</p> <p>Arguments it's fine (stronger position): 1. Separate components: TensorRT isn't part of PITA - users install it separately 2. Mere aggregation: AGPLv3 allows separate programs on same system 3. Optional dependency: TensorRT isn't required for PITA to function 4. Dynamic linking: No static linking or code integration 5. Industry precedent: Many AGPLv3 projects use proprietary databases, drivers, etc.</p> <p>Why we document it: - NVIDIA's license has unusually explicit restrictions - AGPLv3's network clause is stricter than standard GPL - Better to inform users than ignore the question</p>"},{"location":"LICENSING-GUIDE/#our-recommendation","title":"Our Recommendation","text":"<p>For most users: This is likely fine under \"mere aggregation\" doctrine, similar to: - AGPLv3 apps using proprietary databases (Oracle, SQL Server) - AGPLv3 code using proprietary GPU drivers - AGPLv3 software on Windows</p> <p>If you're concerned: 1. Use vLLM or llama.cpp backends instead (both open source) 2. Consult with your legal counsel 3. Obtain PITA commercial license (removes AGPLv3 question entirely)</p> <p>If using TensorRT: - Install TensorRT separately from NVIDIA - Accept NVIDIA's license terms - See TENSORRT-LICENSE-NOTICE.md for details</p>"},{"location":"LICENSING-GUIDE/#for-commercial-license-users","title":"For Commercial License Users","text":"<p>If you have a PITA commercial license: - Still need to obtain TensorRT from NVIDIA separately - Same NVIDIA license requirements apply - But no AGPLv3 interaction question</p>"},{"location":"LICENSING-GUIDE/#huggingface-models","title":"HuggingFace Models","text":""},{"location":"LICENSING-GUIDE/#the-issue","title":"The Issue","text":"<p>While the HuggingFace libraries (transformers, datasets, huggingface-hub) are Apache 2.0 licensed, individual models and datasets have their own licenses.</p>"},{"location":"LICENSING-GUIDE/#model-license-variety","title":"Model License Variety","text":"<p>Models on HuggingFace Hub may be: - \u2705 Permissively licensed (MIT, Apache 2.0, etc.) - \u26a0\ufe0f Copyleft licensed (GPL, AGPLv3, etc.) - \u26a0\ufe0f Non-commercial only - \u26a0\ufe0f Proprietary/custom licenses - \u26a0\ufe0f No clear license</p>"},{"location":"LICENSING-GUIDE/#your-responsibility","title":"Your Responsibility","text":"<p>Users must verify model licenses before use.</p> <p>PITA's license doesn't grant rights to models - those come from model authors.</p>"},{"location":"LICENSING-GUIDE/#how-to-check-model-license","title":"How to Check Model License","text":"<ol> <li>Visit model page on HuggingFace Hub</li> <li>Look for \"License\" field in model card</li> <li>Read any license files in the model repository</li> <li>Check for usage restrictions</li> </ol>"},{"location":"LICENSING-GUIDE/#recommendations","title":"Recommendations","text":"<p>For open source projects: - Verify model license is AGPLv3-compatible - Prefer permissively licensed models</p> <p>For commercial use: - Ensure model allows commercial use - Verify no non-commercial restrictions - Consider contacting model author for licensing</p> <p>Keep records: - Document which models you use - Save copies of license information - Update if you change models</p>"},{"location":"LICENSING-GUIDE/#choosing-a-license","title":"Choosing a License","text":""},{"location":"LICENSING-GUIDE/#decision-tree","title":"Decision Tree","text":"<pre><code>Are you building proprietary/closed-source software?\n\u251c\u2500 YES \u2192 Commercial License\n\u2514\u2500 NO \u2192 Continue...\n\nWill users access your software over a network (SaaS, web API)?\n\u251c\u2500 YES \u2192 Can you provide source code to users?\n\u2502   \u251c\u2500 YES \u2192 AGPLv3+ is fine\n\u2502   \u2514\u2500 NO \u2192 Commercial License\n\u2514\u2500 NO \u2192 AGPLv3+ is fine\n\nDo you plan to keep modifications proprietary?\n\u251c\u2500 YES \u2192 Commercial License\n\u2514\u2500 NO \u2192 AGPLv3+ is fine\n</code></pre>"},{"location":"LICENSING-GUIDE/#comparison-table","title":"Comparison Table","text":"Feature AGPLv3+ Commercial Cost Free Paid Source disclosure Required (including network use) Optional Derivative licensing Must be AGPLv3+ Your choice Commercial use Allowed (with source disclosure) Allowed Support Community Available (negotiable) Best for Open source, research Proprietary products"},{"location":"LICENSING-GUIDE/#still-unsure","title":"Still Unsure?","text":"<p>Contact us: sales@cobi-inc.com</p> <p>We can help determine the best licensing option for your use case.</p>"},{"location":"LICENSING-GUIDE/#faq","title":"FAQ","text":""},{"location":"LICENSING-GUIDE/#general-questions","title":"General Questions","text":"<p>Q: Can I use PITA for free? A: Yes, under the AGPLv3+ license. You must comply with AGPLv3 requirements (mainly source code disclosure).</p> <p>Q: Can I use PITA in commercial products? A: Yes, either under AGPLv3+ (with source disclosure) or with a commercial license (without source disclosure).</p> <p>Q: Can I evaluate PITA before deciding on a license? A: Yes! Use it under AGPLv3+ for evaluation. Contact us about commercial licensing when you're ready to deploy.</p>"},{"location":"LICENSING-GUIDE/#agplv3-questions","title":"AGPLv3+ Questions","text":"<p>Q: What exactly do I need to disclose under AGPLv3? A: Your complete application source code, including modifications to PITA and any code that uses PITA.</p> <p>Q: Does AGPLv3 apply if I only use PITA internally? A: If there are no external users (just employees), source disclosure isn't required. But the code is still AGPLv3-licensed.</p> <p>Q: Can I use AGPLv3 PITA in a web API? A: Yes, but you must offer source code to API users. This is the key AGPLv3 \"network use\" requirement.</p> <p>Q: What if I just use PITA as-is without modifications? A: You still need to provide your application source code to users under AGPLv3.</p>"},{"location":"LICENSING-GUIDE/#commercial-license-questions","title":"Commercial License Questions","text":"<p>Q: How much does a commercial license cost? A: Pricing is customized. Contact sales@cobi-inc.com for a quote.</p> <p>Q: Can I get support with a commercial license? A: Support and maintenance can be included in commercial license terms.</p> <p>Q: Does commercial license cover future versions? A: Terms specify which versions are covered. Typically negotiated as part of agreement.</p> <p>Q: Can I switch from AGPLv3 to commercial later? A: Yes. Obtain a commercial license and your future use will be under commercial terms.</p>"},{"location":"LICENSING-GUIDE/#dependency-questions","title":"Dependency Questions","text":"<p>Q: Do I need to worry about dependency licenses? A: All dependencies are permissively licensed. Just include the NOTICE file and maintain attributions.</p> <p>Q: Can I use PITA's permissive dependencies in my proprietary code? A: Yes (with commercial PITA license). The dependencies allow this. But PITA itself still requires appropriate licensing.</p>"},{"location":"LICENSING-GUIDE/#tensorrt-questions","title":"TensorRT Questions","text":"<p>Q: Can I use TensorRT with AGPLv3 PITA? A: Likely yes, under \"mere aggregation\" - similar to using proprietary databases. See TENSORRT-LICENSE-NOTICE.md for details. If concerned, consult legal counsel or get commercial license.</p> <p>Q: Do I need to pay NVIDIA for TensorRT? A: TensorRT has its own licensing from NVIDIA. Check NVIDIA's terms for your use case.</p> <p>Q: Does PITA commercial license include TensorRT? A: No. TensorRT must be licensed separately from NVIDIA regardless of your PITA license.</p>"},{"location":"LICENSING-GUIDE/#model-questions","title":"Model Questions","text":"<p>Q: Are HuggingFace models included in PITA's license? A: No. Models have separate licenses from their authors. You must verify model licenses independently.</p> <p>Q: Can I use any model with commercial PITA license? A: Only if the model's license allows your use case. Check each model's license.</p> <p>Q: Where do I find model license information? A: On the model's HuggingFace Hub page, in the \"License\" field and any license files in the repository.</p>"},{"location":"LICENSING-GUIDE/#contact","title":"Contact","text":"<p>For licensing questions, commercial licensing inquiries, or legal clarifications:</p> <p>Email: sales@cobi-inc.com Repository: https://github.com/cobi-inc-MC/pita Issues: https://github.com/cobi-inc-MC/pita/issues</p> <p>Disclaimer: This guide provides general information about PITA's licensing. It is not legal advice. For specific legal questions about your use case, consult with qualified legal counsel.</p> <p>Last Updated: January 2026 PITA Version: 0.0.1</p>"},{"location":"NOTICE/","title":"NOTICE","text":"<p>PITA - Probabilistic Inference Time Algorithms Library Copyright (C) 2026 COBI, Inc.</p> <p>This product includes software developed by COBI, Inc. (https://github.com/cobi-inc-MC/pita)</p> <p>================================================================================ THIRD-PARTY SOFTWARE NOTICES AND INFORMATION ================================================================================</p> <p>This software includes or depends upon the following third-party software components. The licenses for these components are included below.</p>"},{"location":"NOTICE/#apache-license-20-components","title":"Apache License 2.0 Components","text":"<p>The following components are licensed under the Apache License, Version 2.0:</p> <ol> <li> <p>Transformers    Copyright 2018- The Hugging Face team    https://github.com/huggingface/transformers    License: Apache License 2.0</p> </li> <li> <p>Datasets    Copyright 2020 The Hugging Face Team    https://github.com/huggingface/datasets    License: Apache License 2.0</p> </li> <li> <p>HuggingFace Hub    Copyright 2021 The Hugging Face Team    https://github.com/huggingface/huggingface_hub    License: Apache License 2.0</p> </li> <li> <p>vLLM    Copyright 2023 vLLM Team    https://github.com/vllm-project/vllm    License: Apache License 2.0</p> </li> <li> <p>TensorRT-LLM    Copyright (c) 2022-2024, NVIDIA CORPORATION &amp; AFFILIATES    https://github.com/NVIDIA/TensorRT-LLM    License: Apache License 2.0    Note: TensorRT-LLM depends on proprietary NVIDIA TensorRT library with    separate licensing. See docs/TENSORRT-LICENSE-NOTICE.md</p> </li> <li> <p>Valkey GLIDE (Python client)    Copyright Valkey GLIDE Project Contributors    https://github.com/valkey-io/valkey-glide    License: Apache License 2.0</p> </li> <li> <p>pytest-asyncio    Copyright (c) 2020 pytest-asyncio contributors    https://github.com/pytest-dev/pytest-asyncio    License: Apache License 2.0</p> </li> </ol> <p>Apache License 2.0 Full Text: https://www.apache.org/licenses/LICENSE-2.0</p>"},{"location":"NOTICE/#mit-license-components","title":"MIT License Components","text":"<p>The following components are licensed under the MIT License:</p> <ol> <li> <p>FastAPI    Copyright (c) 2018 Sebasti\u00e1n Ram\u00edrez    https://github.com/fastapi/fastapi    License: MIT</p> </li> <li> <p>Pydantic    Copyright (c) 2017 to present Pydantic Services Inc. and individual contributors    https://github.com/pydantic/pydantic    License: MIT</p> </li> <li> <p>pylatexenc    Copyright (c) 2014-2021, Philippe Faist    https://github.com/phfaist/pylatexenc    License: MIT</p> </li> <li> <p>llama-cpp-python    Copyright (c) 2023 Andrei Betlen    https://github.com/abetlen/llama-cpp-python    License: MIT</p> </li> <li> <p>pytest    Copyright (c) 2004 Holger Krekel and others    https://github.com/pytest-dev/pytest    License: MIT</p> </li> </ol>"},{"location":"NOTICE/#bsd-3-clause-license-components","title":"BSD 3-Clause License Components","text":"<p>The following components are licensed under the BSD 3-Clause License:</p> <ol> <li> <p>NumPy    Copyright (c) 2005-2024, NumPy Developers    https://github.com/numpy/numpy    License: BSD 3-Clause</p> </li> <li> <p>SciPy    Copyright (c) 2001-2002 Enthought, Inc. 2003-2024, SciPy Developers    https://github.com/scipy/scipy    License: BSD 3-Clause</p> </li> <li> <p>Uvicorn    Copyright \u00a9 2017-present, Encode OSS Ltd    https://github.com/encode/uvicorn    License: BSD 3-Clause</p> </li> <li> <p>pandas    Copyright (c) 2008-2011, AQR Capital Management, LLC, Lambda Foundry, Inc. and PyData Development Team    Copyright (c) 2011-2024, Open source contributors    https://github.com/pandas-dev/pandas    License: BSD 3-Clause</p> </li> <li> <p>psutil    Copyright (c) 2009, Jay Loden, Dave Daeschler, Giampaolo Rodola'    https://github.com/giampaolo/psutil    License: BSD 3-Clause</p> </li> <li> <p>SymPy    Copyright (c) 2006-2024 SymPy Development Team    https://github.com/sympy/sympy    License: BSD 3-Clause</p> </li> <li> <p>Valkey (server)    Copyright (c) 2024 The Valkey Project    https://github.com/valkey-io/valkey    License: BSD 3-Clause</p> </li> </ol> <p>================================================================================ ATTRIBUTION REQUIREMENTS ================================================================================</p> <p>For components licensed under Apache License 2.0: - This NOTICE file must be included in any distribution - Any modifications to Apache 2.0 components must be documented - Attribution must be maintained</p> <p>For components licensed under MIT and BSD: - Copyright notices must be preserved in distributions - License texts should be included</p> <p>================================================================================ ADDITIONAL NOTICES ================================================================================</p> <p>HuggingFace Models: When using this library with models from HuggingFace Hub, users are responsible for complying with individual model licenses, which may include: - GPL/AGPL licenses - Non-commercial licenses - Proprietary licenses - Other restrictions</p> <p>Always verify the license of any model before use.</p> <p>TensorRT (Optional Dependency): The optional TensorRT backend requires NVIDIA's proprietary TensorRT library, which is subject to NVIDIA's Software License Agreement. Users must obtain and accept NVIDIA's license separately. See docs/TENSORRT-LICENSE-NOTICE.md for detailed information.</p> <p>================================================================================ LICENSE INFORMATION ================================================================================</p> <p>This software (PITA) is dual-licensed: - GNU Affero General Public License v3.0 or later (AGPLv3+) for open source use - Commercial license for proprietary use</p> <p>See the LICENSE file for complete terms. For commercial licensing options, see your commercial license agreement or contact us for details. See docs/LICENSING-GUIDE.md for detailed licensing information.</p> <p>For commercial licensing inquiries: sales@cobi-inc.com</p>"},{"location":"TENSORRT-LICENSE-NOTICE/","title":"TensorRT Licensing Notice","text":"<p>Important information about using PITA's optional TensorRT backend</p>"},{"location":"TENSORRT-LICENSE-NOTICE/#overview","title":"Overview","text":"<p>PITA's TensorRT backend is an optional feature that provides inference using NVIDIA's TensorRT-LLM library. While TensorRT-LLM itself is Apache 2.0 licensed, it depends on NVIDIA's proprietary TensorRT runtime, which has separate licensing requirements.</p>"},{"location":"TENSORRT-LICENSE-NOTICE/#tensorrt-proprietary-license","title":"TensorRT Proprietary License","text":""},{"location":"TENSORRT-LICENSE-NOTICE/#nvidias-license-terms","title":"NVIDIA's License Terms","text":"<p>NVIDIA TensorRT is proprietary software licensed under NVIDIA's Software License Agreement (SLA). Key restrictions include:</p> <ul> <li>\u274c Cannot redistribute TensorRT</li> <li>\u274c Cannot create derivative works of TensorRT</li> <li>\u274c Cannot use in a way that \"subjects it to an open source license requiring source redistribution\"</li> <li>\u274c Not designed/tested for certain production business-critical systems (varies by version)</li> <li>\u2705 Users must obtain TensorRT directly from NVIDIA and accept their terms</li> </ul>"},{"location":"TENSORRT-LICENSE-NOTICE/#where-to-find-nvidias-license","title":"Where to Find NVIDIA's License","text":"<ul> <li>Official TensorRT page: https://developer.nvidia.com/tensorrt</li> <li>License agreement: https://docs.nvidia.com/deeplearning/tensorrt/latest/reference/sla.html</li> <li>You must review and accept these terms when installing TensorRT</li> </ul>"},{"location":"TENSORRT-LICENSE-NOTICE/#interaction-with-pitas-agplv3-license","title":"Interaction with PITA's AGPLv3 License","text":""},{"location":"TENSORRT-LICENSE-NOTICE/#the-theoretical-question","title":"The Theoretical Question","text":"<p>NVIDIA's license states TensorRT cannot be \"subjected to an open source license that requires the SOFTWARE to be redistributed in source code form.\" This raises a question: Is there a conflict when using AGPLv3-licensed PITA with proprietary TensorRT?</p>"},{"location":"TENSORRT-LICENSE-NOTICE/#why-this-is-likely-not-a-problem","title":"Why This Is Likely Not a Problem","text":"<p>The overwhelming legal consensus is that this combination is permissible under the \"mere aggregation\" doctrine:</p>"},{"location":"TENSORRT-LICENSE-NOTICE/#1-separate-software-components","title":"1. Separate Software Components","text":"<p>TensorRT and PITA are distinct programs: - PITA doesn't include TensorRT code - Users install TensorRT separately from NVIDIA - PITA doesn't redistribute TensorRT - They run as separate processes</p>"},{"location":"TENSORRT-LICENSE-NOTICE/#2-agplv3-mere-aggregation-exception","title":"2. AGPLv3 \"Mere Aggregation\" Exception","text":"<p>The AGPLv3 license explicitly allows \"mere aggregation\" of separate programs:</p> <p>\"A compilation of a covered work with other separate and independent works... on a storage medium... is called an 'aggregate' if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit.\"</p> <p>This means running PITA (AGPLv3) with TensorRT (proprietary) on the same system is generally considered acceptable.</p>"},{"location":"TENSORRT-LICENSE-NOTICE/#3-dynamic-linking","title":"3. Dynamic Linking","text":"<p>PITA uses TensorRT via: - Python imports (dynamic, at runtime) - No static linking - No compilation together - Clean separation at runtime</p> <p>This further supports the \"separate programs\" argument.</p>"},{"location":"TENSORRT-LICENSE-NOTICE/#4-optional-dependency","title":"4. Optional Dependency","text":"<p>TensorRT is completely optional: - PITA works without TensorRT (vLLM, llama.cpp backends available) - TensorRT support is a plugin-like feature - Core PITA functionality doesn't require TensorRT</p>"},{"location":"TENSORRT-LICENSE-NOTICE/#5-industry-precedent","title":"5. Industry Precedent","text":"<p>This situation is common and generally accepted:</p> <p>Similar Situations Considered Legal: - \u2705 AGPLv3 web applications using proprietary databases (Oracle, SQL Server, MongoDB Enterprise) - \u2705 AGPLv3 software using proprietary GPU drivers (NVIDIA CUDA drivers) - \u2705 AGPLv3 code running on proprietary operating systems (Windows, macOS) - \u2705 AGPLv3 applications calling proprietary APIs (cloud services, etc.)</p> <p>All of these involve AGPLv3 code interacting with proprietary components via dynamic linking/runtime interfaces.</p>"},{"location":"TENSORRT-LICENSE-NOTICE/#legal-opinion","title":"Legal Opinion","text":"<p>Based on: - The \"mere aggregation\" exception in AGPLv3 - Separation of components - Industry practice - Legal commentary on similar situations</p> <p>We believe this combination is legally sound.</p> <p>However, because: - NVIDIA's terms are unusually explicit about open source licenses - AG PLv3's network clause is stricter than standard GPL - Limited specific case law on this exact scenario</p> <p>We recommend consulting legal counsel if you have concerns.</p>"},{"location":"TENSORRT-LICENSE-NOTICE/#comparison-chart","title":"Comparison Chart","text":"Aspect TensorRT + AGPLv3 PITA Similar Precedent Separate installation \u2705 Yes Oracle DB + AGPLv3 app No redistribution \u2705 PITA doesn't distribute TensorRT GPU drivers + Linux (GPL) Dynamic linking \u2705 Python runtime imports System libraries + GPL apps Optional component \u2705 Can use other backends Database choice in apps Mere aggregation \u2705 Separate programs Any proprietary OS + GPL software"},{"location":"TENSORRT-LICENSE-NOTICE/#practical-guidance","title":"Practical Guidance","text":""},{"location":"TENSORRT-LICENSE-NOTICE/#for-agplv3-pita-users","title":"For AGPLv3 PITA Users","text":"<p>If you want to use the TensorRT backend with AGPLv3-licensed PITA:</p> <ol> <li>Install TensorRT separately from NVIDIA</li> <li>Don't expect PITA to bundle or provide TensorRT</li> <li>Download from NVIDIA: https://developer.nvidia.com/tensorrt</li> <li> <p>Accept NVIDIA's license agreement</p> </li> <li> <p>Document your use</p> </li> <li>Note that TensorRT is a separate component</li> <li>Include both PITA's AGPLv3 notice and reference to NVIDIA's TensorRT license</li> <li> <p>Make clear they are separate programs</p> </li> <li> <p>If providing source to users (AGPLv3 requirement)</p> </li> <li>Provide PITA source code (as required by AGPLv3)</li> <li>Do NOT include TensorRT in your source distribution</li> <li> <p>Instruct users to obtain TensorRT from NVIDIA separately</p> </li> <li> <p>If you're concerned</p> </li> <li>Consult with your legal counsel</li> <li>Consider using vLLM or llama.cpp backends (both open source)</li> <li>Consider obtaining PITA commercial license (removes AGPLv3 question)</li> </ol>"},{"location":"TENSORRT-LICENSE-NOTICE/#for-commercial-pita-license-users","title":"For Commercial PITA License Users","text":"<p>If you have a commercial PITA license:</p> <ol> <li>TensorRT licensing still separate</li> <li>You still need to obtain TensorRT from NVIDIA</li> <li>NVIDIA's license terms still apply</li> <li> <p>Commercial PITA license doesn't include TensorRT rights</p> </li> <li> <p>No AGPLv3 interaction question</p> </li> <li>Since you're not using AGPLv3 PITA license, no AGPLv3/TensorRT interaction</li> <li> <p>Only NVIDIA's license requirements apply (for TensorRT)</p> </li> <li> <p>Contact NVIDIA</p> </li> <li>For commercial TensorRT use, review NVIDIA's terms</li> <li>May need commercial TensorRT license depending on your use case</li> </ol>"},{"location":"TENSORRT-LICENSE-NOTICE/#installation-requirements","title":"Installation Requirements","text":""},{"location":"TENSORRT-LICENSE-NOTICE/#technical-requirements","title":"Technical Requirements","text":"<p>When using TensorRT with PITA:</p> <pre><code># Install PITA without TensorRT first\npip install pita\n\n# Install TensorRT separately from NVIDIA\n# (Follow NVIDIA's installation instructions)\n# Accept NVIDIA's license during installation\n\n# Then install tensorrt_llm\npip install tensorrt_llm\n</code></pre> <p>Note: PITA's <code>pita[tensorrt]</code> option lists tensorrt_llm as a dependency, but users must still: 1. Install underlying TensorRT from NVIDIA separately 2. Accept NVIDIA's license agreement 3. Ensure they comply with both licenses</p>"},{"location":"TENSORRT-LICENSE-NOTICE/#version-compatibility","title":"Version Compatibility","text":"<p>Check PITA documentation for compatible TensorRT versions: - Different versions may have different license terms - Ensure compatibility with your TensorRT-LLM version</p>"},{"location":"TENSORRT-LICENSE-NOTICE/#alternatives-to-tensorrt","title":"Alternatives to TensorRT","text":"<p>If you're concerned about TensorRT licensing or compatibility:</p>"},{"location":"TENSORRT-LICENSE-NOTICE/#vllm-backend-recommended","title":"vLLM Backend (Recommended)","text":"<ul> <li>License: Apache 2.0 (permissive, no conflicts)</li> <li>Performance: Excellent for GPU inference</li> <li>Compatibility: Works with AGPLv3 PITA seamlessly</li> <li>Installation: <code>pip install pita[vllm]</code></li> </ul>"},{"location":"TENSORRT-LICENSE-NOTICE/#llamacpp-backend","title":"llama.cpp Backend","text":"<ul> <li>License: MIT (permissive, no conflicts)</li> <li>Performance: Great for CPU inference, good for GPU</li> <li>Compatibility: Works with AGPLv3 PITA seamlessly</li> <li>Installation: <code>pip install pita[llama_cpp]</code></li> </ul> <p>Both alternatives are fully open source and have no licensing ambiguity.</p>"},{"location":"TENSORRT-LICENSE-NOTICE/#summary","title":"Summary","text":""},{"location":"TENSORRT-LICENSE-NOTICE/#bottom-line","title":"Bottom Line","text":"<p>Using TensorRT with AGPLv3 PITA is likely legally permissible under the \"mere aggregation\" doctrine, similar to many common AGPLv3 + proprietary software combinations.</p> <p>However: - TensorRT must be obtained separately from NVIDIA - Users must accept NVIDIA's license terms - If concerned, consult legal counsel or use open source backends</p>"},{"location":"TENSORRT-LICENSE-NOTICE/#decision-matrix","title":"Decision Matrix","text":"Your Situation Recommendation Research/academic use AGPLv3 PITA + TensorRT likely fine, document separation Commercial product (AGPLv3 compliant) Consult counsel if concerned, or use vLLM/llama.cpp Cannot disclose source Get commercial PITA license + obtain TensorRT from NVIDIA Want zero legal questions Use vLLM or llama.cpp backends (fully open source) Need commercial PITA Commercial license + separate TensorRT from NVIDIA"},{"location":"TENSORRT-LICENSE-NOTICE/#additional-resources","title":"Additional Resources","text":""},{"location":"TENSORRT-LICENSE-NOTICE/#nvidia-resources","title":"NVIDIA Resources","text":"<ul> <li>TensorRT Home: https://developer.nvidia.com/tensorrt</li> <li>TensorRT License: https://docs.nvidia.com/deeplearning/tensorrt/latest/reference/sla.html</li> <li>TensorRT-LLM (Apache 2.0): https://github.com/NVIDIA/TensorRT-LLM</li> </ul>"},{"location":"TENSORRT-LICENSE-NOTICE/#pita-resources","title":"PITA Resources","text":"<ul> <li>Complete licensing guide: LICENSING-GUIDE.md</li> <li>Main license: LICENSE</li> <li>Dependency notices: NOTICE</li> <li>Commercial licensing: sales@cobi-inc.com</li> </ul>"},{"location":"TENSORRT-LICENSE-NOTICE/#legal-resources","title":"Legal Resources","text":"<ul> <li>AGPLv3 full text: https://www.gnu.org/licenses/agpl-3.0.txt</li> <li>FSF GPL FAQ: https://www.gnu.org/licenses/gpl-faq.html</li> <li>\"Mere aggregation\" discussion: https://www.gnu.org/licenses/gpl-faq.html#MereAggregation</li> </ul>"},{"location":"TENSORRT-LICENSE-NOTICE/#contact","title":"Contact","text":"<p>For questions about TensorRT compatibility with PITA:</p> <p>Technical questions: Open an issue at https://github.com/cobi-inc-MC/pita/issues Licensing questions: sales@cobi-inc.com Legal concerns: Consult with qualified legal counsel</p> <p>Disclaimer: This document provides information about licensing considerations. It is not legal advice. For specific legal questions about your use case, consult with a qualified intellectual property attorney.</p> <p>Last Updated: January 2026</p>"},{"location":"automated_installation/","title":"Automated llama.cpp CUDA Installation","text":"<p>This guide explains how to use the automated setup script to install <code>pita</code> with llama.cpp and NVIDIA CUDA support on Linux systems.</p>"},{"location":"automated_installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Linux OS: This script is designed for Linux bash environments.</li> <li>NVIDIA GPU: A CUDA-capable GPU with driver installed.</li> <li>Conda: Miniconda or Anaconda installed and initialized.</li> </ul>"},{"location":"automated_installation/#usage","title":"Usage","text":"<p>You can create the script locally by saving the following content as <code>setup_llamacpp_cuda.sh</code>:</p> <pre><code>#!/bin/bash\n# Script to set up pita_llamacpp_cuda environment with CUDA support\n# This builds llama-cpp-python from source to ensure CUDA compatibility\n\nset -e  # Exit on any error\n\nENV_NAME=\"pita_llamacpp_cuda\"\n\n# 1. Create a temporary YAML for the base environment\ncat &lt;&lt;EOF &gt; llamacpp_cuda.yml\nname: \\$ENV_NAME\nchannels:\n  - defaults\n  - nvidia\n  - conda-forge\ndependencies:\n  - python=3.12\n  - pip\n  - cuda-cudart=12.4.127\n  - cuda-toolkit=12.4.1\n  - cmake\nEOF\n\necho \"==========================================\"\necho \"Setting up \\$ENV_NAME environment\"\necho \"==========================================\"\n\n# Check if environment already exists\nif conda info --envs | grep -q \"^\\$ENV_NAME \"; then\n    echo \"Environment \\$ENV_NAME already exists.\"\n    read -p \"Do you want to remove and recreate it? (y/n): \" choice\n    if [[ \"\\$choice\" == \"y\" || \"\\$choice\" == \"Y\" ]]; then\n        echo \"Removing existing environment...\"\n        conda env remove -n \"\\$ENV_NAME\" -y\n    else\n        echo \"Keeping existing environment. Will attempt to install llama-cpp-python...\"\n    fi\nfi\n\n# Create environment if it doesn't exist\nif ! conda info --envs | grep -q \"^\\$ENV_NAME \"; then\n    echo \"Creating conda environment from llamacpp_cuda.yml...\"\n    conda env create -f llamacpp_cuda.yml\nfi\n\necho \"\"\necho \"==========================================\"\necho \"Installing llama-cpp-python with CUDA\"\necho \"==========================================\"\n\n# Get the conda prefix for this environment\nCONDA_PREFIX_PATH=\\$(conda info --envs | grep \"^\\$ENV_NAME \" | awk '{print \\$NF}')\n\nif [[ -z \"\\$CONDA_PREFIX_PATH\" ]]; then\n    CONDA_PREFIX_PATH=\\$(conda info --envs | grep \"^\\$ENV_NAME$\" | awk '{print \\$NF}')\nfi\n\necho \"Environment path: \\$CONDA_PREFIX_PATH\"\n\n# Set up environment variables for CUDA build\nexport CUDACXX=\"\\$CONDA_PREFIX_PATH/bin/nvcc\"\nexport CPATH=\"\\$CONDA_PREFIX_PATH/targets/x86_64-linux/include:\\$CPATH\"\nexport LD_LIBRARY_PATH=\"\\$CONDA_PREFIX_PATH/lib:\\$LD_LIBRARY_PATH\"\nexport CMAKE_ARGS=\"-DGGML_CUDA=on -DCMAKE_CUDA_FLAGS=-allow-unsupported-compiler\"\n\necho \"CUDACXX: \\$CUDACXX\"\necho \"CMAKE_ARGS: \\$CMAKE_ARGS\"\necho \"\"\n\n# Install llama-cpp-python with CUDA support\necho \"Building and installing llama-cpp-python (this may take a few minutes)...\"\nconda run -n \"\\$ENV_NAME\" bash -c \"export CUDACXX='\\$CONDA_PREFIX_PATH/bin/nvcc'; export CPATH='\\$CONDA_PREFIX_PATH/targets/x86_64-linux/include:\\$CPATH'; export LD_LIBRARY_PATH='\\$CONDA_PREFIX_PATH/lib:\\$LD_LIBRARY_PATH'; export CMAKE_ARGS='-DGGML_CUDA=on -DCMAKE_CUDA_FLAGS=-allow-unsupported-compiler'; pip install llama-cpp-python --no-cache-dir\"\n\n# Verify installation\necho \"\"\necho \"==========================================\"\necho \"Verifying CUDA backend installation\"\necho \"==========================================\"\n\nCUDA_CHECK=\\$(conda run -n \"\\$ENV_NAME\" python -c \"\nimport os\nlib_dir = os.path.dirname(__import__('llama_cpp').__file__) + '/lib'\nlibs = os.listdir(lib_dir)\ncuda_libs = [l for l in libs if 'cuda' in l.lower()]\nif cuda_libs:\n    print('SUCCESS: CUDA backend found:', cuda_libs)\nelse:\n    print('WARNING: No CUDA backend found. Available libs:', libs)\n\")\n\necho \"\\$CUDA_CHECK\"\n\necho \"\"\necho \"==========================================\"\necho \"Setup complete!\"\necho \"==========================================\"\necho \"\"\necho \"To activate the environment:\"\necho \"  conda activate \\$ENV_NAME\"\necho \"\"\n</code></pre>"},{"location":"automated_installation/#execution-steps","title":"Execution Steps","text":"<ol> <li> <p>Save the script as <code>setup_llamacpp_cuda.sh</code>.</p> </li> <li> <p>Make the script executable:    <code>bash    chmod +x setup_llamacpp_cuda.sh</code></p> </li> <li> <p>Run the script:    <code>bash    ./setup_llamacpp_cuda.sh</code></p> </li> <li> <p>Follow the prompts:    The script will check if an environment named <code>pita_llamacpp_cuda</code> already exists and ask if you want to recreate it.</p> </li> </ol>"},{"location":"automated_installation/#what-the-script-does","title":"What the Script Does","text":"<p>The script performs the following steps automatically:</p> <ul> <li>Environment Creation: Creates a Conda environment named <code>pita_llamacpp_cuda</code> using the correct Python and dependency versions.</li> <li>CUDA Build Setup: Configures essential environment variables (<code>CUDACXX</code>, <code>CPATH</code>, <code>LD_LIBRARY_PATH</code>) to ensure <code>llama-cpp-python</code> can find your CUDA toolkit.</li> <li>Compilation: Builds <code>llama-cpp-python</code> from source with <code>-DGGML_CUDA=on</code> to enable GPU acceleration.</li> <li>Verification: Runs a small Python check to confirm that the CUDA backend (<code>libggml-cuda</code>) was correctly included in the build.</li> </ul>"},{"location":"automated_installation/#activating-the-environment","title":"Activating the Environment","text":"<p>Once the script completes successfully, activate your new environment:</p> <pre><code>conda activate pita_llamacpp_cuda\n</code></pre> <p>Then install <code>pita</code> in editable mode:</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"installation/","title":"Installation Guide","text":"<p>This guide explains how to install and set up PITA for different hardware configurations and inference engines.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Conda (Miniconda or Anaconda) for environment management</li> <li>Git for cloning the repository</li> </ul> <pre><code>git clone https://github.com/cobi-inc-MC/pita\ncd pita\n</code></pre>"},{"location":"installation/#cpu-installation","title":"CPU Installation","text":"<p>For development, testing, or systems without GPU acceleration.</p>"},{"location":"installation/#option-1-llamacpp-recommended-for-cpu","title":"Option 1: llama.cpp (Recommended for CPU)","text":"<p>Save the following as <code>pita_llama_cpp.yml</code>:</p> <pre><code>name: pita_llama_cpp\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - python=3.12\n  - pip\n  - llama-cpp-python\n  - pytest\n</code></pre> <p>Then run:</p> <pre><code>conda env create -f pita_llama_cpp.yml\nconda activate pita_llama_cpp\n\n# Install pita in editable mode\npip install -e .\n</code></pre>"},{"location":"installation/#option-2-windows-cpu-llamacpp","title":"Option 2: Windows CPU (llama.cpp)","text":"<p>For Windows users without a dedicated GPU.</p> <p>Save the following as <code>llamacpp_windows_cpu.yml</code>:</p> <pre><code>name: pita_llamacpp_windows_cpu\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - python=3.11\n  - pip\n  - llama-cpp-python\n  - pytest\n  - pytest-asyncio\n</code></pre> <p>Then run:</p> <pre><code>conda env create -f llamacpp_windows_cpu.yml\nconda activate pita_llamacpp_windows_cpu\n\n# Install pita (ensure you are in the root directory)\npip install -e \".[llama_cpp]\"\n</code></pre>"},{"location":"installation/#option-3-manual-setup","title":"Option 3: Manual Setup","text":"<pre><code>conda create -n pita_cpu python=3.12 -y\nconda activate pita_cpu\n\npip install llama-cpp-python\npip install -e .\npip install pytest  # For testing\n</code></pre>"},{"location":"installation/#verify-installation","title":"Verify Installation","text":"<pre><code>python -c \"import pita; import llama_cpp; print('CPU installation successful!')\"\n</code></pre>"},{"location":"installation/#nvidia-cuda-installation","title":"NVIDIA CUDA Installation","text":"<p>For systems with NVIDIA GPUs. Choose your preferred inference engine:</p>"},{"location":"installation/#option-a1-llamacpp-with-cuda-using-scripts-recommended","title":"Option A.1: llama.cpp with CUDA using scripts (Recommended)","text":"<p>For a streamlined installation on Linux, we provide an automated setup script that handles environment creation and CUDA compilation for you.</p> <p>See the Automated Installation Guide for detailed instructions on using the <code>setup_llamacpp_cuda.sh</code> script.</p>"},{"location":"installation/#option-a2-llamacpp-with-cuda-manual","title":"Option A.2: llama.cpp with CUDA (Manual)","text":"<p>Best for: Smaller models, lower memory usage, flexible quantization options.</p> <p>Save the following as <code>llamacpp_cuda.yml</code>:</p> <pre><code>name: pita_llamacpp_cuda\nchannels:\n  - defaults\n  - nvidia\n  - conda-forge\ndependencies:\n  - python=3.12\n  - pip\n  - cuda-cudart=12.4.127\n  - cuda-toolkit=12.4.1\n  - cmake\n</code></pre> <p>Then run the setup:</p> <pre><code># 1. Create environment\nconda env create -f llamacpp_cuda.yml\nconda activate pita_llamacpp_cuda\n\n# 2. Set up CUDA build environment\nexport CUDACXX=$CONDA_PREFIX/bin/nvcc\nexport CPATH=$CONDA_PREFIX/targets/x86_64-linux/include:$CPATH\nexport LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH\n\n# 3. Build llama-cpp-python with CUDA support\nCMAKE_ARGS=\"-DGGML_CUDA=on -DCMAKE_CUDA_FLAGS=-allow-unsupported-compiler\" \\\n  pip install llama-cpp-python --no-cache-dir\n\n# 4. Install pita\npip install -e .\n</code></pre>"},{"location":"installation/#verify-cuda-backend","title":"Verify CUDA Backend","text":"<pre><code>python -c \"\nimport os\nlib_dir = os.path.dirname(__import__('llama_cpp').__file__) + '/lib'\nlibs = os.listdir(lib_dir)\nassert 'libggml-cuda.so' in libs, 'CUDA backend not found!'\nprint('CUDA backend installed:', [l for l in libs if 'cuda' in l])\n\"\n</code></pre>"},{"location":"installation/#option-b-vllm-with-cuda","title":"Option B: vLLM with CUDA","text":"<p>Best for: Large models, high throughput, production deployments.</p> <p>Save the following as <code>vllm_cuda.yml</code>:</p> <pre><code>name: pita_vllm_cuda\nchannels:\n  - defaults\n  - nvidia\n  - conda-forge\ndependencies:\n  - python=3.12\n  - pip\n  - cuda-toolkit=12.8\n  - cxx-compiler\n  - valkey-server\n  - pip:\n    - vllm==0.11.0\n    - pandas==2.3.3\n    - datasets==4.3.0\n    - regex==2025.9.18\n</code></pre> <p>Then run:</p> <pre><code># Create environment with vLLM and CUDA 12.8\nconda env create -f vllm_cuda.yml\nconda activate pita_vllm_cuda\n\n# Install pita\npip install -e .\n</code></pre>"},{"location":"installation/#vllm-requirements","title":"vLLM Requirements","text":"<ul> <li>NVIDIA GPU with compute capability 7.0+ (Volta, Turing, Ampere, Ada, Hopper)</li> <li>CUDA 12.x driver installed on host system</li> <li>Sufficient GPU memory for your target model</li> </ul>"},{"location":"installation/#verify-vllm-installation","title":"Verify vLLM Installation","text":"<pre><code>python -c \"import vllm; print(f'vLLM {vllm.__version__} installed successfully')\"\n</code></pre>"},{"location":"installation/#option-c-tensorrt-llm-with-cuda","title":"Option C: TensorRT-LLM with CUDA","text":"<p>Best for: Maximum performance on NVIDIA hardware, production deployments.</p> <p>Save the following as <code>tensorrt_cuda.yml</code>:</p> <pre><code>name: pita_tensorrt_cuda\nchannels:\n  - defaults\n  - nvidia\n  - conda-forge\ndependencies:\n  - python=3.10\n  - pip\n  - cxx-compiler\n  - onnx&lt;1.16.0\n  - mpi4py\n  - openmpi\n  - pytest\n  - pip:\n    - --extra-index-url https://pypi.nvidia.com/\n    - tensorrt_llm\n    - torch\n    - transformers\n    - numpy\n    - scipy\n    - pandas\n    - regex\n    - pydantic\n    - fastapi\n    - valkey\n    - valkey-server\n    - uvicorn\n</code></pre> <p>Then run:</p> <pre><code># 1. Create environment\nconda env create -f tensorrt_cuda.yml\nconda activate pita_tensorrt_cuda\n\n# 2. Install pita in editable mode\npip install -e .\n</code></pre>"},{"location":"installation/#verify-tensorrt-llm-installation","title":"Verify TensorRT-LLM Installation","text":"<pre><code>python -c \"import tensorrt_llm; print(f'TensorRT-LLM {tensorrt_llm.__version__} installed successfully')\"\n</code></pre>"},{"location":"installation/#choosing-an-inference-engine","title":"Choosing an Inference Engine","text":"Feature llama.cpp vLLM TensorRT-LLM Best for Experimentation, quantized models Production, high throughput Maximum performance, production Memory usage Lower (supports aggressive quantization) Higher High (optimized for performance) Model formats GGUF HuggingFace, GPTQ, AWQ TensorRT engines (built from HF/ONNX) Batch processing Limited Excellent Excellent Setup complexity Simple Moderate Moderate/High"},{"location":"installation/#running-tests","title":"Running Tests","text":"<p>After installation, verify everything works:</p> <pre><code># Run the test suite\npytest tests/ -v\n\n# Run specific backend tests\npytest tests/inference/ -v -k \"llama\"     # llama.cpp tests\npytest tests/inference/ -v -k \"vllm\"      # vLLM tests\npytest tests/inference/ -v -k \"tensorrt\"  # TensorRT-LLM tests\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#llamacpp-cuda-build-errors","title":"llama.cpp CUDA Build Errors","text":"<p>\"unsupported GNU version\": Add the compiler compatibility flag:</p> <pre><code>CMAKE_ARGS=\"-DGGML_CUDA=on -DCMAKE_CUDA_FLAGS=-allow-unsupported-compiler\" pip install llama-cpp-python\n</code></pre> <p>\"cuda_runtime.h: No such file\": Set the CUDA headers path:</p> <pre><code>export CPATH=$CONDA_PREFIX/targets/x86_64-linux/include:$CPATH\n</code></pre>"},{"location":"installation/#vllm-import-errors","title":"vLLM Import Errors","text":"<p>GPU not detected: Ensure NVIDIA drivers are installed:</p> <pre><code>nvidia-smi  # Should show your GPU\n</code></pre>"},{"location":"installation/#tensorrt-llm-issues","title":"TensorRT-LLM Issues","text":"<p>\"AttributeError: module 'onnx.helper' has no attribute 'float32_to_bfloat16'\": Ensure you are using <code>onnx&lt;1.16.0</code>.</p> <p>\"ImportError: libmpi.so.40\": Ensure <code>openmpi</code> is installed via conda (<code>conda list openmpi</code>).</p>"},{"location":"installation/#general-issues","title":"General Issues","text":"<p>Environment conflicts: Create a fresh environment:</p> <pre><code>conda env remove -n &lt;env_name&gt; -y\nconda env create -f &lt;environment_file.yml&gt;\n</code></pre>"},{"location":"installation/#platform-support-matrix","title":"Platform Support Matrix","text":"Platform llama.cpp vLLM TensorRT-LLM Status Linux + NVIDIA CUDA \u2705 \u2705 \u2705 Fully supported Linux + CPU \u2705 \u274c \u274c llama.cpp only macOS + Apple Silicon \ud83d\udd04 \ud83d\udd04 \u274c In development Linux + AMD ROCm \ud83d\udd04 \ud83d\udd04 \u274c In development <p>\u2705 = Supported | \ud83d\udd04 = In development | \u274c = Not supported</p>"},{"location":"usage/","title":"Usage Guide","text":"<p>This guide provides a step-by-step guide to using the <code>pita</code> library.</p>"},{"location":"usage/#step-1-choose-an-inference-backend","title":"Step 1: Choose an Inference Backend","text":"<p>The core functionality of <code>pita</code> revolves around inference engines and sampling strategies. First choose an inference backend that you would like to use:</p> <ul> <li>vLLM: A GPU/Accelerated focused platform that leverages pyTorch for inference. Can be used with multi-GPU setups. Limited under-the-hood customization.</li> <li>llama.cpp: A general purpose backed for both CPUs and GPUs. Written in C++.</li> <li>transformers: A general purpose backed for both CPUs and GPUs. Not optimized for inference, but exposes many under-the-hood customization options.</li> <li>TensorRT: Nvidia specific platform that is optimized for inference. Can only be used with select GPUs.</li> <li>DeepSpeed: TODO</li> </ul>"},{"location":"usage/#step-2-choose-programmatic-or-api-serving-modes","title":"Step 2: Choose Programmatic or API Serving Modes","text":"<p><code>pita</code> can be used in two different modes:</p> <ul> <li>Programmatic: Use <code>pita</code> as a library to run offline inference and sampling strategies.</li> <li>API: Use <code>pita</code> as a server with limited customization options, but an openAI API compatible endpoint.</li> </ul>"},{"location":"usage/#step-3a-choose-a-sampling-strategy","title":"Step 3.A: Choose a Sampling Strategy","text":"<p><code>pita</code> provides several sampling strategies to generate diverse and high-quality outputs. Choose the strategy that best suits your needs:</p> <ul> <li>Power Sampling: Leverage Metropolis-Hastings MCMC Sampling to generate diverse and high-quality outputs.</li> <li>Sequential Monte Carlo/Particle Filtering: Sequential Monte Carlo/Particle Filtering generates diverse and high-quality token sequences, parsing and extending sequences.</li> <li>Best-of-N: Select the best N outputs from a set of candidate sequences.</li> <li>Beam Search: </li> <li>Combination of Strategies: Combine multiple strategies together to increase the reasoning capabilites of a model.</li> </ul>"},{"location":"usage/#step-3b-choose-a-token-metric","title":"Step 3.B: Choose a Token Metric","text":"<p><code>pita</code> provides several token metrics to evaluate the quality of generated outputs. Choose the metric that best suits your needs:</p> <ul> <li>Log Probability: Decide based on the log probability of the generated tokens with regular </li> <li>Power Sampling: </li> <li>Entropy: </li> </ul>"},{"location":"usage/#running-from-command-line","title":"Running from Command Line","text":"<p>The <code>pita</code> command provides a CLI interface for serving the API:</p> <pre><code># Start with default settings\npita serve\n\n# Customize with options\npita serve --model Qwen/Qwen2.5-0.5B-Instruct --engine vllm --port 8001\n\n# Use short flags\npita serve -m ./model.gguf -e llama_cpp -p 8080\n\n# View all options\npita serve --help\n</code></pre> <p>Available options: - <code>--model, -m</code>: Model name or path (default: <code>Qwen/Qwen2.5-0.5B-Instruct</code>) - <code>--engine, -e</code>: Inference engine (<code>vllm</code> or <code>llama_cpp</code>, default: <code>vllm</code>) - <code>--tokenizer, -t</code>: Tokenizer path (optional, defaults to model path) - <code>--port, -p</code>: Server port (default: <code>8001</code>) - <code>--host, -h</code>: Host address (default: <code>0.0.0.0</code>)</p> <p>All options can also be set via environment variables (<code>PITA_MODEL</code>, <code>PITA_ENGINE</code>, etc.).</p>"},{"location":"usage/#using-in-python-code","title":"Using in Python Code","text":"<p>You can import and use <code>pita</code> components directly in your Python scripts.</p> <pre><code>from pita.inference.LLM_backend import AutoregressiveSampler\n\n# Initialize the sampler\nsampler = AutoregressiveSampler(\n    engine=\"vllm\",\n    model=\"facebook/opt-125m\",\n    logits_processor=True,\n    max_probs=100\n)\n\n# Configure sampling parameters\nsampler.sampling_params.max_tokens = 50\nsampler.sampling_params.temperature = 1.0\n\n# Generate text\ncontext = \"What is the capital of France?\"\noutput = sampler.sample(context)\ngenerated_text = sampler.tokenizer.decode(output.output_ids)\nprint(generated_text)\n\n# Further usage with advanced sampling strategies (see examples)\n</code></pre> <p>Refer to the API Reference for detailed documentation on each module.</p>"},{"location":"api/api_template/","title":"Template","text":""},{"location":"api/api_template/#pita.api.api_template.ChatCompletionChoice","title":"<code>ChatCompletionChoice</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single completion choice from the model.</p> <p>Attributes:</p> Name Type Description <code>index</code> <code>int</code> <p>The index of this choice in the list of choices.</p> <code>message</code> <code>ChatCompletionMessage</code> <p>The generated message for this choice.</p> <code>finish_reason</code> <code>str</code> <p>The reason why the model stopped generating.</p> Source code in <code>pita/api/api_template.py</code> <pre><code>class ChatCompletionChoice(BaseModel):\n    \"\"\"\n    A single completion choice from the model.\n\n    Attributes:\n        index: The index of this choice in the list of choices.\n        message: The generated message for this choice.\n        finish_reason: The reason why the model stopped generating.\n    \"\"\"\n    index: int\n    message: ChatCompletionMessage\n    finish_reason: str = Field(..., alias=\"finish_reason\")\n</code></pre>"},{"location":"api/api_template/#pita.api.api_template.ChatCompletionMessage","title":"<code>ChatCompletionMessage</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single message in a chat conversation.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>ChatMessageRole</code> <p>The role of the message sender (system, user, or assistant).</p> <code>content</code> <code>str</code> <p>The text content of the message.</p> <code>name</code> <code>Optional[str]</code> <p>Optional name of the message author.</p> Source code in <code>pita/api/api_template.py</code> <pre><code>class ChatCompletionMessage(BaseModel):\n    \"\"\"\n    A single message in a chat conversation.\n\n    Attributes:\n        role: The role of the message sender (system, user, or assistant).\n        content: The text content of the message.\n        name: Optional name of the message author.\n    \"\"\"\n    role: ChatMessageRole\n    content: str\n    name: Optional[str] = Field(None, alias=\"name\")\n</code></pre>"},{"location":"api/api_template/#pita.api.api_template.ChatCompletionRequest","title":"<code>ChatCompletionRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request model for chat completion API endpoint.</p> <p>This model defines all parameters that can be sent when requesting a chat completion, following the OpenAI-compatible API format.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>Name of the model to use for completion.</p> <code>messages</code> <code>List[ChatCompletionMessage]</code> <p>List of messages in chat format (conversation history).</p> <code>max_tokens</code> <code>Optional[int]</code> <p>Maximum number of new tokens to generate.</p> <code>temperature</code> <code>Optional[float]</code> <p>Sampling temperature. Higher values make output more random.</p> <code>top_p</code> <code>Optional[float]</code> <p>Nucleus sampling parameter.</p> <code>n</code> <code>Optional[int]</code> <p>Number of completions to generate for each prompt.</p> <code>stream</code> <code>Optional[bool]</code> <p>Whether to stream responses incrementally.</p> <code>stop</code> <code>Optional[List[str]]</code> <p>List of sequences where the API will stop generating.</p> <code>presence_penalty</code> <code>Optional[float]</code> <p>Penalty for tokens based on their presence in the text so far.</p> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Penalty for tokens based on their frequency in the text so far.</p> <code>logit_bias</code> <code>Optional[Dict[str, int]]</code> <p>Map of token IDs to bias values.</p> <code>user</code> <code>Optional[str]</code> <p>Optional unique identifier for the end-user.</p> Source code in <code>pita/api/api_template.py</code> <pre><code>class ChatCompletionRequest(BaseModel):\n    \"\"\"\n    Request model for chat completion API endpoint.\n\n    This model defines all parameters that can be sent when requesting a chat completion,\n    following the OpenAI-compatible API format.\n\n    Attributes:\n        model: Name of the model to use for completion.\n        messages: List of messages in chat format (conversation history).\n        max_tokens: Maximum number of new tokens to generate.\n        temperature: Sampling temperature. Higher values make output more random.\n        top_p: Nucleus sampling parameter.\n        n: Number of completions to generate for each prompt.\n        stream: Whether to stream responses incrementally.\n        stop: List of sequences where the API will stop generating.\n        presence_penalty: Penalty for tokens based on their presence in the text so far.\n        frequency_penalty: Penalty for tokens based on their frequency in the text so far.\n        logit_bias: Map of token IDs to bias values.\n        user: Optional unique identifier for the end-user.\n    \"\"\"\n    model: str\n    messages: List[ChatCompletionMessage]\n    max_tokens: Optional[int] = Field(None, alias=\"max_tokens\")\n    temperature: Optional[float] = Field(None, alias=\"temperature\")\n    top_p: Optional[float] = Field(None, alias=\"top_p\")\n    n: Optional[int] = Field(None, alias=\"n\")\n    stream: Optional[bool] = Field(None, alias=\"stream\")\n    stop: Optional[List[str]] = Field(None, alias=\"stop\")\n    presence_penalty: Optional[float] = Field(None, alias=\"presence_penalty\")\n    frequency_penalty: Optional[float] = Field(None, alias=\"frequency_penalty\")\n    logit_bias: Optional[Dict[str, int]] = Field(None, alias=\"logit_bias\")\n    user: Optional[str] = Field(None, alias=\"user\")\n</code></pre>"},{"location":"api/api_template/#pita.api.api_template.ChatCompletionResponse","title":"<code>ChatCompletionResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response model for chat completion API endpoint.</p> <p>This model defines the structure of the response returned by the chat completion endpoint, following the OpenAI-compatible API format.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for this completion.</p> <code>object</code> <code>str</code> <p>Object type (always \"chat.completion\" for non-streaming).</p> <code>created</code> <code>int</code> <p>Unix timestamp of when the completion was created.</p> <code>model</code> <code>str</code> <p>The model used for this completion.</p> <code>choices</code> <code>List[ChatCompletionChoice]</code> <p>List of completion choices (typically one unless n &gt; 1 in request).</p> <code>usage</code> <code>Usage</code> <p>Token usage statistics for this request.</p> Source code in <code>pita/api/api_template.py</code> <pre><code>class ChatCompletionResponse(BaseModel):\n    \"\"\"\n    Response model for chat completion API endpoint.\n\n    This model defines the structure of the response returned by the chat completion endpoint,\n    following the OpenAI-compatible API format.\n\n    Attributes:\n        id: Unique identifier for this completion.\n        object: Object type (always \"chat.completion\" for non-streaming).\n        created: Unix timestamp of when the completion was created.\n        model: The model used for this completion.\n        choices: List of completion choices (typically one unless n &gt; 1 in request).\n        usage: Token usage statistics for this request.\n    \"\"\"\n    id: str = Field(..., alias=\"id\")\n    object: str = Field(..., alias=\"object\")\n    created: int = Field(..., alias=\"created\")\n    model: str = Field(..., alias=\"model\")\n    choices: List[ChatCompletionChoice]\n    usage: Usage\n</code></pre>"},{"location":"api/api_template/#pita.api.api_template.ChatMessageRole","title":"<code>ChatMessageRole</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of possible roles for chat messages.</p> Values <p>System: System message role for instructions. User: User message role for user inputs. Assistant: Assistant message role for AI responses.</p> Source code in <code>pita/api/api_template.py</code> <pre><code>class ChatMessageRole(str, Enum):\n    \"\"\"\n    Enumeration of possible roles for chat messages.\n\n    Values:\n        System: System message role for instructions.\n        User: User message role for user inputs.\n        Assistant: Assistant message role for AI responses.\n    \"\"\"\n    System = \"system\"\n    User = \"user\"\n    Assistant = \"assistant\"\n</code></pre>"},{"location":"api/api_template/#pita.api.api_template.Usage","title":"<code>Usage</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Token usage statistics for API requests and responses.</p> <p>Attributes:</p> Name Type Description <code>prompt_tokens</code> <code>int</code> <p>Number of tokens in the prompt/input.</p> <code>completion_tokens</code> <code>int</code> <p>Number of tokens in the completion/output.</p> <code>total_tokens</code> <code>int</code> <p>Total number of tokens (prompt + completion).</p> Source code in <code>pita/api/api_template.py</code> <pre><code>class Usage(BaseModel):\n    \"\"\"\n    Token usage statistics for API requests and responses.\n\n    Attributes:\n        prompt_tokens: Number of tokens in the prompt/input.\n        completion_tokens: Number of tokens in the completion/output.\n        total_tokens: Total number of tokens (prompt + completion).\n    \"\"\"\n    prompt_tokens: int\n    completion_tokens: int\n    total_tokens: int\n</code></pre>"},{"location":"api/serve/","title":"Serve","text":""},{"location":"api/serve/#pita.api.serve.run_server","title":"<code>run_server()</code>","text":"<p>Run the PITA API server using command-line arguments.</p> This function provides backward compatibility for running the server via <p>python -m pita.api.serve</p> <p>For programmatic use, prefer start_server() instead.</p> Source code in <code>pita/api/serve.py</code> <pre><code>def run_server():\n    \"\"\"\n    Run the PITA API server using command-line arguments.\n\n    This function provides backward compatibility for running the server via:\n        python -m pita.api.serve\n\n    For programmatic use, prefer start_server() instead.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"PITA API Server\")\n    default_config = get_default_config()\n    parser.add_argument(\"--model\", type=str, default=default_config[\"model\"], help=\"Model name or path\")\n    parser.add_argument(\"--engine\", type=str, default=default_config[\"engine\"], choices=[\"vllm\", \"llama_cpp\"], help=\"Inference engine\")\n    parser.add_argument(\"--tokenizer\", type=str, default=default_config[\"tokenizer\"], help=\"Tokenizer path (optional)\")\n    parser.add_argument(\"--port\", type=int, default=default_config[\"port\"], help=\"Port number\")\n    parser.add_argument(\"--host\", type=str, default=default_config[\"host\"], help=\"Host address\")\n\n    args = parser.parse_args()\n\n    start_server(\n        model=args.model,\n        engine=args.engine,\n        tokenizer=args.tokenizer,\n        port=args.port,\n        host=args.host\n    )\n</code></pre>"},{"location":"api/serve/#pita.api.serve.start_server","title":"<code>start_server(model: Optional[str] = None, engine: Optional[str] = None, tokenizer: Optional[str] = None, port: Optional[int] = None, host: Optional[str] = None)</code>","text":"<p>Start the PITA API server with the specified configuration.</p> <p>This function is the main entry point for starting the server programmatically or via the CLI. Parameters default to environment variables if not specified.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Optional[str]</code> <p>Model name or path (default: PITA_MODEL env or 'Qwen/Qwen2.5-0.5B-Instruct')</p> <code>None</code> <code>engine</code> <code>Optional[str]</code> <p>Inference engine - 'vllm' or 'llama_cpp' (default: PITA_ENGINE env or 'vllm')</p> <code>None</code> <code>tokenizer</code> <code>Optional[str]</code> <p>Tokenizer path (default: PITA_TOKENIZER env or None)</p> <code>None</code> <code>port</code> <code>Optional[int]</code> <p>Port number (default: PITA_PORT env or 8001)</p> <code>None</code> <code>host</code> <code>Optional[str]</code> <p>Host address (default: PITA_HOST env or '0.0.0.0')</p> <code>None</code> Source code in <code>pita/api/serve.py</code> <pre><code>def start_server(\n    model: Optional[str] = None,\n    engine: Optional[str] = None,\n    tokenizer: Optional[str] = None,\n    port: Optional[int] = None,\n    host: Optional[str] = None\n):\n    \"\"\"\n    Start the PITA API server with the specified configuration.\n\n    This function is the main entry point for starting the server programmatically\n    or via the CLI. Parameters default to environment variables if not specified.\n\n    Args:\n        model: Model name or path (default: PITA_MODEL env or 'Qwen/Qwen2.5-0.5B-Instruct')\n        engine: Inference engine - 'vllm' or 'llama_cpp' (default: PITA_ENGINE env or 'vllm')\n        tokenizer: Tokenizer path (default: PITA_TOKENIZER env or None)\n        port: Port number (default: PITA_PORT env or 8001)\n        host: Host address (default: PITA_HOST env or '0.0.0.0')\n    \"\"\"\n    default_config = get_default_config()\n\n    # Use provided values or fall back to defaults from environment\n    config = {\n        \"model\": model if model is not None else default_config[\"model\"],\n        \"engine\": engine if engine is not None else default_config[\"engine\"],\n        \"tokenizer\": tokenizer if tokenizer is not None else default_config[\"tokenizer\"],\n        \"port\": port if port is not None else default_config[\"port\"],\n        \"host\": host if host is not None else default_config[\"host\"],\n    }\n\n    # Create a dedicated app instance with the specified config\n    server_app = create_app(config)\n\n    uvicorn.run(server_app, host=config[\"host\"], port=config[\"port\"])\n</code></pre>"},{"location":"api/test_time_coding/","title":"Test Time Coding","text":""},{"location":"api/test_time_coding/#pita.api.test_time_coding.decode","title":"<code>decode(system_string: str) -&gt; tuple[Optional[Sequential_Monte_Carlo], Optional[Power_Sampling]]</code>","text":"<p>Decode test-time scaling parameters from a system prompt string.</p> <p>Parameters:</p> Name Type Description Default <code>system_string</code> <code>str</code> <p>The encoded string containing test-time scaling parameters. Format: \"ITS__\" required <p>Returns:</p> Type Description <code>Optional[Sequential_Monte_Carlo]</code> <p>A tuple of (chain_sampling, token_sampling) where each element is either</p> <code>Optional[Power_Sampling]</code> <p>a parameter object or None if that sampling technique was not specified.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the format is invalid or parameter values are non-numeric.</p> Source code in <code>pita/api/test_time_coding.py</code> <pre><code>def decode(system_string: str) -&gt; tuple[Optional[Sequential_Monte_Carlo], Optional[Power_Sampling]]:\n    \"\"\"\n    Decode test-time scaling parameters from a system prompt string.\n\n    Args:\n        system_string: The encoded string containing test-time scaling parameters.\n            Format: \"ITS_&lt;chain&gt;_&lt;chain_params...&gt;_&lt;token&gt;_&lt;token_params...&gt;\"\n\n    Returns:\n        A tuple of (chain_sampling, token_sampling) where each element is either\n        a parameter object or None if that sampling technique was not specified.\n\n    Raises:\n        ValueError: If the format is invalid or parameter values are non-numeric.\n    \"\"\"\n    # Split the string into parts (only first token before space)\n    parts = system_string.split(\" \")[0].split(\"_\")\n\n    if len(parts) &lt; 2 or parts[0] != \"ITS\":\n        raise ValueError(\"Invalid system string format. Must start with 'ITS'.\")\n\n    chain_sampling = None\n    token_sampling = None\n\n    i = 1  # Start after \"ITS\"\n\n    # Parse chain sampling method\n    if parts[i] == \"NONE\":\n        i += 1\n    elif parts[i] == \"SMC\":\n        # SMC requires 3 params: num_particles, tokens_per_step, stop_on_eos\n        if i + 3 &gt;= len(parts):\n            raise ValueError(\"SMC requires 3 parameters: num_particles, tokens_per_step, stop_on_eos\")\n        if not all(parts[i+j].isdigit() for j in range(1, 4)):\n            raise ValueError(f\"Invalid SMC parameters: expected 3 integers after 'SMC'\")\n        chain_sampling = Sequential_Monte_Carlo(\n            num_particles=int(parts[i+1]),\n            tokens_per_step=int(parts[i+2]),\n            stop_on_eos=bool(int(parts[i+3]))\n        )\n        i += 4\n    else:\n        raise ValueError(f\"Unknown chain sampling method: '{parts[i]}'. Expected 'SMC' or 'NONE'.\")\n\n    # Parse token sampling method\n    if i &gt;= len(parts):\n        raise ValueError(\"Missing token sampling specification. Expected 'PS' or 'NONE'.\")\n\n    if parts[i] == \"NONE\":\n        pass  # token_sampling stays None\n    elif parts[i] == \"PS\":\n        # PS requires 2 params: block_size, MCMC_steps\n        if i + 2 &gt;= len(parts):\n            raise ValueError(\"PS requires 2 parameters: block_size, MCMC_steps\")\n        if not all(parts[i+j].isdigit() for j in range(1, 3)):\n            raise ValueError(f\"Invalid PS parameters: expected 2 integers after 'PS'\")\n        token_sampling = Power_Sampling(\n            block_size=int(parts[i+1]),\n            MCMC_steps=int(parts[i+2])\n        )\n    else:\n        raise ValueError(f\"Unknown token sampling method: '{parts[i]}'. Expected 'PS' or 'NONE'.\")\n\n    return chain_sampling, token_sampling\n</code></pre>"},{"location":"api/test_time_coding/#pita.api.test_time_coding.encode","title":"<code>encode(chain_sampling: Optional[Sequential_Monte_Carlo] = None, token_sampling: Optional[Power_Sampling] = None) -&gt; str</code>","text":"<p>Encode test-time scaling parameters into a string for embedding in system prompts.</p> <p>Parameters:</p> Name Type Description Default <code>chain_sampling</code> <code>Optional[Sequential_Monte_Carlo]</code> <p>Chain sampling configuration (SMC).</p> <code>None</code> <code>token_sampling</code> <code>Optional[Power_Sampling]</code> <p>Token sampling configuration (Power Sampling).</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>A formatted string: \"ITS__\" <code>str</code> <p>Returns empty string if no parameters are provided.</p> Source code in <code>pita/api/test_time_coding.py</code> <pre><code>def encode(\n    chain_sampling: Optional[Sequential_Monte_Carlo] = None,\n    token_sampling: Optional[Power_Sampling] = None\n) -&gt; str:\n    \"\"\"\n    Encode test-time scaling parameters into a string for embedding in system prompts.\n\n    Args:\n        chain_sampling: Chain sampling configuration (SMC).\n        token_sampling: Token sampling configuration (Power Sampling).\n\n    Returns:\n        A formatted string: \"ITS_&lt;chain&gt;_&lt;chain_params&gt;_&lt;token&gt;_&lt;token_params&gt;\"\n        Returns empty string if no parameters are provided.\n    \"\"\"\n    if chain_sampling is None and token_sampling is None:\n        return \"\"\n\n    parts = [\"ITS\"]\n\n    # Encode chain sampling method\n    if chain_sampling is None:\n        parts.append(\"NONE\")\n    elif isinstance(chain_sampling, Sequential_Monte_Carlo):\n        parts.extend([\"SMC\", str(chain_sampling.num_particles), \n                      str(chain_sampling.tokens_per_step), \n                      str(int(chain_sampling.stop_on_eos))])\n    else:\n        raise ValueError(f\"Unknown chain sampling type: {type(chain_sampling)}\")\n\n    # Encode token sampling method\n    if token_sampling is None:\n        parts.append(\"NONE\")\n    elif isinstance(token_sampling, Power_Sampling):\n        parts.extend([\"PS\", str(token_sampling.block_size), \n                      str(token_sampling.MCMC_steps)])\n    else:\n        raise ValueError(f\"Unknown token sampling type: {type(token_sampling)}\")\n\n    return \"_\".join(parts)\n</code></pre>"},{"location":"examples/inference_backends/","title":"Inference Backend Examples","text":"<p>This page provides examples of how to initialize and use different inference backends supported by <code>pita</code>.</p>"},{"location":"examples/inference_backends/#vllm-backend","title":"vLLM Backend","text":"<p>vLLM is a high-throughput, memory-efficient serving engine for LLMs.</p> <pre><code>from pita.inference.LLM_backend import AutoregressiveSampler\n\n# Initialize vLLM sampler\nsampler = AutoregressiveSampler(\n    engine=\"vllm\",\n    model=\"facebook/opt-125m\",\n    dtype=\"auto\",\n    gpu_memory_utilization=0.85,\n    max_model_len=1024,\n    max_probs=100,  # Return top 100 logits/logprobs\n    logits_processor=True  # Enable logits processor for entropy/normalization\n)\n\n# Configure sampling parameters\nsampler.sampling_params.max_tokens = 10\nsampler.sampling_params.temperature = 1.0\n\n# Sample from the model\ncontext = \"What is the capital of France?\"\noutput = sampler.sample(context)\ngenerated_text = sampler.tokenizer.decode(output.output_ids)\nprint(generated_text)\n</code></pre>"},{"location":"examples/inference_backends/#llamacpp-backend","title":"Llama.cpp Backend","text":"<p>Llama.cpp is a general-purpose backend for both CPUs and GPUs, optimized for GGUF models.</p> <pre><code>from pita.inference.LLM_backend import AutoregressiveSampler\n\n# Initialize Llama.cpp sampler\nsampler = AutoregressiveSampler(\n    engine=\"llama_cpp\",\n    model=\"path/to/your/model.gguf\",\n    dtype=\"auto\",  # dtype is handled by GGUF quantization\n    max_model_len=1024,\n    max_probs=100\n)\n\n# Configure sampling parameters\nsampler.sampling_params.max_tokens = 50\n\n# Sample from the model\ncontext = \"Explain the theory of relativity.\"\noutput = sampler.sample(context)\ngenerated_text = sampler.tokenizer.decode(output.output_ids)\nprint(generated_text)\n</code></pre>"},{"location":"examples/inference_backends/#tensorrt-backend","title":"TensorRT Backend","text":"<p>TensorRT provides optimized inference for NVIDIA GPUs.</p> <pre><code>from pita.inference.LLM_backend import AutoregressiveSampler\n\n# Initialize TensorRT sampler\nsampler = AutoregressiveSampler(\n    engine=\"tensorrt\",\n    model=\"path/to/tensorrt/engine\",\n    max_model_len=1024,\n    max_probs=100,\n    logits_processor=True\n)\n\n# Sample from the model\ncontext = \"Describe quantum computing.\"\noutput = sampler.sample(context)\ngenerated_text = sampler.tokenizer.decode(output.output_ids)\nprint(generated_text)\n</code></pre>"},{"location":"examples/inference_backends/#transformers-backend","title":"Transformers Backend","text":"<p>The Transformers backend is useful for models not yet supported by vLLM or llama.cpp, or when deep customization is needed.</p> <pre><code>from pita.inference.LLM_backend import AutoregressiveSampler\n\n# Initialize Transformers sampler\nsampler = AutoregressiveSampler(\n    engine=\"transformers\",\n    model=\"facebook/opt-125m\",\n    dtype=\"auto\",\n    max_model_len=1024\n)\n\n# Sample from the model\ncontext = \"Once upon a time\"\noutput = sampler.sample(context)\ngenerated_text = sampler.tokenizer.decode(output.output_ids)\nprint(generated_text)\n</code></pre>"},{"location":"examples/sampling_strategies/","title":"Sampling Strategy Examples","text":"<p><code>pita</code> provides advanced sampling strategies to improve the quality and reasoning capabilities of models.</p>"},{"location":"examples/sampling_strategies/#power-sampling","title":"Power Sampling","text":"<p>Power Sampling uses Metropolis-Hastings MCMC to iteratively refine generated tokens. It operates at the token level, proposing and accepting/rejecting token replacements based on a decision metric.</p> <pre><code>from pita.inference.LLM_backend import AutoregressiveSampler\n\n# Initialize sampler\nsampler = AutoregressiveSampler(\n    engine=\"vllm\",\n    model=\"Qwen/Qwen2.5-0.5B-Instruct\",\n    logits_processor=True  # Required for power sampling\n)\n\n# Enable power sampling\nsampler.enable_power_sampling(\n    block_size=250,          # Tokens generated per block\n    MCMC_steps=3,            # Number of MCMC refinement steps\n    token_metric=\"power_distribution\"  # Metric for accept/reject decisions\n)\n\n# Use token sampling\nprompt = \"Solve the equation: 3x + 7 = 22\"\noutput = sampler.token_sample(prompt)\ngenerated_text = sampler.tokenizer.decode(output.output_ids)\nprint(generated_text)\n</code></pre>"},{"location":"examples/sampling_strategies/#available-token-metrics-for-power-sampling","title":"Available Token Metrics for Power Sampling","text":"<ul> <li><code>\"logprobs\"</code>: Standard log probability scoring</li> <li><code>\"power_distribution\"</code>: Temperature-scaled power distribution (recommended)</li> <li><code>\"entropy\"</code>: Entropy-based metric</li> <li><code>\"likelihood_confidence\"</code>: Combined probability and confidence</li> </ul>"},{"location":"examples/sampling_strategies/#sequential-monte-carlo-smc","title":"Sequential Monte Carlo (SMC)","text":"<p>SMC maintains multiple candidate sequences (particles) and selectively prunes/extends them based on quality metrics. It operates at the chain level.</p> <pre><code>from pita.inference.LLM_backend import AutoregressiveSampler\n\n# Initialize sampler\nsampler = AutoregressiveSampler(\n    engine=\"vllm\",\n    model=\"Qwen/Qwen2.5-0.5B-Instruct\",\n    logits_processor=True  # Required for SMC metrics\n)\n\n# Enable SMC\nsampler.enable_smc(\n    num_particles=5,         # Number of candidate sequences to maintain\n    tokens_per_step=50,      # Tokens generated per SMC step\n    stop_on_eos=True,        # Stop when EOS token is generated\n    token_metric=\"likelihood_confidence\",  # Metric for particle scoring\n    aggregation=\"last\"       # How to aggregate token scores (\"last\", \"minimum\", \"product\")\n)\n\n# Use chain sampling\nprompt = \"Write a detailed explanation of photosynthesis.\"\noutput = sampler.chain_sample(prompt)\ngenerated_text = sampler.tokenizer.decode(output.output_ids)\nprint(generated_text)\n</code></pre>"},{"location":"examples/sampling_strategies/#smc-aggregation-methods","title":"SMC Aggregation Methods","text":"<ul> <li><code>\"last\"</code>: Use only the last token's metric for scoring</li> <li><code>\"minimum\"</code>: Use the minimum metric across all tokens</li> <li><code>\"product\"</code>: Multiply metrics across all tokens</li> <li><code>\"model_aggregate\"</code>: Custom model-based aggregation (WIP)</li> </ul>"},{"location":"examples/sampling_strategies/#combining-strategies-advanced","title":"Combining Strategies (Advanced)","text":"<p>You can combine chain-level and token-level strategies for hybrid scaling. </p> <pre><code>from pita.inference.LLM_backend import AutoregressiveSampler\n\n# Initialize sampler\nsampler = AutoregressiveSampler(\n    engine=\"vllm\",\n    model=\"Qwen/Qwen2.5-0.5B-Instruct\",\n    logits_processor=True\n)\n\n# Enable token-level (Power Sampling)\nsampler.enable_power_sampling(\n    block_size=200,\n    MCMC_steps=2,\n    token_metric=\"power_distribution\"\n)\n\n# Use power sampling\noutput_power = sampler.token_sample(prompt)\n</code></pre>"},{"location":"examples/sampling_strategies/#using-sampling-strategies-via-api","title":"Using Sampling Strategies via API","text":"<p>You can trigger sampling strategies via the API server using special system prompts:</p> <pre><code>import openai\n\nclient = openai.OpenAI(\n    base_url=\"http://localhost:8001/v1\",\n    api_key=\"none\"\n)\n\n# Power Sampling via API: ITS PS_&lt;max_tokens&gt;_&lt;block_size&gt;_&lt;MCMC_steps&gt;\nresponse = client.chat.completions.create(\n    model=\"Qwen/Qwen2.5-0.5B-Instruct\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"ITS PS_1000_250_3 You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Solve: 5x - 3 = 17\"}\n    ]\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"examples/sampling_strategies/#disabling-sampling-strategies","title":"Disabling Sampling Strategies","text":"<p>To revert to standard sampling:</p> <pre><code># Disable token sampling (if enabled)\nif hasattr(sampler, 'token_sample_name'):\n    sampler.token_sample_name = None\n    sampler.token_sample_fn = None\n\n# Disable chain sampling (if enabled)\nif hasattr(sampler, 'chain_sample_name'):\n    sampler.chain_sample_name = None\n    sampler.chain_sample_fn = None\n\n# Now sampler.sample() will use standard autoregressive sampling\noutput = sampler.sample(prompt)\n</code></pre>"},{"location":"examples/serving_modes/","title":"Serving Mode Examples","text":"<p><code>pita</code> supports two primary modes of operation: Programmatic and API.</p>"},{"location":"examples/serving_modes/#programmatic-mode","title":"Programmatic Mode","text":"<p>Use <code>pita</code> directly in your Python code for maximum control and offline processing.</p> <pre><code>from pita.inference.LLM_backend import AutoregressiveSampler\n\n# Initialize the sampler\nsampler = AutoregressiveSampler(\n    engine=\"vllm\",\n    model=\"Qwen/Qwen2.5-0.5B-Instruct\",\n    logits_processor=True\n)\n\n# Basic sampling\nprompt = \"Write a short story about a robot.\"\noutput = sampler.sample(prompt)\ngenerated_text = sampler.tokenizer.decode(output.output_ids)\nprint(f\"Generated text: {generated_text}\")\n\n# Power Sampling\nsampler.enable_power_sampling(\n    block_size=250,\n    MCMC_steps=3,\n    token_metric=\"power_distribution\"\n)\n\noutput = sampler.token_sample(prompt)\ngenerated_text = sampler.tokenizer.decode(output.output_ids)\nprint(f\"Generated text (Power Sampling): {generated_text}\")\n</code></pre>"},{"location":"examples/serving_modes/#api-mode","title":"API Mode","text":"<p>Run <code>pita</code> as a server with an OpenAI-compatible API endpoint.</p>"},{"location":"examples/serving_modes/#starting-the-server","title":"Starting the Server","text":"<p>Start the server using the <code>pita serve</code> command:</p> <pre><code># Start with defaults\npita serve\n\n# Customize model, engine, and port\npita serve --model Qwen/Qwen2.5-0.5B-Instruct --engine vllm --port 8001\n\n# Short options are also available\npita serve -m Qwen/Qwen2.5-0.5B-Instruct -e vllm -p 8001\n</code></pre> <p>You can also use environment variables:</p> <pre><code>export PITA_ENGINE=vllm\nexport PITA_MODEL=Qwen/Qwen2.5-0.5B-Instruct\nexport PITA_PORT=8001\npita serve\n</code></pre>"},{"location":"examples/serving_modes/#querying-the-api","title":"Querying the API","text":"<p>Once the server is running, you can use any OpenAI-compatible client.</p> <pre><code>import openai\n\nclient = openai.OpenAI(\n    base_url=\"http://localhost:8001/v1\",\n    api_key=\"none\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"Qwen/Qwen2.5-0.5B-Instruct\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Tell me a joke.\"}\n    ]\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"examples/serving_modes/#advanced-sampling-via-system-prompt","title":"Advanced Sampling via System Prompt","text":"<p>You can trigger advanced sampling strategies by prefixing your system prompt with <code>ITS</code> and specific parameters:</p> <pre><code># Example: Trigger Power Sampling with 1000 tokens, block size 250\nsystem_prompt = \"ITS PS_1000_250_3 You are a helpful assistant.\"\n</code></pre>"},{"location":"examples/token_metrics/","title":"Token Metric Examples","text":"<p>Token metrics are used to evaluate and guide the generation process. The <code>pita.sampling.token_metrics</code> module provides utilities for calculating various metrics from model outputs.</p>"},{"location":"examples/token_metrics/#available-metrics","title":"Available Metrics","text":"<p>PITA supports four main token metrics:</p> <ul> <li>logprobs: Log probabilities of the generated tokens (standard model confidence)</li> <li>power_distribution: Temperature-scaled power distribution using logits and normalization constants</li> <li>entropy: Model uncertainty at each token position</li> <li>likelihood_confidence: Combined metric multiplying probability by confidence (exp(-entropy))</li> </ul>"},{"location":"examples/token_metrics/#calculating-token-metrics","title":"Calculating Token Metrics","text":"<pre><code>from pita.inference.LLM_backend import AutoregressiveSampler\nfrom pita.sampling.token_metrics import calc_token_metric\n\n# Initialize sampler with logits processor enabled for entropy/power metrics\nsampler = AutoregressiveSampler(\n    engine=\"vllm\",\n    model=\"facebook/opt-125m\",\n    logits_processor=True  # Required for entropy and power_distribution\n)\n\n# Generate output\ncontext = \"The capital of France is\"\noutput = sampler.sample(context)\n\n# Calculate different token metrics\nlogprobs = calc_token_metric(output, sampler, metric=\"logprobs\")\npower_dist = calc_token_metric(output, sampler, metric=\"power_distribution\")\nentropy = calc_token_metric(output, sampler, metric=\"entropy\")\n\nprint(f\"Log probabilities shape: {logprobs.shape}\")\nprint(f\"Power distribution shape: {power_dist.shape}\")\nprint(f\"Entropy shape: {entropy.shape}\")\n</code></pre> <p>In most workflows, you do not need to call <code>calc_token_metric</code> directly when generating text. Sampling strategies like power sampling will compute the requested metric internally when you pass a <code>token_metric</code> name (for example, <code>\"logprobs\"</code>, <code>\"power_distribution\"</code>, <code>\"entropy\"</code>, or <code>\"likelihood_confidence\"</code>). Manual metric calculation is mainly useful for inspection, analysis, or debugging outside of these sampling helpers.</p>"},{"location":"examples/token_metrics/#calculating-sequence-probabilities","title":"Calculating Sequence Probabilities","text":"<pre><code>from pita.sampling.token_metrics import calc_sequence_prob, calc_sequence_logprob\n\n# Calculate probability of a sequence (tokens 0-5)\nseq_prob = calc_sequence_prob(\n    output=output,\n    sampler=sampler,\n    starting_index=0,\n    ending_index=5,\n    metric=\"logprobs\"\n)\n\n# Calculate log probability of a sequence\nseq_logprob = calc_sequence_logprob(\n    output=output,\n    sampler=sampler,\n    starting_index=0,\n    ending_index=5,\n    metric=\"likelihood_confidence\"  # Combines logprobs with entropy\n)\n\nprint(f\"Sequence probability: {seq_prob}\")\nprint(f\"Sequence log probability: {seq_logprob}\")\n</code></pre>"},{"location":"examples/token_metrics/#length-normalized-metrics","title":"Length-Normalized Metrics","text":"<p>For comparing sequences of different lengths, use length-normalized versions:</p> <pre><code>from pita.sampling.token_metrics import (\n    calc_sequence_length_normalized_prob,\n    calc_sequence_length_normalized_logprob\n)\n\n# Length-normalized probability\nnorm_prob = calc_sequence_length_normalized_prob(\n    output=output,\n    sampler=sampler,\n    starting_index=0,\n    ending_index=5,\n    metric=\"logprobs\"\n)\n\n# Length-normalized log probability\nnorm_logprob = calc_sequence_length_normalized_logprob(\n    output=output,\n    sampler=sampler,\n    starting_index=0,\n    ending_index=5,\n    metric=\"power_distribution\"\n)\n</code></pre>"},{"location":"examples/token_metrics/#accessing-raw-metrics-from-output","title":"Accessing Raw Metrics from Output","text":"<p>The <code>Output</code> object contains raw metrics:</p> <pre><code>output = sampler.sample(context)\n\n# Access raw metrics\nprint(f\"Output IDs: {output.output_ids}\")\nprint(f\"Top-k logprobs: {output.top_k_logprobs}\")\nprint(f\"Top-k logits: {output.top_k_logits}\")\nprint(f\"Entropy: {output.entropy}\")\nprint(f\"Normalization constants: {output.unprocessed_log_normalization_constant}\")\n</code></pre>"},{"location":"examples/token_metrics/#using-metrics-in-sampling-strategies","title":"Using Metrics in Sampling Strategies","text":"<p>Token metrics are used internally by sampling strategies for decision-making:</p> <pre><code># Enable power sampling with a specific token metric\nsampler.enable_power_sampling(\n    block_size=250,\n    MCMC_steps=3,\n    token_metric=\"power_distribution\"  # or \"logprobs\", \"entropy\", \"likelihood_confidence\"\n)\n\noutput = sampler.token_sample(context)\n</code></pre>"},{"location":"inference/LLM_backend/","title":"LLM Backend","text":""},{"location":"inference/LLM_backend/#pita.inference.LLM_backend.AutoregressiveSampler","title":"<code>AutoregressiveSampler</code>","text":"<p>Stores parameters concerning the LLM, autoregressive sampling, and power sampling.</p> <p>Attributes:</p> Name Type Description <code>engine</code> <code>str</code> <p>The engine used for sampling.</p> <code>model</code> <code>str</code> <p>The LLM Model name.</p> <code>llm</code> <code>object</code> <p>LLM object from engine used for inference/sampling.</p> <code>tokenizer</code> <code>object</code> <p>Tokenizer to use for encoding/decoding (HuggingFace AutoTokenizer).</p> <code>sample_fn</code> <code>object</code> <p>Standard Sampling Function to use for sampling from the autoregressive model without test time scaling.</p> <code>sampling_params</code> <code>object</code> <p>Parameters to use for standard sampling.</p> <code>chain_sampling</code> <code>object</code> <p>Chain Sampling Object used for chain level test time scaling (i.e Best-of-N, SMC, etc.)</p> <code>token_sampling</code> <code>object</code> <p>Token Sampling Object used for token level test time scaling (i.e Metropolis-Hastings Sampling)</p> <code>chain_sample_fn</code> <code>object</code> <p>The chain sampling function to use for chain level test time scaling.</p> <code>token_sample_fn</code> <code>object</code> <p>The token sampling function to use for token level test time scaling.</p> Source code in <code>pita/inference/LLM_backend.py</code> <pre><code>class AutoregressiveSampler:\n    \"\"\"Stores parameters concerning the LLM, autoregressive sampling, and power sampling.\n\n    Attributes:\n        engine (str): The engine used for sampling.\n        model (str): The LLM Model name.\n        llm (object): LLM object from engine used for inference/sampling.\n        tokenizer (object): Tokenizer to use for encoding/decoding (HuggingFace AutoTokenizer).\n        sample_fn (object): Standard Sampling Function to use for sampling from the autoregressive model without test time scaling.\n        sampling_params (object): Parameters to use for standard sampling.\n        chain_sampling (object): Chain Sampling Object used for chain level test time scaling (i.e Best-of-N, SMC, etc.)\n        token_sampling (object): Token Sampling Object used for token level test time scaling (i.e Metropolis-Hastings Sampling)\n        chain_sample_fn (object): The chain sampling function to use for chain level test time scaling.\n        token_sample_fn (object): The token sampling function to use for token level test time scaling.\n    \"\"\"\n    def __init__(\n        self,\n        engine: str,\n        model: str,\n        dtype: str,\n        tokenizer_path: str,\n        gpu_memory_utilization: float,\n        max_model_len: int,\n        max_probs: int,\n        logits_processor: bool,\n        trust_remote_code: bool,\n        sampling_params: Sampling_Params,\n        **kwargs: Any\n    ) -&gt; None:      \n\n        \"\"\"Create an AutoregressiveSampler object given the engine, engine parameters, and model name.\n\n        Args:\n            engine (str): Engine to use for autoregressive sampling. Currently only \"vllm\" and \"llama_cpp\" are supported.\n            model (str): Model to load.\n            dtype (str): Data type to use when loading the model. \"auto\" lets the engine decide.\n            tokenizer_path (str): Path to a model with a tokenizer if the model path doesn't include a tokenizer.\n            gpu_memory_utilization (float): GPU memory utilization to use.\n            max_model_len (int): Max model context length (context window = prompt + generated tokens).\n            max_probs (int): Number of top ranked probabilities (logits &amp; logprobs) to store per output token.\n            logits_processor (bool): Whether to enable the internal logits processor that allows for normalization constants and entropy to be calculated.\n            trust_remote_code (bool): Whether to trust remote code when loading the model.\n            sampling_params (Sampling_Params): General sampling parameters to use (Sampling_Params Class).\n            **kwargs: Additional keyword arguments passed to the backend LLM creation function.\n\n        Raises:\n            ValueError: If the engine is not supported.\n        \"\"\"\n        self.engine = engine\n        self.model = model\n\n        print(f\"Loading model {model} with {engine}...\")\n\n        # Separate Backend Loading for each engine\n        if(engine == \"vllm\"):\n            backend = _get_vllm_backend()\n\n            if(max_probs &gt; 0 and logits_processor == False):\n                print(\"max_probs is set but logits_processor is False. Setting logits_processor to True.\")\n                logits_processor = True\n\n            # Create the LLM object\n            self.llm = backend.create_LLM_object(\n                model_name = model, \n                dtype = dtype,\n                gpu_memory_utilization = gpu_memory_utilization,\n                max_model_len = max_model_len,\n                max_probs = max_probs,\n                logits_processor = logits_processor,\n                **kwargs\n            )  \n\n            # Set the autoregressive sampler function\n            self.sample_fn = backend.sample\n\n            # Create the engine parameters used for the completion function in vLLM\n            engine_params = backend.create_vllm_engine_params()\n\n            # Set the redis client for the LogitsLoggingProcessor\n            # Add the normalization_constants and normalization_constants_temp_scaled lists to extra_args\n            if(logits_processor):\n                print(\"Enabling logits processing in engine parameters extra_args.\")\n                engine_params.extra_args = {}\n                engine_params.extra_args[\"req_id\"] = \"my_request_\" + str(time.time())\n\n        elif(engine == \"llama_cpp\"):\n            backend = _get_llama_cpp_backend()\n            # Extract model_type from kwargs if provided, otherwise let backend infer it\n            llama_model_type = kwargs.pop('model_type', None)\n            # Create the LLM object\n            self.llm = backend.create_LLM_object(\n                model_name = model, \n                model_type = llama_model_type,\n                dtype = dtype, \n                gpu_memory_utilization = gpu_memory_utilization, \n                max_model_len = max_model_len,\n                max_logprobs = max_probs,\n                logits_processor = logits_processor,\n                **kwargs\n            )\n            # Set the autoregressive sampler function\n            self.sample_fn = backend.sample\n            # Llama.cpp does not have a separate engine params class\n            engine_params = None\n\n        elif(engine == \"tensorrt\"):\n            backend = _get_tensorrt_backend()\n            # Create the LLM object\n            self.llm = backend.create_LLM_object(\n                model_name = model, \n                dtype = dtype, \n                gpu_memory_utilization = gpu_memory_utilization, \n                max_model_len = max_model_len,\n                max_logprobs = max_probs,\n                logits_processor = logits_processor,\n                **kwargs\n            )\n            # Set the autoregressive sampler function\n            self.sample_fn = backend.sample\n            # TensorRT-LLM uses per-request engine params, create a default instance\n            engine_params = backend.create_tensorrt_engine_params()\n\n        else:\n            raise ValueError(f\"Engine {engine} not supported for Autoregressive Sampler. Supported engines are: 'vllm', 'llama_cpp', 'tensorrt'\")\n\n        # Create tokenizer depending on whether a tokenizer path is provided\n        # Needed as some models do not include the tokenizer files in the same repo as the model\n        if tokenizer_path is not None:\n            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, trust_remote_code=trust_remote_code)\n        else:\n            self.tokenizer = AutoTokenizer.from_pretrained(model, trust_remote_code=trust_remote_code)\n\n        print(\"Engine Params Extra Args:\", getattr(engine_params, \"extra_args\", \"N/A\") if engine_params is not None else \"N/A\")\n\n        # Intialize the Sampling Params\n        if(sampling_params is None):\n            self.sampling_params = Sampling_Params(\n                engine = engine, \n                engine_params = engine_params, \n                logprobs_per_token = max_probs,\n                logits_per_token = max_probs\n            )\n        else:\n            self.sampling_params = sampling_params\n\n        # Intialize the other test-time sampling parameters to Nones \n        self.chain_sampling = None\n        self.token_sampling = None\n\n    def sample(self,\n        context: str,\n        **kwargs: Any\n    )-&gt; Output:\n        \"\"\"Samples programmatically from the LLM given a context and max new tokens. Sample function is the engine_backend.sample function.\n\n        Args:\n            context (str): The input context.\n            **kwargs: Additional keyword arguments passed to the chosen LLM Inference Engine.\n\n        Returns:\n            Output: The output of the sample function.\n        \"\"\"\n        return self.sample_fn(self, context, **kwargs)\n\n    def token_sample(self,\n        context: str,\n        **kwargs: Any\n    )-&gt; Output:\n        \"\"\"Samples programmatically from the LLM using the token sampling function\n\n        Args:\n            context (str): The input context.\n            **kwargs: Additional keyword arguments passed to the chosen LLM Inference Engine.\n\n        Returns:\n            Output: The output of the sample function.\n        \"\"\"\n        if getattr(self, \"token_sample_name\", None) == \"Power Sampling\":\n            return self.token_sample_fn(self, context, **kwargs)\n        else:\n            raise ValueError(\"Token sampling is not enabled for this LLM/Engine.\")\n\n    def chain_sample(self,\n        context: str,\n        **kwargs: Any\n    )-&gt; Output:\n        \"\"\"Samples programmatically from the LLM using the chain sampling function\n\n        Args:\n            context (str): The input context.\n            **kwargs: Additional keyword arguments passed to the chosen LLM Inference Engine.\n\n        Returns:\n            Output: The output of the sample function.\n        \"\"\"\n        if getattr(self, \"chain_sample_name\", None) == \"SMC\" or getattr(self, \"chain_sample_name\", None) == \"Best-of-N\":\n            return self.chain_sample_fn(self, context, **kwargs)\n        else:\n            raise ValueError(\"Chain sampling is not enabled for this LLM/Engine.\")\n\n    # Chain Sampling Methods\n    def enable_smc(\n        self,\n        num_particles: int,\n        tokens_per_step: int,\n        stop_on_eos: bool,\n        token_metric: str,  \n        aggregation: str\n    )-&gt; None:\n        \"\"\"\n        Enables SMC sampling for the chosen LLM/Engine.\n\n        Args:\n            num_particles (int): Number of particles to use for SMC.\n            tokens_per_step (int): Number of tokens to generate per step.\n            stop_on_eos (bool): (WIP)Whether to stop on end of sequence.\n            token_metric (str): Token metric to use to grade each particle. Can be logprobs, power_distribution, entropy, or PRM\n            aggregation (str): Aggregation method of the scores of each particle. Can be the last, minimum, product, or model_aggregate.\n\n        Returns:\n            None\n        \"\"\"\n        # Check if chain sampling has already been enabled. If so replace it with SMC.\n        if(self.chain_sampling is not None):\n            print(\"Warning: Current Chain Sampling Strategy is being replaced with SMC.\")\n\n        # Check if the engine/LLM is set up for SMC\n        if(token_metric == \"PRM\"):\n            raise ValueError(\"PRM is not supported YET for SMC.\")\n        elif(token_metric == \"logprobs\" or token_metric == \"power_distribution\" or token_metric == \"entropy\"):\n            if(self.engine == \"vllm\"):\n                vllm_backend.check_token_metric_compatibility(self, token_metric)\n            elif(self.engine == \"llama_cpp\"):\n                llama_cpp_backend.check_token_metric_compatibility(self, token_metric)\n            elif(self.engine == \"tensorrt\"):\n                tensorrt_backend.check_token_metric_compatibility(self, token_metric)\n        else:\n            raise ValueError(f\"{token_metric} not supported for SMC.\")\n\n        # Check if the aggregation method is supported\n        if(aggregation == \"last\" or aggregation == \"minimum\" or aggregation == \"product\" or aggregation == \"model_aggregate\"):\n            pass\n        else:\n            raise ValueError(f\"{aggregation} not supported for SMC.\")\n\n        # Create the SMC Class\n        from pita.sampling.smc import Sequential_Monte_Carlo\n        self.chain_sampling = Sequential_Monte_Carlo(\n            num_particles=num_particles,\n            tokens_per_step=tokens_per_step,\n            stop_on_eos=stop_on_eos,\n            token_metric=token_metric,\n            aggregation=aggregation\n        )\n\n        # Set the chain sampling function to the SMC sample function\n        self.chain_sample_fn = self.chain_sampling.sample\n        self.chain_sample_name = \"SMC\"\n\n    # Token Sampling Methods\n    def enable_power_sampling(\n        self,\n        block_size: int,\n        MCMC_steps: int,\n        token_metric: str,\n    )-&gt; None:\n        \"\"\"\n        Enables Power Sampling for the chosen LLM/Engine. Checks to see if the engine/LLM is compatible with Power Sampling by verifying that the token metric is supported/available to be used\n\n        Args:\n            block_size (int): Number of tokens to generate per step.\n            MCMC_steps (int): Number of MCMC steps to use for Power Sampling.\n            token_metric (str): Token metric to use to grade each particle. Can be logprobs, power_distribution, entropy, or PRM\n\n        Returns:\n            None\n        \"\"\"\n        # Check if chain sampling has already been enabled. If so replace it with Power Sampling.\n        if(self.token_sampling is not None):\n            print(\"Warning: Current Token Sampling Strategy is being replaced with Power Sampling.\")\n\n        # Check if the engine/LLM is set up for Power Sampling\n        if(token_metric == \"PRM\"):\n            raise ValueError(\"PRM is not supported YET for Power Sampling.\")\n        elif(token_metric == \"logprobs\" or token_metric == \"power_distribution\" or token_metric == \"entropy\"):\n            if(self.engine == \"vllm\"):\n                vllm_backend.check_token_metric_compatibility(self, token_metric)\n            elif(self.engine == \"llama_cpp\"):\n                llama_cpp_backend.check_token_metric_compatibility(self, token_metric)\n            elif(self.engine == \"tensorrt\"):\n                tensorrt_backend.check_token_metric_compatibility(self, token_metric)\n        else:\n            raise ValueError(f\"{token_metric} not supported for Power Sampling.\")\n\n        # Create the Power Sampling Class\n        from pita.sampling.power_sample import Power_Sampling\n        self.token_sampling = Power_Sampling(\n            block_size=block_size,\n            MCMC_steps=MCMC_steps,\n            token_metric=token_metric\n        )\n\n        # Set the token sampling function to the Power Sampling sample function\n        self.token_sample_fn = self.token_sampling.sample\n        self.token_sample_name = \"Power Sampling\"\n</code></pre>"},{"location":"inference/LLM_backend/#pita.inference.LLM_backend.AutoregressiveSampler.__init__","title":"<code>__init__(engine: str, model: str, dtype: str, tokenizer_path: str, gpu_memory_utilization: float, max_model_len: int, max_probs: int, logits_processor: bool, trust_remote_code: bool, sampling_params: Sampling_Params, **kwargs: Any) -&gt; None</code>","text":"<p>Create an AutoregressiveSampler object given the engine, engine parameters, and model name.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>str</code> <p>Engine to use for autoregressive sampling. Currently only \"vllm\" and \"llama_cpp\" are supported.</p> required <code>model</code> <code>str</code> <p>Model to load.</p> required <code>dtype</code> <code>str</code> <p>Data type to use when loading the model. \"auto\" lets the engine decide.</p> required <code>tokenizer_path</code> <code>str</code> <p>Path to a model with a tokenizer if the model path doesn't include a tokenizer.</p> required <code>gpu_memory_utilization</code> <code>float</code> <p>GPU memory utilization to use.</p> required <code>max_model_len</code> <code>int</code> <p>Max model context length (context window = prompt + generated tokens).</p> required <code>max_probs</code> <code>int</code> <p>Number of top ranked probabilities (logits &amp; logprobs) to store per output token.</p> required <code>logits_processor</code> <code>bool</code> <p>Whether to enable the internal logits processor that allows for normalization constants and entropy to be calculated.</p> required <code>trust_remote_code</code> <code>bool</code> <p>Whether to trust remote code when loading the model.</p> required <code>sampling_params</code> <code>Sampling_Params</code> <p>General sampling parameters to use (Sampling_Params Class).</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the backend LLM creation function.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the engine is not supported.</p> Source code in <code>pita/inference/LLM_backend.py</code> <pre><code>def __init__(\n    self,\n    engine: str,\n    model: str,\n    dtype: str,\n    tokenizer_path: str,\n    gpu_memory_utilization: float,\n    max_model_len: int,\n    max_probs: int,\n    logits_processor: bool,\n    trust_remote_code: bool,\n    sampling_params: Sampling_Params,\n    **kwargs: Any\n) -&gt; None:      \n\n    \"\"\"Create an AutoregressiveSampler object given the engine, engine parameters, and model name.\n\n    Args:\n        engine (str): Engine to use for autoregressive sampling. Currently only \"vllm\" and \"llama_cpp\" are supported.\n        model (str): Model to load.\n        dtype (str): Data type to use when loading the model. \"auto\" lets the engine decide.\n        tokenizer_path (str): Path to a model with a tokenizer if the model path doesn't include a tokenizer.\n        gpu_memory_utilization (float): GPU memory utilization to use.\n        max_model_len (int): Max model context length (context window = prompt + generated tokens).\n        max_probs (int): Number of top ranked probabilities (logits &amp; logprobs) to store per output token.\n        logits_processor (bool): Whether to enable the internal logits processor that allows for normalization constants and entropy to be calculated.\n        trust_remote_code (bool): Whether to trust remote code when loading the model.\n        sampling_params (Sampling_Params): General sampling parameters to use (Sampling_Params Class).\n        **kwargs: Additional keyword arguments passed to the backend LLM creation function.\n\n    Raises:\n        ValueError: If the engine is not supported.\n    \"\"\"\n    self.engine = engine\n    self.model = model\n\n    print(f\"Loading model {model} with {engine}...\")\n\n    # Separate Backend Loading for each engine\n    if(engine == \"vllm\"):\n        backend = _get_vllm_backend()\n\n        if(max_probs &gt; 0 and logits_processor == False):\n            print(\"max_probs is set but logits_processor is False. Setting logits_processor to True.\")\n            logits_processor = True\n\n        # Create the LLM object\n        self.llm = backend.create_LLM_object(\n            model_name = model, \n            dtype = dtype,\n            gpu_memory_utilization = gpu_memory_utilization,\n            max_model_len = max_model_len,\n            max_probs = max_probs,\n            logits_processor = logits_processor,\n            **kwargs\n        )  \n\n        # Set the autoregressive sampler function\n        self.sample_fn = backend.sample\n\n        # Create the engine parameters used for the completion function in vLLM\n        engine_params = backend.create_vllm_engine_params()\n\n        # Set the redis client for the LogitsLoggingProcessor\n        # Add the normalization_constants and normalization_constants_temp_scaled lists to extra_args\n        if(logits_processor):\n            print(\"Enabling logits processing in engine parameters extra_args.\")\n            engine_params.extra_args = {}\n            engine_params.extra_args[\"req_id\"] = \"my_request_\" + str(time.time())\n\n    elif(engine == \"llama_cpp\"):\n        backend = _get_llama_cpp_backend()\n        # Extract model_type from kwargs if provided, otherwise let backend infer it\n        llama_model_type = kwargs.pop('model_type', None)\n        # Create the LLM object\n        self.llm = backend.create_LLM_object(\n            model_name = model, \n            model_type = llama_model_type,\n            dtype = dtype, \n            gpu_memory_utilization = gpu_memory_utilization, \n            max_model_len = max_model_len,\n            max_logprobs = max_probs,\n            logits_processor = logits_processor,\n            **kwargs\n        )\n        # Set the autoregressive sampler function\n        self.sample_fn = backend.sample\n        # Llama.cpp does not have a separate engine params class\n        engine_params = None\n\n    elif(engine == \"tensorrt\"):\n        backend = _get_tensorrt_backend()\n        # Create the LLM object\n        self.llm = backend.create_LLM_object(\n            model_name = model, \n            dtype = dtype, \n            gpu_memory_utilization = gpu_memory_utilization, \n            max_model_len = max_model_len,\n            max_logprobs = max_probs,\n            logits_processor = logits_processor,\n            **kwargs\n        )\n        # Set the autoregressive sampler function\n        self.sample_fn = backend.sample\n        # TensorRT-LLM uses per-request engine params, create a default instance\n        engine_params = backend.create_tensorrt_engine_params()\n\n    else:\n        raise ValueError(f\"Engine {engine} not supported for Autoregressive Sampler. Supported engines are: 'vllm', 'llama_cpp', 'tensorrt'\")\n\n    # Create tokenizer depending on whether a tokenizer path is provided\n    # Needed as some models do not include the tokenizer files in the same repo as the model\n    if tokenizer_path is not None:\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, trust_remote_code=trust_remote_code)\n    else:\n        self.tokenizer = AutoTokenizer.from_pretrained(model, trust_remote_code=trust_remote_code)\n\n    print(\"Engine Params Extra Args:\", getattr(engine_params, \"extra_args\", \"N/A\") if engine_params is not None else \"N/A\")\n\n    # Intialize the Sampling Params\n    if(sampling_params is None):\n        self.sampling_params = Sampling_Params(\n            engine = engine, \n            engine_params = engine_params, \n            logprobs_per_token = max_probs,\n            logits_per_token = max_probs\n        )\n    else:\n        self.sampling_params = sampling_params\n\n    # Intialize the other test-time sampling parameters to Nones \n    self.chain_sampling = None\n    self.token_sampling = None\n</code></pre>"},{"location":"inference/LLM_backend/#pita.inference.LLM_backend.AutoregressiveSampler.chain_sample","title":"<code>chain_sample(context: str, **kwargs: Any) -&gt; Output</code>","text":"<p>Samples programmatically from the LLM using the chain sampling function</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>str</code> <p>The input context.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the chosen LLM Inference Engine.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Output</code> <code>Output</code> <p>The output of the sample function.</p> Source code in <code>pita/inference/LLM_backend.py</code> <pre><code>def chain_sample(self,\n    context: str,\n    **kwargs: Any\n)-&gt; Output:\n    \"\"\"Samples programmatically from the LLM using the chain sampling function\n\n    Args:\n        context (str): The input context.\n        **kwargs: Additional keyword arguments passed to the chosen LLM Inference Engine.\n\n    Returns:\n        Output: The output of the sample function.\n    \"\"\"\n    if getattr(self, \"chain_sample_name\", None) == \"SMC\" or getattr(self, \"chain_sample_name\", None) == \"Best-of-N\":\n        return self.chain_sample_fn(self, context, **kwargs)\n    else:\n        raise ValueError(\"Chain sampling is not enabled for this LLM/Engine.\")\n</code></pre>"},{"location":"inference/LLM_backend/#pita.inference.LLM_backend.AutoregressiveSampler.enable_power_sampling","title":"<code>enable_power_sampling(block_size: int, MCMC_steps: int, token_metric: str) -&gt; None</code>","text":"<p>Enables Power Sampling for the chosen LLM/Engine. Checks to see if the engine/LLM is compatible with Power Sampling by verifying that the token metric is supported/available to be used</p> <p>Parameters:</p> Name Type Description Default <code>block_size</code> <code>int</code> <p>Number of tokens to generate per step.</p> required <code>MCMC_steps</code> <code>int</code> <p>Number of MCMC steps to use for Power Sampling.</p> required <code>token_metric</code> <code>str</code> <p>Token metric to use to grade each particle. Can be logprobs, power_distribution, entropy, or PRM</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>pita/inference/LLM_backend.py</code> <pre><code>def enable_power_sampling(\n    self,\n    block_size: int,\n    MCMC_steps: int,\n    token_metric: str,\n)-&gt; None:\n    \"\"\"\n    Enables Power Sampling for the chosen LLM/Engine. Checks to see if the engine/LLM is compatible with Power Sampling by verifying that the token metric is supported/available to be used\n\n    Args:\n        block_size (int): Number of tokens to generate per step.\n        MCMC_steps (int): Number of MCMC steps to use for Power Sampling.\n        token_metric (str): Token metric to use to grade each particle. Can be logprobs, power_distribution, entropy, or PRM\n\n    Returns:\n        None\n    \"\"\"\n    # Check if chain sampling has already been enabled. If so replace it with Power Sampling.\n    if(self.token_sampling is not None):\n        print(\"Warning: Current Token Sampling Strategy is being replaced with Power Sampling.\")\n\n    # Check if the engine/LLM is set up for Power Sampling\n    if(token_metric == \"PRM\"):\n        raise ValueError(\"PRM is not supported YET for Power Sampling.\")\n    elif(token_metric == \"logprobs\" or token_metric == \"power_distribution\" or token_metric == \"entropy\"):\n        if(self.engine == \"vllm\"):\n            vllm_backend.check_token_metric_compatibility(self, token_metric)\n        elif(self.engine == \"llama_cpp\"):\n            llama_cpp_backend.check_token_metric_compatibility(self, token_metric)\n        elif(self.engine == \"tensorrt\"):\n            tensorrt_backend.check_token_metric_compatibility(self, token_metric)\n    else:\n        raise ValueError(f\"{token_metric} not supported for Power Sampling.\")\n\n    # Create the Power Sampling Class\n    from pita.sampling.power_sample import Power_Sampling\n    self.token_sampling = Power_Sampling(\n        block_size=block_size,\n        MCMC_steps=MCMC_steps,\n        token_metric=token_metric\n    )\n\n    # Set the token sampling function to the Power Sampling sample function\n    self.token_sample_fn = self.token_sampling.sample\n    self.token_sample_name = \"Power Sampling\"\n</code></pre>"},{"location":"inference/LLM_backend/#pita.inference.LLM_backend.AutoregressiveSampler.enable_smc","title":"<code>enable_smc(num_particles: int, tokens_per_step: int, stop_on_eos: bool, token_metric: str, aggregation: str) -&gt; None</code>","text":"<p>Enables SMC sampling for the chosen LLM/Engine.</p> <p>Parameters:</p> Name Type Description Default <code>num_particles</code> <code>int</code> <p>Number of particles to use for SMC.</p> required <code>tokens_per_step</code> <code>int</code> <p>Number of tokens to generate per step.</p> required <code>stop_on_eos</code> <code>bool</code> <p>(WIP)Whether to stop on end of sequence.</p> required <code>token_metric</code> <code>str</code> <p>Token metric to use to grade each particle. Can be logprobs, power_distribution, entropy, or PRM</p> required <code>aggregation</code> <code>str</code> <p>Aggregation method of the scores of each particle. Can be the last, minimum, product, or model_aggregate.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>pita/inference/LLM_backend.py</code> <pre><code>def enable_smc(\n    self,\n    num_particles: int,\n    tokens_per_step: int,\n    stop_on_eos: bool,\n    token_metric: str,  \n    aggregation: str\n)-&gt; None:\n    \"\"\"\n    Enables SMC sampling for the chosen LLM/Engine.\n\n    Args:\n        num_particles (int): Number of particles to use for SMC.\n        tokens_per_step (int): Number of tokens to generate per step.\n        stop_on_eos (bool): (WIP)Whether to stop on end of sequence.\n        token_metric (str): Token metric to use to grade each particle. Can be logprobs, power_distribution, entropy, or PRM\n        aggregation (str): Aggregation method of the scores of each particle. Can be the last, minimum, product, or model_aggregate.\n\n    Returns:\n        None\n    \"\"\"\n    # Check if chain sampling has already been enabled. If so replace it with SMC.\n    if(self.chain_sampling is not None):\n        print(\"Warning: Current Chain Sampling Strategy is being replaced with SMC.\")\n\n    # Check if the engine/LLM is set up for SMC\n    if(token_metric == \"PRM\"):\n        raise ValueError(\"PRM is not supported YET for SMC.\")\n    elif(token_metric == \"logprobs\" or token_metric == \"power_distribution\" or token_metric == \"entropy\"):\n        if(self.engine == \"vllm\"):\n            vllm_backend.check_token_metric_compatibility(self, token_metric)\n        elif(self.engine == \"llama_cpp\"):\n            llama_cpp_backend.check_token_metric_compatibility(self, token_metric)\n        elif(self.engine == \"tensorrt\"):\n            tensorrt_backend.check_token_metric_compatibility(self, token_metric)\n    else:\n        raise ValueError(f\"{token_metric} not supported for SMC.\")\n\n    # Check if the aggregation method is supported\n    if(aggregation == \"last\" or aggregation == \"minimum\" or aggregation == \"product\" or aggregation == \"model_aggregate\"):\n        pass\n    else:\n        raise ValueError(f\"{aggregation} not supported for SMC.\")\n\n    # Create the SMC Class\n    from pita.sampling.smc import Sequential_Monte_Carlo\n    self.chain_sampling = Sequential_Monte_Carlo(\n        num_particles=num_particles,\n        tokens_per_step=tokens_per_step,\n        stop_on_eos=stop_on_eos,\n        token_metric=token_metric,\n        aggregation=aggregation\n    )\n\n    # Set the chain sampling function to the SMC sample function\n    self.chain_sample_fn = self.chain_sampling.sample\n    self.chain_sample_name = \"SMC\"\n</code></pre>"},{"location":"inference/LLM_backend/#pita.inference.LLM_backend.AutoregressiveSampler.sample","title":"<code>sample(context: str, **kwargs: Any) -&gt; Output</code>","text":"<p>Samples programmatically from the LLM given a context and max new tokens. Sample function is the engine_backend.sample function.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>str</code> <p>The input context.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the chosen LLM Inference Engine.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Output</code> <code>Output</code> <p>The output of the sample function.</p> Source code in <code>pita/inference/LLM_backend.py</code> <pre><code>def sample(self,\n    context: str,\n    **kwargs: Any\n)-&gt; Output:\n    \"\"\"Samples programmatically from the LLM given a context and max new tokens. Sample function is the engine_backend.sample function.\n\n    Args:\n        context (str): The input context.\n        **kwargs: Additional keyword arguments passed to the chosen LLM Inference Engine.\n\n    Returns:\n        Output: The output of the sample function.\n    \"\"\"\n    return self.sample_fn(self, context, **kwargs)\n</code></pre>"},{"location":"inference/LLM_backend/#pita.inference.LLM_backend.AutoregressiveSampler.token_sample","title":"<code>token_sample(context: str, **kwargs: Any) -&gt; Output</code>","text":"<p>Samples programmatically from the LLM using the token sampling function</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>str</code> <p>The input context.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the chosen LLM Inference Engine.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Output</code> <code>Output</code> <p>The output of the sample function.</p> Source code in <code>pita/inference/LLM_backend.py</code> <pre><code>def token_sample(self,\n    context: str,\n    **kwargs: Any\n)-&gt; Output:\n    \"\"\"Samples programmatically from the LLM using the token sampling function\n\n    Args:\n        context (str): The input context.\n        **kwargs: Additional keyword arguments passed to the chosen LLM Inference Engine.\n\n    Returns:\n        Output: The output of the sample function.\n    \"\"\"\n    if getattr(self, \"token_sample_name\", None) == \"Power Sampling\":\n        return self.token_sample_fn(self, context, **kwargs)\n    else:\n        raise ValueError(\"Token sampling is not enabled for this LLM/Engine.\")\n</code></pre>"},{"location":"inference/LLM_backend/#pita.inference.LLM_backend.Output","title":"<code>Output</code>","text":"<p>Output object for any LLM sampling.</p> <p>Attributes:</p> Name Type Description <code>tokens</code> <code>list[int] | list[list[int]]</code> <p>The generated token IDs.</p> <code>top_k_logits</code> <code>list[float] | list[list[float]] | None</code> <p>The top_k logits (if logits_per_token is set). First value is always the chosen token logit.</p> <code>top_k_logprobs</code> <code>list[float] | list[list[float]] | None</code> <p>The top_k logprobs (if logprobs is set). First value is always the chosen token logprob.</p> <code>unprocessed_log_normalization_constant</code> <code>list[float] | list[list[float]]</code> <p>The log(Normalization Constants - Unprocessed) for each token.</p> <code>temp_processed_log_normalization_constant</code> <code>list[float] | list[list[float]]</code> <p>The log(Normalization Constants - Temperature Processed) for each token.</p> <code>entropy</code> <code>list[float] | list[list[float]]</code> <p>The entropy for each token.</p> Source code in <code>pita/inference/LLM_backend.py</code> <pre><code>class Output:\n    \"\"\" Output object for any LLM sampling.\n\n    Attributes:\n        tokens (list[int] | list[list[int]]): The generated token IDs.\n        top_k_logits (list[float] | list[list[float]] | None): The top_k logits (if logits_per_token is set). First value is always the chosen token logit.\n        top_k_logprobs (list[float] | list[list[float]] | None): The top_k logprobs (if logprobs is set). First value is always the chosen token logprob.\n        unprocessed_log_normalization_constant (list[float] | list[list[float]]): The log(Normalization Constants - Unprocessed) for each token.\n        temp_processed_log_normalization_constant (list[float] | list[list[float]]): The log(Normalization Constants - Temperature Processed) for each token.\n        entropy (list[float] | list[list[float]]): The entropy for each token.\n    \"\"\"\n    def __init__(\n        self,\n        tokens: list[int] | list[list[int]] = None,\n        top_k_logits: list[float] | list[list[float]] | None = None,\n        top_k_logprobs: list[float] | list[list[float]] | None = None,\n        unprocessed_log_normalization_constant: list[float] | list[list[float]] = None,\n        temp_processed_log_normalization_constant: list[float] | list[list[float]] = None,\n        entropy: list[float] | list[list[float]] = None,\n    ):\n        self.tokens = tokens\n        self.top_k_logits = top_k_logits\n        self.top_k_logprobs = top_k_logprobs\n        self.unprocessed_log_normalization_constant = unprocessed_log_normalization_constant\n        self.temp_processed_log_normalization_constant = temp_processed_log_normalization_constant\n        self.entropy = entropy\n\n    def append(self, other: 'Output'):\n        \"\"\"\n        Appends the data from another Output object to this one by extending internal lists.\n\n        Args:\n            other (Output): The other output object to append.\n        \"\"\"\n        if other is None:\n            return\n\n        # Helper function to extend list attributes safely\n        def _extend_field(field_name):\n            self_val = getattr(self, field_name)\n            other_val = getattr(other, field_name)\n\n            if other_val is not None:\n                if self_val is None:\n                    # Use deepcopy for consistency\n                    setattr(self, field_name, copy.deepcopy(other_val) if isinstance(other_val, list) else other_val)\n                elif isinstance(self_val, list) and isinstance(other_val, list):\n                    self_val.extend(other_val)\n\n        _extend_field('tokens')\n        _extend_field('top_k_logits')\n        _extend_field('top_k_logprobs')\n        _extend_field('unprocessed_log_normalization_constant')\n        _extend_field('temp_processed_log_normalization_constant')\n        _extend_field('entropy')\n</code></pre>"},{"location":"inference/LLM_backend/#pita.inference.LLM_backend.Output.append","title":"<code>append(other: Output)</code>","text":"<p>Appends the data from another Output object to this one by extending internal lists.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Output</code> <p>The other output object to append.</p> required Source code in <code>pita/inference/LLM_backend.py</code> <pre><code>def append(self, other: 'Output'):\n    \"\"\"\n    Appends the data from another Output object to this one by extending internal lists.\n\n    Args:\n        other (Output): The other output object to append.\n    \"\"\"\n    if other is None:\n        return\n\n    # Helper function to extend list attributes safely\n    def _extend_field(field_name):\n        self_val = getattr(self, field_name)\n        other_val = getattr(other, field_name)\n\n        if other_val is not None:\n            if self_val is None:\n                # Use deepcopy for consistency\n                setattr(self, field_name, copy.deepcopy(other_val) if isinstance(other_val, list) else other_val)\n            elif isinstance(self_val, list) and isinstance(other_val, list):\n                self_val.extend(other_val)\n\n    _extend_field('tokens')\n    _extend_field('top_k_logits')\n    _extend_field('top_k_logprobs')\n    _extend_field('unprocessed_log_normalization_constant')\n    _extend_field('temp_processed_log_normalization_constant')\n    _extend_field('entropy')\n</code></pre>"},{"location":"inference/LLM_backend/#pita.inference.LLM_backend.Sampling_Params","title":"<code>Sampling_Params</code>","text":"<p>Sampling parameters used for generating results from the LLM. Generalized across all engines. Changes to this class should be reflected in the engine specific parameter classes.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>str</code> <p>Engine name (e.g., \"vllm\", \"transformers\", etc.).</p> <code>None</code> <code>engine_params</code> <code>object</code> <p>Engine specific parameter Class (vLLM: SamplingParams, llama.cpp: None).</p> <code>None</code> <code>enable_thinking</code> <code>bool</code> <p>Whether to enable thinking.</p> <code>False</code> <code>max_tokens</code> <code>int</code> <p>Max Number of tokens to generate per sequence.</p> <code>16</code> <code>temperature</code> <code>float</code> <p>Controls randomness of sampling. Lower is more deterministic, higher is more random.</p> <code>1.0</code> <code>top_p</code> <code>float</code> <p>Controls tokens to consider based on cumulative probability. Must be in (0, 1].</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>Controls number of top tokens to consider. 0 considers all tokens.</p> <code>0</code> <code>logprobs_per_token</code> <code>int</code> <p>Number of logprobs to return per output token. logprobs+1 token returned (includes chosen token).</p> <code>None</code> <code>logits_per_token</code> <code>int</code> <p>Number of descending ranked logits to return per output token.</p> <code>None</code> <code>presence_penalty</code> <code>float</code> <p>Penalizes new tokens based on appearance in generated text so far. &gt; 0 encourages new tokens, &lt; 0 encourages repeats.</p> <code>0.0</code> <code>frequency_penalty</code> <code>float</code> <p>Penalizes new tokens based on frequency in generated text so far. &gt; 0 encourages new tokens, &lt; 0 encourages repeats.</p> <code>0.0</code> <code>repetition_penalty</code> <code>float</code> <p>Penalizes new tokens based on appearance in prompt AND generated text so far. &gt; 1 encourages new tokens, &lt; 1 encourages repeats.</p> <code>1.0</code> <code>min_p</code> <code>float</code> <p>Represents the minimum probability for a token to be considered. 0 disables.</p> <code>0.0</code> <code>seed</code> <code>int</code> <p>Random seed.</p> <code>None</code> <code>stop</code> <code>list[str]</code> <p>Strings that stop token generation. Returned output excludes stop strings.</p> <code>None</code> <code>stop_token_ids</code> <code>list[int]</code> <p>Token IDs that stop token generation. Returned output excludes stop tokens.</p> <code>None</code> <code>ignore_eos</code> <code>bool</code> <p>Continues generating tokens after EOS token is generated.</p> <code>False</code> <code>min_tokens</code> <code>int</code> <p>Minimum Number of tokens to generate per sequence before EOS or stop is considered.</p> <code>0</code> <code>enable_normalization_constants</code> <code>bool</code> <p>Whether to enable normalization constants.</p> <code>False</code> <code>enable_entropy</code> <code>bool</code> <p>Whether to enable entropy.</p> <code>False</code> Source code in <code>pita/inference/LLM_backend.py</code> <pre><code>class Sampling_Params:\n    \"\"\"Sampling parameters used for generating results from the LLM. Generalized across all engines. Changes to this class should be reflected in the engine specific parameter classes.\n\n    Args:\n        engine (str): Engine name (e.g., \"vllm\", \"transformers\", etc.).\n        engine_params (object): Engine specific parameter Class (vLLM: SamplingParams, llama.cpp: None).\n        enable_thinking (bool): Whether to enable thinking.\n        max_tokens (int): Max Number of tokens to generate per sequence.\n        temperature (float): Controls randomness of sampling. Lower is more deterministic, higher is more random.\n        top_p (float): Controls tokens to consider based on cumulative probability. Must be in (0, 1].\n        top_k (int): Controls number of top tokens to consider. 0 considers all tokens.\n        logprobs_per_token (int): Number of logprobs to return per output token. logprobs+1 token returned (includes chosen token).\n        logits_per_token (int): Number of descending ranked logits to return per output token.\n        presence_penalty (float): Penalizes new tokens based on appearance in generated text so far. &gt; 0 encourages new tokens, &lt; 0 encourages repeats.\n        frequency_penalty (float): Penalizes new tokens based on frequency in generated text so far. &gt; 0 encourages new tokens, &lt; 0 encourages repeats.\n        repetition_penalty (float): Penalizes new tokens based on appearance in prompt AND generated text so far. &gt; 1 encourages new tokens, &lt; 1 encourages repeats.\n        min_p (float): Represents the minimum probability for a token to be considered. 0 disables.\n        seed (int): Random seed.\n        stop (list[str]): Strings that stop token generation. Returned output excludes stop strings.\n        stop_token_ids (list[int]): Token IDs that stop token generation. Returned output excludes stop tokens.\n        ignore_eos (bool): Continues generating tokens after EOS token is generated.\n        min_tokens (int): Minimum Number of tokens to generate per sequence before EOS or stop is considered.\n        enable_normalization_constants (bool): Whether to enable normalization constants.\n        enable_entropy (bool): Whether to enable entropy.\n    \"\"\"\n    def __init__(\n        self,\n        engine: str = None,\n        engine_params: object = None,\n        enable_thinking: bool = False,\n        max_tokens: int = 16,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        top_k: int = 0,\n        logprobs_per_token: int = None,\n        logits_per_token: int = None,\n        presence_penalty: float = 0.0,\n        frequency_penalty: float = 0.0,\n        repetition_penalty: float = 1.0,\n        min_p: float = 0.0,\n        seed: int = None,\n        stop: list[str] = None,\n        stop_token_ids: list[int] = None,\n        ignore_eos: bool = False,\n        min_tokens: int = 0,\n        enable_normalization_constants: bool = False,\n        enable_entropy: bool = False\n    ):  \n        object.__setattr__(self, 'engine', engine)\n        object.__setattr__(self, 'engine_params', engine_params)\n        object.__setattr__(self, 'enable_thinking', enable_thinking)\n        object.__setattr__(self, 'max_tokens', max_tokens)\n        object.__setattr__(self, 'temperature', temperature)\n        object.__setattr__(self, 'top_p', top_p)\n        object.__setattr__(self, 'top_k', top_k)\n        object.__setattr__(self, 'logprobs_per_token', logprobs_per_token)\n        object.__setattr__(self, 'logits_per_token', logits_per_token)\n        object.__setattr__(self, 'presence_penalty', presence_penalty)\n        object.__setattr__(self, 'frequency_penalty', frequency_penalty)\n        object.__setattr__(self, 'repetition_penalty', repetition_penalty)\n        object.__setattr__(self, 'min_p', min_p)\n        object.__setattr__(self, 'seed', seed)\n        object.__setattr__(self, 'stop', stop)\n        object.__setattr__(self, 'stop_token_ids', stop_token_ids)\n        object.__setattr__(self, 'ignore_eos', ignore_eos)\n        object.__setattr__(self, 'min_tokens', min_tokens)\n        object.__setattr__(self, 'enable_normalization_constants', enable_normalization_constants)\n        object.__setattr__(self, 'enable_entropy', enable_entropy)\n\n        # Sync all parameters to engine_params after initialization\n        if engine is not None and engine_params is not None:\n            for param_name in ['max_tokens', 'temperature', 'top_p', 'top_k', 'logprobs_per_token', 'logits_per_token',\n                               'presence_penalty', 'frequency_penalty', 'repetition_penalty',\n                               'min_p', 'seed', 'stop', 'stop_token_ids', 'ignore_eos', 'min_tokens']:\n                self._sync_param_to_engine(param_name, getattr(self, param_name))\n\n\n    def __setattr__(self, name, value):\n        # Also sync to engine_params if it exists\n        super().__setattr__(name, value)\n\n        # If attribute is dependent on a Logits Processor, makes sure to propagate the change\n        if(self.engine == \"vllm\"):\n            if(name == \"enable_normalization_constants\"):\n                self.engine_params.extra_args[\"normalization_constants\"] = value\n                return\n            elif(name == \"enable_entropy\"):\n                self.engine_params.extra_args[\"entropy\"] = value\n                return\n\n        self._sync_param_to_engine(name, value)\n\n\n    def _sync_param_to_engine(self, param_name, value):\n        # Skip syncing for llama_cpp as it does not use a separate engine_params class\n        if self.engine == \"llama_cpp\":\n            return\n\n        \"\"\"Sync a single parameter to engine_params\"\"\"\n        if not hasattr(self, 'engine') or self.engine is None:\n            raise ValueError(\"Engine must be set in Sampling_Params to sync parameters to engine_params.\")\n\n        if self.engine_params is None:\n            raise ValueError(\"engine_params Class must be set in Sampling_Params to sync parameters to engine_params.\")\n\n        # Check if engine is vLLM and logprobs/logits are being changed\n        if self.engine == \"vllm\":\n            if(param_name == \"logprobs_per_token\"):\n                if(value &lt; self.logits_per_token):\n                    # Do not overwrite the vLLM engine parameter \"logprobs\" as logits_per_token will fail\n                    return\n            if(param_name == \"logits_per_token\"):\n                if(value &lt; self.logprobs_per_token):\n                    # Do not overwrite the vLLM engine parameter \"logits_per_token\" as logprobs_per_token will fail\n                    return\n\n        # Handle tensorrt-specific top_k value conversion\n        # TensorRT-LLM requires top_k &gt;= 0, where 0 means \"consider all tokens\"\n        # Other backends like vLLM use -1 to mean the same thing\n        if self.engine == \"tensorrt\" and param_name == \"top_k\" and value == -1:\n            value = 0\n\n        # Sync logic here\n        engine_map = ENGINE_PARAM_MAPS.get(self.engine, {})\n        engine_param_name = engine_map.get(param_name)\n        # If the engine supports this parameter, set it\n        if engine_param_name is not None:\n            setattr(self.engine_params, engine_param_name, value)\n</code></pre>"},{"location":"inference/llama_cpp_backend/","title":"Llama CPP Backend","text":""},{"location":"inference/llama_cpp_backend/#pita.inference.llama_cpp_backend.check_token_metric_compatibility","title":"<code>check_token_metric_compatibility(sampler: Any, token_metric: str) -&gt; None</code>","text":"<p>Check that the llama.cpp engine can support the given token metric with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Any</code> <p>The sampler object containing sampling parameters and the LLM engine.</p> required <code>token_metric</code> <code>str</code> <p>The token metric to check compatibility for.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the configuration doesn't support the requested token metric.</p> Source code in <code>pita/inference/llama_cpp_backend.py</code> <pre><code>def check_token_metric_compatibility(sampler: Any, token_metric: str) -&gt; None:\n    \"\"\"\n    Check that the llama.cpp engine can support the given token metric with the given configuration.\n\n    Args:\n        sampler: The sampler object containing sampling parameters and the LLM engine.\n        token_metric: The token metric to check compatibility for.\n\n    Raises:\n        ValueError: If the configuration doesn't support the requested token metric.\n    \"\"\"\n    if token_metric == \"logprobs\":\n        # logprobs requires logits_per_token to be set\n        if sampler.sampling_params.logits_per_token is None or sampler.sampling_params.logits_per_token &lt; 1:\n            raise ValueError(\n                \"logits_per_token must be set to at least 1 to use 'logprobs' token metric with llama.cpp backend.\"\n            )\n        # Enable normalization constants for logprobs calculation\n        sampler.sampling_params.enable_normalization_constants = True\n        print(\"Enabled normalization constants in sampling params for logprobs metric.\")\n\n    elif token_metric == \"power_distribution\":\n        # power_distribution requires normalization constants\n        if sampler.sampling_params.logits_per_token is None or sampler.sampling_params.logits_per_token &lt; 1:\n            raise ValueError(\n                \"logits_per_token must be set to at least 1 to use 'power_distribution' token metric with llama.cpp backend.\"\n            )\n        # Enable normalization constants\n        sampler.sampling_params.enable_normalization_constants = True\n        print(\"Enabled normalization constants in sampling params for power_distribution metric.\")\n\n    elif token_metric == \"entropy\":\n        # entropy requires the entropy calculation to be enabled\n        if sampler.sampling_params.logits_per_token is None or sampler.sampling_params.logits_per_token &lt; 1:\n            raise ValueError(\n                \"logits_per_token must be set to at least 1 to use 'entropy' token metric with llama.cpp backend.\"\n            )\n        # Enable entropy calculation\n        sampler.sampling_params.enable_entropy = True\n        print(\"Enabled entropy calculation in sampling params for entropy metric.\")\n\n    elif token_metric == \"likelihood_confidence\":\n        # likelihood_confidence requires logprobs\n        if sampler.sampling_params.logits_per_token is None or sampler.sampling_params.logits_per_token &lt; 1:\n            raise ValueError(\n                \"logits_per_token must be set to at least 1 to use 'likelihood_confidence' token metric with llama.cpp backend.\"\n            )\n        sampler.sampling_params.enable_normalization_constants = True\n        print(\"Enabled normalization constants in sampling params for likelihood_confidence metric.\")\n    else:\n        raise ValueError(f\"Unknown token metric: {token_metric}\")\n</code></pre>"},{"location":"inference/llama_cpp_backend/#pita.inference.llama_cpp_backend.create_LLM_object","title":"<code>create_LLM_object(model_name: str, model_type: str | None = None, dtype: str = 'auto', gpu_memory_utilization: float = 0.85, max_model_len: int = 2048, max_logprobs: int | None = None, logits_processor: bool = False, **kwargs: Any) -&gt; Llama</code>","text":"<p>Create the LLM object given the model name and engine parameters.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to load (Hugging Face repo ID for GGUF models).</p> required <code>model_type</code> <code>str</code> <p>The type of model. Inferred from model_name if not provided. Currently only 'gguf' is supported.</p> <code>None</code> <code>dtype</code> <code>str</code> <p>The data type/quantization to use. Defaults to \"auto\" (f16).</p> <code>'auto'</code> <code>gpu_memory_utilization</code> <code>float</code> <p>The fraction of GPU memory to use. Defaults to 0.85.</p> <code>0.85</code> <code>max_model_len</code> <code>int</code> <p>The maximum context length. Defaults to 2048.</p> <code>2048</code> <code>max_logprobs</code> <code>int</code> <p>Unused for llama.cpp, kept for API compatibility.</p> <code>None</code> <code>logits_processor</code> <code>bool</code> <p>Whether logits processing is enabled.  When True, scores are available via llm.scores. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the Llama constructor.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Llama</code> <code>Llama</code> <p>The initialized llama.cpp Llama object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model_type is 'safetensors' (not supported) or unsupported.</p> Source code in <code>pita/inference/llama_cpp_backend.py</code> <pre><code>def create_LLM_object(\n        model_name: str,\n        model_type: str | None = None,\n        dtype: str = \"auto\",\n        gpu_memory_utilization: float = 0.85,\n        max_model_len: int = 2048,\n        max_logprobs: int | None = None,\n        logits_processor: bool = False,\n        **kwargs: Any\n    ) -&gt; Llama:\n    \"\"\"\n    Create the LLM object given the model name and engine parameters.\n\n    Args:\n        model_name (str): The name of the model to load (Hugging Face repo ID for GGUF models).\n        model_type (str, optional): The type of model. Inferred from model_name if not provided.\n            Currently only 'gguf' is supported.\n        dtype (str, optional): The data type/quantization to use. Defaults to \"auto\" (f16).\n        gpu_memory_utilization (float, optional): The fraction of GPU memory to use. Defaults to 0.85.\n        max_model_len (int, optional): The maximum context length. Defaults to 2048.\n        max_logprobs (int, optional): Unused for llama.cpp, kept for API compatibility.\n        logits_processor (bool, optional): Whether logits processing is enabled. \n            When True, scores are available via llm.scores. Defaults to False.\n        **kwargs: Additional keyword arguments passed to the Llama constructor.\n\n    Returns:\n        Llama: The initialized llama.cpp Llama object.\n\n    Raises:\n        ValueError: If model_type is 'safetensors' (not supported) or unsupported.\n    \"\"\"\n    # Infer model_type from model_name if not provided\n    if model_type is None:\n        # Check if model name contains common GGUF indicators\n        if 'gguf' in model_name.lower() or model_name.endswith('.gguf'):\n            model_type = \"gguf\"\n        else:\n            # Default to gguf for llama.cpp\n            model_type = \"gguf\"\n            print(f\"Warning: model_type not specified, defaulting to 'gguf' for llama.cpp backend.\")\n\n    if model_type == \"gguf\":\n        # Find the correct dtype in the GGUF Hugging Face Repo\n        if dtype == \"auto\":\n            kwargs['filename'] = \"*f16*\"\n        else:\n            kwargs['filename'] = f\"*{dtype}*\"\n    elif model_type == \"safetensors\":\n        raise ValueError(\"safetensors model type is not currently supported in llama.cpp backend. Please use gguf model type.\")\n    else:\n        raise ValueError(f\"{model_type} is an unsupported model type. Supported types are 'gguf'.\")\n\n    # Check to see if the user wants to use the GPU\n    if gpu_memory_utilization &gt; 0 and 'n_gpu_layers' not in kwargs:\n        kwargs['n_gpu_layers'] = -1  # Use as many GPU layers as possible\n\n    # Get the System VRAM\n    total_vram_mb = get_total_vram()\n\n    # Get the VRAM usage before loading the model\n    vram_before = get_gpu_vram_usage_mb() or 0\n\n    # Determine if we need logits_all\n    # With logits processor, we don't need logits_all=True as the processor captures what we need\n    # However, for compatibility with direct scores access, we may still want it\n    logits_all = logits_processor\n\n    # Initialize LLaMA.cpp locally\n    llm = Llama.from_pretrained(\n        repo_id=model_name,\n        n_ctx=max_model_len,\n        logits_all=logits_all,\n        **kwargs\n    )\n\n    # Get the VRAM used to load the model\n    vram_after = get_gpu_vram_usage_mb() or 0\n    vram_mb = vram_after - vram_before\n\n    if vram_mb &lt; 1:\n        print(\"Warning: Could not extract VRAM usage from llama.cpp logs. Model may be loaded into CPU RAM. Proceeding without VRAM check.\")    \n    else:\n        try:\n            total_vram_int = int(total_vram_mb)\n            vram_mb_int = int(vram_mb)\n        except (ValueError, TypeError):\n            print(f\"Warning: Could not extract total VRAM value ('{total_vram_mb}'). Skipping VRAM utilization check.\")\n        else:\n            if vram_mb_int / total_vram_int &gt; gpu_memory_utilization:\n                raise ValueError(\n                    \"VRAM usage exceeds the specified GPU memory utilization threshold.\\n\"\n                    \"Options to Reduce VRAM:\\n\"\n                    \"1. Reduce the context size (n_ctx parameter)\\n\"\n                    \"2. Turn off GPU KV-caching with kwarg: offload_kqv = True\\n\"\n                    \"3. Load only 'N' layers to the GPU kwarg: n_gpu_layers = N\\n\"\n                )\n            else:\n                print(f\"VRAM Usage for Model Load: {vram_mb_int} MiB / {total_vram_int} MiB ({(vram_mb_int/total_vram_int)*100:.2f} %)\")\n\n    print(\"--- Model Initialization Complete. ---\")\n\n    # Return created LLM object\n    return llm\n</code></pre>"},{"location":"inference/llama_cpp_backend/#pita.inference.llama_cpp_backend.sample","title":"<code>sample(self, context: str | list[str], **kwargs: Any) -&gt; Output</code>","text":"<p>Generate text from the given context using the llama.cpp backend.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>str | list[str]</code> <p>The input context string to generate from.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the underlying llama.cpp generation function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Output</code> <code>Output</code> <p>An Output object containing: - tokens: The generated token IDs. - top_k_logits: The top_k logits (if logits_per_token is set). - top_k_logprobs: The top_k logprobs (if logprobs_per_token is set). - unprocessed_log_normalization_constant: The log normalization constants for each token. - temp_processed_log_normalization_constant: The temperature-scaled log normalization constants. - entropy: The entropy for each token.</p> Source code in <code>pita/inference/llama_cpp_backend.py</code> <pre><code>def sample(\n        self,\n        context: str | list[str],\n        **kwargs: Any\n    ) -&gt; Output:\n    \"\"\"\n    Generate text from the given context using the llama.cpp backend.\n\n    Args:\n        context (str | list[str]): The input context string to generate from.\n        **kwargs: Additional keyword arguments passed to the underlying llama.cpp generation function.\n\n    Returns:\n        Output: An Output object containing:\n            - tokens: The generated token IDs.\n            - top_k_logits: The top_k logits (if logits_per_token is set).\n            - top_k_logprobs: The top_k logprobs (if logprobs_per_token is set).\n            - unprocessed_log_normalization_constant: The log normalization constants for each token.\n            - temp_processed_log_normalization_constant: The temperature-scaled log normalization constants.\n            - entropy: The entropy for each token.\n    \"\"\"\n    # Determine if we need normalization constants or entropy\n    calculate_normalization = getattr(self.sampling_params, 'enable_normalization_constants', False)\n    calculate_entropy = getattr(self.sampling_params, 'enable_entropy', False)\n\n    # Create a fresh logits processor for this sample call\n    logits_processor_list, logits_processor = create_logits_processor_list(\n        temperature=self.sampling_params.temperature,\n        calculate_normalization=calculate_normalization,\n        calculate_entropy=calculate_entropy\n    )\n\n    # Check if context is a list of strings or a single string\n    if isinstance(context, list):\n        context_list_len = len(context)\n    else:\n        context_list_len = 1\n        context = [context]  # Normalize to list for uniform handling\n\n    # For batch processing, we'd need to handle multiple contexts\n    # Currently llama.cpp doesn't support true batching, so we process sequentially\n    all_outputs = []\n\n    for context_input in context:\n        # Reset the LLM state for a fresh start with each context\n        self.llm.reset()\n        logits_processor.reset()\n\n        # Use generate() to extract the token_ids instead of create_completion\n        if isinstance(context_input, str):\n            prompt_tokens = self.llm.tokenize(context_input.encode('utf-8'))\n        else:\n            prompt_tokens = context_input\n\n        tokens = []\n        top_k_logits = []\n        logits_per_token = self.sampling_params.logits_per_token or 0\n\n        if len(prompt_tokens) &gt; 0:\n            self.llm.eval(prompt_tokens)\n\n        # Generation loop\n        generator = self.llm.generate(\n            [],\n            top_k=self.sampling_params.top_k,\n            top_p=self.sampling_params.top_p,\n            min_p=self.sampling_params.min_p,\n            temp=self.sampling_params.temperature,\n            repeat_penalty=self.sampling_params.repetition_penalty,\n            frequency_penalty=self.sampling_params.frequency_penalty,\n            presence_penalty=self.sampling_params.presence_penalty,\n            logits_processor=logits_processor_list,\n            reset=False,\n            **kwargs\n        )\n\n        for token in generator:\n            # For each token, self.llm.scores[self.llm.n_tokens - 1] contains the logits\n            # that were used to sample it.\n            current_logits = self.llm.scores[self.llm.n_tokens - 1, :]\n\n            if logits_per_token &gt; 0:\n                # Extract logits for the current step.\n                # We always place the chosen token's logit first, then fill with the\n                # highest remaining logits until we reach logits_per_token elements\n                # or run out of logits.\n\n                # Use argpartition to find top logits efficiently (O(N) instead of O(N log N))\n                if len(current_logits) &gt; logits_per_token:\n                    # Get indices of top logits_per_token elements\n                    # We might need logits_per_token elements to fill the list if the chosen token isn't in top K\n                    top_indices = np.argpartition(current_logits, -logits_per_token)[-logits_per_token:]\n                    # Sort only these top elements\n                    sorted_short_indices = np.argsort(current_logits[top_indices])[::-1]\n                    sorted_indices = top_indices[sorted_short_indices]\n                else:\n                    sorted_indices = np.argsort(current_logits)[::-1]\n\n                # Ensure the chosen token logit is first as requested\n                step_logits = [float(current_logits[token])]\n\n                for idx in sorted_indices:\n                    if idx == token:\n                        continue\n                    if len(step_logits) &gt;= logits_per_token:\n                        break\n                    step_logits.append(float(current_logits[idx]))\n                top_k_logits.append(step_logits)\n\n            tokens.append(int(token))\n\n            # Check stopping criteria\n            if len(tokens) &gt;= self.sampling_params.max_tokens:\n                break\n            if token == self.llm.token_eos():\n                break\n            if self.sampling_params.stop_token_ids and token in self.sampling_params.stop_token_ids:\n                break\n\n        # Find the token count from the token_ids\n        token_count = len(tokens)\n\n        # We only trim data from the logits processor as it is the only source that is guaranteed to have the wrong length\n        unprocessed_log_normalization_constant = logits_processor.log_norm_constants[:token_count]\n        temp_processed_log_normalization_constant = logits_processor.log_norm_constants_temp_scaled[:token_count]\n        entropy = logits_processor.entropy[:token_count]\n\n        # Use the temp_processed_log_normalization_constant to calculate the logprobs\n        top_k_logprobs = []\n        logprobs_per_token = self.sampling_params.logprobs_per_token or 0\n        if logprobs_per_token &gt; 0 and top_k_logits:\n            for i in range(token_count):\n                logits_row = np.array(top_k_logits[i])\n                temp_norm = temp_processed_log_normalization_constant[i]\n                # logprob = (logit / temp) - logsumexp(logits / temp)\n                row_logprobs = (logits_row / self.sampling_params.temperature) - temp_norm\n                # Slice to the requested logprobs amount\n                top_k_logprobs.append(row_logprobs[:logprobs_per_token].tolist())\n        else:\n            top_k_logprobs = [[]] * token_count\n\n        if not top_k_logits:\n            top_k_logits = [[]] * token_count\n\n        output = Output(\n            tokens=tokens,\n            top_k_logits=top_k_logits,\n            top_k_logprobs=top_k_logprobs,\n            unprocessed_log_normalization_constant=unprocessed_log_normalization_constant,\n            temp_processed_log_normalization_constant=temp_processed_log_normalization_constant,\n            entropy=entropy\n        )\n        all_outputs.append(output)\n\n    # If only one context was provided, return single Output\n    if context_list_len == 1:\n        return all_outputs[0]\n\n    # For multiple contexts, combine into a single Output with lists of lists\n    # This matches the vLLM batch behavior\n    combined = Output(\n        tokens=[o.tokens for o in all_outputs],\n        top_k_logits=[o.top_k_logits for o in all_outputs],\n        top_k_logprobs=[o.top_k_logprobs for o in all_outputs],\n        unprocessed_log_normalization_constant=[o.unprocessed_log_normalization_constant for o in all_outputs],\n        temp_processed_log_normalization_constant=[o.temp_processed_log_normalization_constant for o in all_outputs],\n        entropy=[o.entropy for o in all_outputs]\n    )\n    return combined\n</code></pre>"},{"location":"inference/tensorRT_backend/","title":"TensorRT Backend","text":"<p>TensorRT-LLM Backend for PITA inference framework.</p> <p>This module provides functions to create and use TensorRT-LLM for text generation, following the same pattern as vllm_backend.py and llama_cpp_backend.py.</p>"},{"location":"inference/tensorRT_backend/#pita.inference.tensorRT_backend.check_token_metric_compatibility","title":"<code>check_token_metric_compatibility(sampler: Any, token_metric: str) -&gt; None</code>","text":"<p>Check that the TensorRT-LLM engine can support the given token metric with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Any</code> <p>The sampler object containing sampling parameters and the LLM engine.</p> required <code>token_metric</code> <code>str</code> <p>The token metric to check compatibility for.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the configuration doesn't support the requested token metric.</p> Source code in <code>pita/inference/tensorRT_backend.py</code> <pre><code>def check_token_metric_compatibility(sampler: Any, token_metric: str) -&gt; None:\n    \"\"\"\n    Check that the TensorRT-LLM engine can support the given token metric with the given configuration.\n\n    Args:\n        sampler: The sampler object containing sampling parameters and the LLM engine.\n        token_metric: The token metric to check compatibility for.\n\n    Raises:\n        ValueError: If the configuration doesn't support the requested token metric.\n    \"\"\"\n    if token_metric == \"logprobs\":\n        # logprobs requires logits_per_token to be set\n        if sampler.sampling_params.logprobs_per_token is None or sampler.sampling_params.logprobs_per_token &lt; 1:\n            raise ValueError(\n                \"logprobs_per_token must be set to at least 1 to use 'logprobs' token metric with TensorRT-LLM backend.\"\n            )\n        # Enable normalization constants for logprobs calculation\n        sampler.sampling_params.enable_normalization_constants = True\n        print(\"Enabled normalization constants in sampling params for logprobs metric.\")\n\n    elif token_metric == \"power_distribution\":\n        # power_distribution requires normalization constants\n        if sampler.sampling_params.logits_per_token is None or sampler.sampling_params.logits_per_token &lt; 1 or sampler.sampling_params.logprobs_per_token is None or sampler.sampling_params.logprobs_per_token &lt; 1:\n            raise ValueError(\n                \"logits_per_token (and logprobs_per_token, which logits_per_token depends on) must be set to at least 1 to use 'power_distribution' token metric with TensorRT-LLM backend.\"\n            )\n        # Enable normalization constants\n        sampler.sampling_params.enable_normalization_constants = True\n        print(\"Enabled normalization constants in sampling params for power_distribution metric.\")\n\n    elif token_metric == \"entropy\":\n        # Enable entropy calculation\n        sampler.sampling_params.enable_entropy = True\n        print(\"Enabled entropy calculation in sampling params for entropy metric.\")\n\n    elif token_metric == \"likelihood_confidence\":\n        # likelihood_confidence requires logprobs and entropy\n        if sampler.sampling_params.logprobs_per_token is None or sampler.sampling_params.logprobs_per_token &lt; 1:\n            raise ValueError(\n                \"logprobs_per_token must be set to at least 1 to use 'likelihood_confidence' token metric with TensorRT-LLM backend.\"\n            )\n        sampler.sampling_params.enable_normalization_constants = True\n        sampler.sampling_params.enable_entropy = True\n        print(\"Enabled normalization constants and entropy in sampling params for likelihood_confidence metric.\")\n    else:\n        raise ValueError(f\"Unknown token metric: {token_metric}\")\n</code></pre>"},{"location":"inference/tensorRT_backend/#pita.inference.tensorRT_backend.create_LLM_object","title":"<code>create_LLM_object(model_name: str, model_type: str | None = None, dtype: str = 'auto', gpu_memory_utilization: float = 0.85, max_model_len: int = 2048, max_logprobs: int | None = None, logits_processor: bool = False, **kwargs: Any) -&gt; LLM</code>","text":"<p>Create the LLM object given the model name and engine parameters.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to load (HuggingFace model name or path).</p> required <code>model_type</code> <code>str</code> <p>The type of model. Defaults to None.</p> <code>None</code> <code>dtype</code> <code>str</code> <p>The data type to use. Defaults to \"auto\".</p> <code>'auto'</code> <code>gpu_memory_utilization</code> <code>float</code> <p>Kept for API compatibility with other backends; ignored by TensorRT-LLM and not passed to the LLM constructor.</p> <code>0.85</code> <code>max_model_len</code> <code>int</code> <p>The maximum context length. Defaults to 2048.</p> <code>2048</code> <code>max_logprobs</code> <code>int</code> <p>Kept for API compatibility with other backends; ignored by TensorRT-LLM and not passed to the LLM constructor.</p> <code>None</code> <code>logits_processor</code> <code>bool</code> <p>Whether logits processing is enabled. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the LLM constructor.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>LLM</code> <code>LLM</code> <p>The initialized TensorRT-LLM LLM object.</p> Source code in <code>pita/inference/tensorRT_backend.py</code> <pre><code>def create_LLM_object(\n        model_name: str,\n        model_type: str | None = None,\n        dtype: str = \"auto\",\n        gpu_memory_utilization: float = 0.85,\n        max_model_len: int = 2048,\n        max_logprobs: int | None = None,\n        logits_processor: bool = False,\n        **kwargs: Any\n    ) -&gt; LLM:\n    \"\"\"\n    Create the LLM object given the model name and engine parameters.\n\n    Args:\n        model_name (str): The name of the model to load (HuggingFace model name or path).\n        model_type (str, optional): The type of model. Defaults to None.\n        dtype (str, optional): The data type to use. Defaults to \"auto\".\n        gpu_memory_utilization (float, optional): Kept for API compatibility with other backends; ignored by TensorRT-LLM and not passed to the LLM constructor.\n        max_model_len (int, optional): The maximum context length. Defaults to 2048.\n        max_logprobs (int, optional): Kept for API compatibility with other backends; ignored by TensorRT-LLM and not passed to the LLM constructor.\n        logits_processor (bool, optional): Whether logits processing is enabled. Defaults to False.\n        **kwargs: Additional keyword arguments passed to the LLM constructor.\n\n    Returns:\n        LLM: The initialized TensorRT-LLM LLM object.\n    \"\"\"\n    # TensorRT-LLM LLM class handles model loading and optimization\n    llm = LLM(\n        model=model_name,\n        dtype=dtype,\n        max_num_tokens=max_model_len,\n        trust_remote_code=True,\n        **kwargs\n    )\n\n    if logits_processor:\n        print(\"TensorRT-LLM LogitsProcessor enabled. Normalization constants and entropy will be calculated per-request.\")\n\n    print(\"--- TensorRT-LLM Model Initialization Complete. ---\")\n\n    return llm\n</code></pre>"},{"location":"inference/tensorRT_backend/#pita.inference.tensorRT_backend.create_tensorrt_engine_params","title":"<code>create_tensorrt_engine_params() -&gt; SamplingParams</code>","text":"<p>Create the TensorRT-LLM SamplingParams object.</p> <p>Returns:</p> Name Type Description <code>SamplingParams</code> <code>SamplingParams</code> <p>A new instance of TensorRT-LLM SamplingParams.</p> Source code in <code>pita/inference/tensorRT_backend.py</code> <pre><code>def create_tensorrt_engine_params() -&gt; SamplingParams:\n    \"\"\"\n    Create the TensorRT-LLM SamplingParams object.\n\n    Returns:\n        SamplingParams: A new instance of TensorRT-LLM SamplingParams.\n    \"\"\"\n    return SamplingParams()\n</code></pre>"},{"location":"inference/tensorRT_backend/#pita.inference.tensorRT_backend.sample","title":"<code>sample(self, context: str | list[str], **kwargs: Any) -&gt; Output</code>","text":"<p>Generate text from the given context using the TensorRT-LLM engine.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>str | list[str]</code> <p>The input context string to generate from.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the TensorRT-LLM generate function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Output</code> <code>Output</code> <p>An Output object containing: - tokens: The generated token IDs. - top_k_logits: The top_k logits (if logits_per_token is set). - top_k_logprobs: The top_k logprobs (if logprobs_per_token is set). - unprocessed_log_normalization_constant: The log normalization constants for each token. - temp_processed_log_normalization_constant: The temperature-scaled log normalization constants. - entropy: The entropy for each token.</p> <code>Output</code> <p>See the :class:<code>pita.inference.LLM_backend.Output</code> class documentation</p> <code>Output</code> <p>for a complete description of the fields and their semantics.</p> Source code in <code>pita/inference/tensorRT_backend.py</code> <pre><code>def sample(\n        self,\n        context: str | list[str],\n        **kwargs: Any\n    ) -&gt; Output:\n    \"\"\"\n    Generate text from the given context using the TensorRT-LLM engine.\n\n    Args:\n        context (str | list[str]): The input context string to generate from.\n        **kwargs: Additional keyword arguments passed to the TensorRT-LLM generate function.\n\n    Returns:\n        Output: An Output object containing:\n            - tokens: The generated token IDs.\n            - top_k_logits: The top_k logits (if logits_per_token is set).\n            - top_k_logprobs: The top_k logprobs (if logprobs_per_token is set).\n            - unprocessed_log_normalization_constant: The log normalization constants for each token.\n            - temp_processed_log_normalization_constant: The temperature-scaled log normalization constants.\n            - entropy: The entropy for each token.\n\n        See the :class:`pita.inference.LLM_backend.Output` class documentation\n        for a complete description of the fields and their semantics.\n    \"\"\"\n    # Determine if we need normalization constants or entropy\n    calculate_normalization = getattr(self.sampling_params, 'enable_normalization_constants', False)\n    calculate_entropy = getattr(self.sampling_params, 'enable_entropy', False)\n\n    # Check if context is a list of strings or a single string\n    if isinstance(context, list):\n        context_list_len = len(context)\n    else:\n        context_list_len = 1\n        context = [context]  # Normalize to list for uniform handling\n\n    all_outputs = []\n\n    for context_input in context:\n        # Generate unique request ID for Redis IPC\n        req_id = f\"tensorrt_{uuid.uuid4().hex}\"\n\n        # Create logits processor if normalization or entropy is needed\n        if calculate_normalization or calculate_entropy:\n            logits_processor = create_logits_processor(\n                req_id=req_id,\n                temperature=self.sampling_params.temperature,\n                calculate_normalization=calculate_normalization,\n                calculate_entropy=calculate_entropy\n            )\n            self.sampling_params.engine_params.logits_processor = logits_processor\n        else:\n            self.sampling_params.engine_params.logits_processor = None\n\n        # Check if logprobs_per_token/logits_per_token is greater than 1. If so, raise an error for unsupported configuration\n        if self.sampling_params.logprobs_per_token and self.sampling_params.logprobs_per_token &gt; 1:\n            raise ValueError(\n                \"logprobs_per_token &gt; 1 is not supported for the TensorRT-LLM backend. \"\n                \"Please set logprobs_per_token to 1 or disable it.\"\n            )\n        if self.sampling_params.logits_per_token and self.sampling_params.logits_per_token &gt; 1:\n            raise ValueError(\n                \"logits_per_token &gt; 1 is not supported for the TensorRT-LLM backend. \"\n                \"Please set logits_per_token to 1 or disable it.\"\n            )\n\n        # Generate\n        llm_output = self.llm.generate(\n            context_input, \n            sampling_params=self.sampling_params.engine_params,\n            **kwargs\n        )\n\n        # Extract tokens from output\n        tokens = list(llm_output.outputs[0].token_ids)\n        n_completion = len(tokens)\n\n        # Retrieve normalization constants and entropy from Redis\n        unprocessed_log_normalization_constant = []\n        temp_processed_log_normalization_constant = []\n        entropy = []\n\n        if calculate_normalization or calculate_entropy:\n            valkey_client = None\n            try:\n                valkey_client = valkey.Valkey(host=VALKEY_HOST, port=VALKEY_PORT, db=0, decode_responses=True)\n\n                # Set a TTL as a fallback in case cleanup fails (e.g., process crash)\n                valkey_client.expire(req_id, 60)\n\n                # Retrieve all values from Valkey using the request ID\n                normalization_terms = valkey_client.lrange(req_id, 0, -1)\n\n                # Parse the normalization terms (format: \"norm_val,norm_temp_val,entropy_val\")\n                for term in normalization_terms:\n                    parts = term.split(',')\n                    unprocessed_log_normalization_constant.append(float(parts[0]))\n                    temp_processed_log_normalization_constant.append(float(parts[1]))\n                    entropy.append(float(parts[2]))\n            except Exception as e:\n                print(f\"Warning: Failed to retrieve results from Valkey: {e}\")\n            finally:\n                # Always clean up the Valkey key, even if an exception occurred\n                if valkey_client is not None:\n                    try:\n                        valkey_client.delete(req_id)\n                    except Exception:\n                        pass  # Ignore cleanup errors; TTL will handle expiration\n\n        # Extract logprobs if available\n        logprobs_per_token = self.sampling_params.logprobs_per_token \n        logits_per_token = self.sampling_params.logits_per_token \n\n        top_k_logits = []\n        top_k_logprobs = []\n\n        if hasattr(llm_output.outputs[0], 'logprobs') and llm_output.outputs[0].logprobs:\n            # Extract logprobs from output\n            for token_logprobs in llm_output.outputs[0].logprobs:\n                if token_logprobs:\n                    # Get top-k logprobs\n                    # Sort by logprob value - token_logprobs is dict of token_id -&gt; Logprob object\n                    sorted_logprobs = sorted(\n                        token_logprobs.items(), \n                        key=lambda x: getattr(x[1], 'logprob', x[1]) if hasattr(x[1], 'logprob') else x[1], \n                        reverse=True\n                    )\n                    # Extract the float value from Logprob objects\n                    token_top_logprobs = [\n                        getattr(lp, 'logprob', lp) if hasattr(lp, 'logprob') else lp \n                        for _, lp in sorted_logprobs[:logprobs_per_token]\n                    ]\n                    top_k_logprobs.append(token_top_logprobs)\n\n                    # Calculate logits from logprobs (logit = logprob + log_norm_constant)\n                    if temp_processed_log_normalization_constant and len(temp_processed_log_normalization_constant) &gt; len(top_k_logits):\n                        idx = len(top_k_logits)\n                        token_top_logits = [\n                            (lp + temp_processed_log_normalization_constant[idx]) * self.sampling_params.temperature \n                            for lp in token_top_logprobs[:logits_per_token]\n                        ]\n                        top_k_logits.append(token_top_logits)\n                    else:\n                        top_k_logits.append([])\n                else:\n                    top_k_logprobs.append([])\n                    top_k_logits.append([])\n        else:\n            # No logprobs available, fill with empty lists\n            top_k_logits = [[] for _ in range(n_completion)]\n            top_k_logprobs = [[] for _ in range(n_completion)]\n\n        # Ensure arrays have consistent length\n        while len(unprocessed_log_normalization_constant) &lt; n_completion:\n            unprocessed_log_normalization_constant.append(0.0)\n        while len(temp_processed_log_normalization_constant) &lt; n_completion:\n            temp_processed_log_normalization_constant.append(0.0)\n        while len(entropy) &lt; n_completion:\n            entropy.append(0.0)\n\n        # Trim to n_completion if needed\n        unprocessed_log_normalization_constant = unprocessed_log_normalization_constant[:n_completion]\n        temp_processed_log_normalization_constant = temp_processed_log_normalization_constant[:n_completion]\n        entropy = entropy[:n_completion]\n\n        output = Output(\n            tokens=tokens,\n            top_k_logits=top_k_logits,\n            top_k_logprobs=top_k_logprobs,\n            unprocessed_log_normalization_constant=unprocessed_log_normalization_constant,\n            temp_processed_log_normalization_constant=temp_processed_log_normalization_constant,\n            entropy=entropy\n        )\n        all_outputs.append(output)\n\n    # If only one context was provided, return single Output\n    if context_list_len == 1:\n        return all_outputs[0]\n\n    # For multiple contexts, combine into a single Output with lists of lists\n    combined = Output(\n        tokens=[o.tokens for o in all_outputs],\n        top_k_logits=[o.top_k_logits for o in all_outputs],\n        top_k_logprobs=[o.top_k_logprobs for o in all_outputs],\n        unprocessed_log_normalization_constant=[o.unprocessed_log_normalization_constant for o in all_outputs],\n        temp_processed_log_normalization_constant=[o.temp_processed_log_normalization_constant for o in all_outputs],\n        entropy=[o.entropy for o in all_outputs]\n    )\n    return combined\n</code></pre>"},{"location":"inference/vllm_backend/","title":"vLLM Backend","text":""},{"location":"inference/vllm_backend/#pita.inference.vllm_backend.check_token_metric_compatibility","title":"<code>check_token_metric_compatibility(sampler: AutoregressiveSampler, token_metric: str)</code>","text":"<p>Check that the vLLM engine can support the given token metric with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>AutoregressiveSampler</code> <p>The sampler object containing sampling parameters and the LLM engine.</p> required <code>token_metric</code> <code>str</code> <p>The token metric to check compatibility for.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If logits_per_token is not set.</p> <code>ValueError</code> <p>If vLLM engine logprobs_mode is not 'raw_logits'.</p> <code>ValueError</code> <p>If 'req_id' is not in extra_args.</p> Source code in <code>pita/inference/vllm_backend.py</code> <pre><code>def check_token_metric_compatibility(\n    sampler: AutoregressiveSampler, \n    token_metric: str):\n    \"\"\"\n    Check that the vLLM engine can support the given token metric with the given configuration.\n\n    Args:\n        sampler: The sampler object containing sampling parameters and the LLM engine.\n        token_metric: The token metric to check compatibility for.\n\n    Raises:\n        ValueError: If logits_per_token is not set.\n        ValueError: If vLLM engine logprobs_mode is not 'raw_logits'.\n        ValueError: If 'req_id' is not in extra_args.\n    \"\"\"\n    if (token_metric == \"logprobs\" or token_metric == \"power_distribution\" or token_metric == \"entropy\" or token_metric == \"likelihood_confidence\"):\n        # Make sure the user has actually set logits_per_token\n        if(sampler.sampling_params.logits_per_token &lt; 1):\n            raise ValueError(\"LLM engine logits_per_token must be set to at least 1 to enable power sampling.\")\n\n        # For vLLM, make sure that logprobs_mode is set to 'raw_logits' to get unprocessed logits\n        if(sampler.llm.llm_engine.model_config.logprobs_mode != 'raw_logits'):\n            raise ValueError(\n                f\"vLLM engine logprobs_mode must be set to 'raw_logits' to enable power sampling.\"\n                f\"\\nvLLM engine logprobs_mode is set to {sampler.llm.llm_engine.model_config.logprobs_mode}.\" \n                f\"\\nThis is done by setting logits=True when creating the LLM object.\"\n                            )\n        # Print all the extra_args of the vLLM SamplingParams\n        print(\"vLLM SamplingParams extra_args:\", sampler.sampling_params.engine_params.extra_args)  \n\n        # Make sure the user has enabled the logits processor\n        if('req_id' not in sampler.sampling_params.engine_params.extra_args):\n            raise ValueError(\"req_id must be set to use power sampling with vLLM.\")\n\n        # Set the normalization constant in the extra_args of the vLLM SamplingParams to True\n        if(token_metric == \"logprobs\" or token_metric == \"power_distribution\" or token_metric == \"likelihood_confidence\"):\n            sampler.sampling_params.enable_normalization_constants = True\n            print(\"Enabled normalization constants in vLLM SamplingParams for power sampling.\")\n\n        if(token_metric == \"entropy\" or token_metric == \"likelihood_confidence\"):\n            sampler.sampling_params.enable_entropy = True\n            print(\"Enabled entropy in vLLM SamplingParams for power sampling.\")\n</code></pre>"},{"location":"inference/vllm_backend/#pita.inference.vllm_backend.create_LLM_object","title":"<code>create_LLM_object(model_name: str, model_type: str | None = None, dtype: str = 'auto', gpu_memory_utilization: float = 0.85, max_model_len: int = 2048, max_probs: int = 1000, logits_processor: bool = False, **kwargs: Any) -&gt; LLM</code>","text":"<p>Create the LLM object given the model name and engine parameters.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to load.</p> required <code>model_type</code> <code>str</code> <p>The type of model (e.g., 'safetensors', 'gguf'). Defaults to None.</p> <code>None</code> <code>dtype</code> <code>str</code> <p>The data type to use. Defaults to \"auto\".</p> <code>'auto'</code> <code>gpu_memory_utilization</code> <code>float</code> <p>The fraction of GPU memory to use. Defaults to 0.85.</p> <code>0.85</code> <code>max_model_len</code> <code>int</code> <p>The maximum length of the model context. Defaults to 2048.</p> <code>2048</code> <code>max_probs</code> <code>int</code> <p>Controls how many logprobs or logits are stored for each token. Defaults to 1000.</p> <code>1000</code> <code>logits_processor</code> <code>bool</code> <p>Whether to enable the Redis logging logits processor. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the LLM constructor.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>LLM</code> <code>LLM</code> <p>The initialized vLLM LLM object.</p> Source code in <code>pita/inference/vllm_backend.py</code> <pre><code>def create_LLM_object(\n        model_name: str,\n        model_type: str | None = None,\n        dtype: str = \"auto\",\n        gpu_memory_utilization: float = 0.85,\n        max_model_len: int = 2048,\n        max_probs: int = 1000,\n        logits_processor: bool = False,\n        **kwargs: Any\n    ) -&gt; LLM:\n    \"\"\"\n    Create the LLM object given the model name and engine parameters.\n\n    Args:\n        model_name (str): The name of the model to load.\n        model_type (str, optional): The type of model (e.g., 'safetensors', 'gguf'). Defaults to None.\n        dtype (str, optional): The data type to use. Defaults to \"auto\".\n        gpu_memory_utilization (float, optional): The fraction of GPU memory to use. Defaults to 0.85.\n        max_model_len (int, optional): The maximum length of the model context. Defaults to 2048.\n        max_probs (int, optional): Controls how many logprobs or logits are stored for each token. Defaults to 1000.\n        logits_processor (bool, optional): Whether to enable the Redis logging logits processor. Defaults to False.\n        **kwargs: Additional keyword arguments passed to the LLM constructor.\n\n    Returns:\n        LLM: The initialized vLLM LLM object.\n    \"\"\"\n\n    if(logits_processor):\n        # Enable the Valkey logging logits processor by adding it to the kwargs\n        kwargs[\"logits_processors\"] = [LogitsLoggingProcessor]\n        ValkeyManager.start()\n        print(\"LogitsLoggingProcessor enabled. Logits will be logged.\")\n    else:\n        print(\"LogitsLoggingProcessor not enabled. Logits will not be logged.\")\n\n    # Initialize VLLM locally for performance (as done in power_sample.py main)\n    llm = LLM(model=model_name,\n              dtype=dtype,\n              gpu_memory_utilization=gpu_memory_utilization,\n              max_model_len=max_model_len,\n              max_logprobs=max_probs, # Controls how many logprobs or logits are stored for each token\n              logprobs_mode='raw_logits',\n              **kwargs)\n\n    return llm\n</code></pre>"},{"location":"inference/vllm_backend/#pita.inference.vllm_backend.create_vllm_engine_params","title":"<code>create_vllm_engine_params() -&gt; SamplingParams</code>","text":"<p>Create the vLLM SamplingParams object from the common Sampling_Params.</p> <p>Returns:</p> Name Type Description <code>SamplingParams</code> <code>SamplingParams</code> <p>A new instance of vLLM SamplingParams.</p> Source code in <code>pita/inference/vllm_backend.py</code> <pre><code>def create_vllm_engine_params() -&gt; SamplingParams:\n    \"\"\"\n    Create the vLLM SamplingParams object from the common Sampling_Params.\n\n    Returns:\n        SamplingParams: A new instance of vLLM SamplingParams.\n    \"\"\"\n    # Create the vLLM SamplingParams object from the common Sampling_Params\n    vllm_params = SamplingParams()\n    return vllm_params\n</code></pre>"},{"location":"inference/vllm_backend/#pita.inference.vllm_backend.sample","title":"<code>sample(self, context: str | list[str], **kwargs: Any) -&gt; Output</code>","text":"<p>Generate text from the given context using the vLLM engine.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>str | list[str]</code> <p>The input context string to generate from.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the vLLM generate function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Output</code> <code>Output</code> <p>An Output object containing: - tokens: The generated token IDs. - top_k_logits: The top_k logits (if logits_per_token is set). - top_k_logprobs: The top_k logprobs (if logprobs is set). - unprocessed_log_normalization_constant: The log(Normalization Constants - Unprocessed) for each token. - temp_processed_log_normalization_constant: The log(Normalization Constants - Temperature Processed) for each token. - entropy: The entropy for each token.</p> Source code in <code>pita/inference/vllm_backend.py</code> <pre><code>def sample(\n        self,\n        context: str | list[str],\n        **kwargs: Any\n    ) -&gt; Output:\n    \"\"\"\n    Generate text from the given context using the vLLM engine.\n\n    Args:\n        context (str | list[str]): The input context string to generate from.\n        **kwargs: Additional keyword arguments passed to the vLLM generate function.\n\n    Returns:\n        Output: An Output object containing:\n            - tokens: The generated token IDs.\n            - top_k_logits: The top_k logits (if logits_per_token is set).\n            - top_k_logprobs: The top_k logprobs (if logprobs is set).\n            - unprocessed_log_normalization_constant: The log(Normalization Constants - Unprocessed) for each token.\n            - temp_processed_log_normalization_constant: The log(Normalization Constants - Temperature Processed) for each token.\n            - entropy: The entropy for each token.\n    \"\"\"\n\n    # Generate a new response from the LLM\n    llm_output = self.llm.generate(\n        context, \n        sampling_params=self.sampling_params.engine_params, \n        **kwargs\n    )\n\n    # Get the generated tokens\n    tokens = llm_output[0].outputs[0].token_ids\n\n    # Create a 2D array of NaNs to hold the logits\n    logits_expected = max(self.sampling_params.logprobs_per_token or 0, self.sampling_params.logits_per_token or 0)\n    logits = np.full((len(tokens), 1 + logits_expected), np.nan, dtype=float)\n    for token_idx in range(len(tokens)):\n        for logit_idx, values in enumerate(llm_output[0].outputs[0].logprobs[token_idx].values()):\n            logits[token_idx][logit_idx] = values.logprob\n\n    # Get the Normalization Constants from Redis\n    unprocessed_log_normalization_constant = []\n    temp_processed_log_normalization_constant = []\n    entropy = []\n    if (hasattr(self.sampling_params.engine_params, 'extra_args') and 'req_id' in self.sampling_params.engine_params.extra_args):        \n        # Set the req_id used to store the normalization constants in Redis\n        req_id = self.sampling_params.engine_params.extra_args[\"req_id\"]\n\n        # Create a local Valkey client to retrieve the normalization constants\n        valkey_client = valkey.Valkey(host=VALKEY_HOST, port=VALKEY_PORT, db=0, decode_responses=True)\n\n        # Retrieve the normalization constants from Valkey using the req_id\n        normalization_terms = valkey_client.lrange(req_id, 0, -1)\n\n        # Clean up the Valkey key after retrieval\n        valkey_client.delete(req_id)\n\n        # Parse the normalization terms (format: \"norm_val,norm_temp_val,max_val\")\n        for term in normalization_terms:\n            parts = term.split(',')\n            unprocessed_log_normalization_constant.append(float(parts[0]))\n            temp_processed_log_normalization_constant.append(float(parts[1]))\n            entropy.append(float(parts[2]))\n\n    # Find the logprobs for each token with the logits and temp_processed_log_normalization_constant\n    logprobs = (logits / self.sampling_params.engine_params.temperature) - np.array(temp_processed_log_normalization_constant)[:, np.newaxis]    \n\n    # Create the output object\n    output = Output(\n        tokens=tokens,\n        top_k_logits=logits[:, :self.sampling_params.logits_per_token],\n        top_k_logprobs=logprobs[:, :self.sampling_params.logprobs_per_token],\n        unprocessed_log_normalization_constant=unprocessed_log_normalization_constant,\n        temp_processed_log_normalization_constant=temp_processed_log_normalization_constant,\n        entropy=entropy\n    )\n\n    # Returns the output object\n    return output\n</code></pre>"},{"location":"inference/vllm_logits_processor/","title":"vLLM Logits Processor","text":""},{"location":"inference/vllm_logits_processor/#pita.inference.vllm_logits_processor.LogitsLoggingProcessor","title":"<code>LogitsLoggingProcessor</code>","text":"<p>               Bases: <code>LogitsProcessor</code></p> <p>Custom vLLM logits processor that logs normalization constants and entropy to Valkey.</p> <p>This processor intercepts logits during generation to calculate and store normalization constants and entropy values. These values are stored in Valkey for retrieval by the main process after generation completes.</p> <p>Attributes:</p> Name Type Description <code>active_req_ids</code> <code>Dict[int, sampling_params]</code> <p>Dictionary mapping request indices to their sampling parameters.</p> <code>valkey_client</code> <p>Valkey client for storing computed values.</p> <code>temperature</code> <p>Default temperature value.</p> Source code in <code>pita/inference/vllm_logits_processor.py</code> <pre><code>class LogitsLoggingProcessor(LogitsProcessor):\n    \"\"\"\n    Custom vLLM logits processor that logs normalization constants and entropy to Valkey.\n\n    This processor intercepts logits during generation to calculate and store normalization\n    constants and entropy values. These values are stored in Valkey for retrieval by the\n    main process after generation completes.\n\n    Attributes:\n        active_req_ids: Dictionary mapping request indices to their sampling parameters.\n        valkey_client: Valkey client for storing computed values.\n        temperature: Default temperature value.\n    \"\"\"\n    def __init__(\n         self,\n        vllm_config: VllmConfig,\n        device: torch.device,\n        is_pin_memory: bool\n    ) -&gt; None:\n        \"\"\"\n        Initialize the LogitsLoggingProcessor.\n\n        Args:\n            vllm_config: vLLM configuration object.\n            device: PyTorch device for tensor operations.\n            is_pin_memory: Whether to use pinned memory for tensors.\n        \"\"\"\n        self.active_req_ids: Dict[int, sampling_params] = {}\n        self.valkey_client = None\n        self.temperature = 1.0  # Default temperature, can be configured per request\n\n    def _ensure_valkey(self) -&gt; None:\n        \"\"\"\n        Ensure Valkey client is initialized and connected.\n\n        Lazily initializes the Valkey connection on first use to avoid connection\n        issues during processor instantiation.\n        \"\"\"\n        if self.valkey_client is None:\n            try:\n                self.valkey_client = valkey.Valkey(\n                    host=VALKEY_HOST, port=VALKEY_PORT, db=0, decode_responses=True\n                )\n            except Exception as e:\n                # If we can't log to Valkey, we are flying blind, but try printing just in case\n                print(f\"CRITICAL WORKER ERROR: Valkey connect failed: {e}\")\n\n    def is_argmax_invariant(self) -&gt; bool:\n        \"\"\"\n        Indicate whether this processor changes which token has the highest probability.\n\n        Returns:\n            False to ensure apply() is always called, even when it doesn't change argmax.\n        \"\"\"\n        return False  # Must be False to ensure apply() is called\n\n    def update_state(self, batch_update: Optional[BatchUpdate]) -&gt; None:\n        \"\"\"\n        Update processor state when requests are added, removed, or moved in the batch.\n\n        This method is called by vLLM to notify the processor of batch changes. It tracks\n        request IDs and their associated sampling parameters.\n\n        Args:\n            batch_update: Information about requests added, removed, or moved in the batch.\n                Can be None if no updates occurred.\n        \"\"\"\n        self._ensure_valkey()\n\n        if batch_update is None:\n            return\n\n        for req_index, params, _, _ in batch_update.added:\n            # Debug: Check if extra_args survived the trip\n            args_str = str(params.extra_args) if params.extra_args else \"None\"\n\n            # Update the req_id map\n            if params.extra_args and \"req_id\" in params.extra_args:\n                req_id = params.extra_args[\"req_id\"]\n                self.active_req_ids[req_index] = sampling_params(\n                    req_id, \n                    params.extra_args.get(\"normalization_constants\", False), \n                    params.temperature, \n                    params.extra_args.get(\"entropy\", False), \n                    params.extra_args.get(\"entropy_inference\", False), \n                    params.extra_args.get(\"gradient_steps\", 0), \n                    params.extra_args.get(\"learning_rate\", 0.0), \n                    params.extra_args.get(\"delta\", 0.0)\n                )\n            else:\n                print(f\"WARNING: No req_id found in extra_args for req_index {req_index}. extra_args: {args_str}. Logits logging will be skipped for this request.\")\n\n        # Handle removals to keep map clean\n        for req_index in batch_update.removed:\n            if req_index in self.active_req_ids:\n                self.active_req_ids.pop(req_index)\n\n        # Handle index movements \n        for from_idx, to_idx, direction in batch_update.moved:\n            if direction == MoveDirectionality.SWAP:\n                self.active_req_ids[to_idx], self.active_req_ids[from_idx] = (\n                    self.active_req_ids[from_idx], self.active_req_ids[to_idx]\n                )\n            else:\n                if from_idx in self.active_req_ids:\n                    self.active_req_ids[to_idx] = self.active_req_ids[from_idx]\n                    del self.active_req_ids[from_idx]\n\n    def apply(self, logits: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Process logits to calculate and log normalization constants and entropy.\n\n        This method is called by vLLM for each token generation step. It calculates\n        normalization constants (logsumexp) and entropy values, then stores them in\n        Redis for later retrieval.\n\n        Args:\n            logits: Raw logits tensor of shape (batch_size, vocab_size).\n\n        Returns:\n            The unmodified logits tensor (this processor only observes, doesn't modify).\n        \"\"\"\n        self._ensure_valkey()\n\n        if not self.active_req_ids:\n            print(\"WARNING: active_req_ids is empty in apply()!\")\n            return logits\n\n        # Store the max_logits and shift_logits of each request\n        log_norm_constant = torch.zeros(len(self.active_req_ids), device=logits.device)\n        log_norm_constant_temp_scaled = torch.zeros(len(self.active_req_ids), device=logits.device)\n        entropy = torch.zeros(len(self.active_req_ids), device=logits.device)\n\n        # Calculate the Normalization Constants if normalization_constants = True or entropy = True\n        for row_idx, params in self.active_req_ids.items():\n            if params.normalization_constants:\n                # Calculate the Normalization Constants if required\n                log_norm_constant[row_idx] = torch.logsumexp(logits[row_idx], dim=-1)\n                log_norm_constant_temp_scaled[row_idx] = torch.logsumexp(logits[row_idx] / params.temperature, dim=-1)\n            # If entropy = True, calculate the entropy\n            if params.entropy:\n                entropy[row_idx] = Categorical(logits=logits[row_idx]).entropy()\n\n\n        # Prepare pipeline for batch Valkey operations\n        pipe = self.valkey_client.pipeline()\n\n        found_any = False\n        for row_idx, params in self.active_req_ids.items():\n            req_id = params.req_id\n            if row_idx &lt; logits.size(0):\n                # Store as JSON-like string with all normalization info\n                data = f\"{log_norm_constant[row_idx]},{log_norm_constant_temp_scaled[row_idx]},{entropy[row_idx]}\"\n                pipe.rpush(req_id, data)\n                found_any = True\n            else:\n                print(f\"row_idx {row_idx} &gt;= batch size {logits.size(0)}, skipping\")\n\n        # Push values to Valkey if we found any valid ones\n        if found_any:\n            pipe.execute()\n\n        return logits\n</code></pre>"},{"location":"inference/vllm_logits_processor/#pita.inference.vllm_logits_processor.LogitsLoggingProcessor.__init__","title":"<code>__init__(vllm_config: VllmConfig, device: torch.device, is_pin_memory: bool) -&gt; None</code>","text":"<p>Initialize the LogitsLoggingProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>vllm_config</code> <code>VllmConfig</code> <p>vLLM configuration object.</p> required <code>device</code> <code>device</code> <p>PyTorch device for tensor operations.</p> required <code>is_pin_memory</code> <code>bool</code> <p>Whether to use pinned memory for tensors.</p> required Source code in <code>pita/inference/vllm_logits_processor.py</code> <pre><code>def __init__(\n     self,\n    vllm_config: VllmConfig,\n    device: torch.device,\n    is_pin_memory: bool\n) -&gt; None:\n    \"\"\"\n    Initialize the LogitsLoggingProcessor.\n\n    Args:\n        vllm_config: vLLM configuration object.\n        device: PyTorch device for tensor operations.\n        is_pin_memory: Whether to use pinned memory for tensors.\n    \"\"\"\n    self.active_req_ids: Dict[int, sampling_params] = {}\n    self.valkey_client = None\n    self.temperature = 1.0  # Default temperature, can be configured per request\n</code></pre>"},{"location":"inference/vllm_logits_processor/#pita.inference.vllm_logits_processor.LogitsLoggingProcessor.apply","title":"<code>apply(logits: torch.Tensor) -&gt; torch.Tensor</code>","text":"<p>Process logits to calculate and log normalization constants and entropy.</p> <p>This method is called by vLLM for each token generation step. It calculates normalization constants (logsumexp) and entropy values, then stores them in Redis for later retrieval.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Raw logits tensor of shape (batch_size, vocab_size).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The unmodified logits tensor (this processor only observes, doesn't modify).</p> Source code in <code>pita/inference/vllm_logits_processor.py</code> <pre><code>def apply(self, logits: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Process logits to calculate and log normalization constants and entropy.\n\n    This method is called by vLLM for each token generation step. It calculates\n    normalization constants (logsumexp) and entropy values, then stores them in\n    Redis for later retrieval.\n\n    Args:\n        logits: Raw logits tensor of shape (batch_size, vocab_size).\n\n    Returns:\n        The unmodified logits tensor (this processor only observes, doesn't modify).\n    \"\"\"\n    self._ensure_valkey()\n\n    if not self.active_req_ids:\n        print(\"WARNING: active_req_ids is empty in apply()!\")\n        return logits\n\n    # Store the max_logits and shift_logits of each request\n    log_norm_constant = torch.zeros(len(self.active_req_ids), device=logits.device)\n    log_norm_constant_temp_scaled = torch.zeros(len(self.active_req_ids), device=logits.device)\n    entropy = torch.zeros(len(self.active_req_ids), device=logits.device)\n\n    # Calculate the Normalization Constants if normalization_constants = True or entropy = True\n    for row_idx, params in self.active_req_ids.items():\n        if params.normalization_constants:\n            # Calculate the Normalization Constants if required\n            log_norm_constant[row_idx] = torch.logsumexp(logits[row_idx], dim=-1)\n            log_norm_constant_temp_scaled[row_idx] = torch.logsumexp(logits[row_idx] / params.temperature, dim=-1)\n        # If entropy = True, calculate the entropy\n        if params.entropy:\n            entropy[row_idx] = Categorical(logits=logits[row_idx]).entropy()\n\n\n    # Prepare pipeline for batch Valkey operations\n    pipe = self.valkey_client.pipeline()\n\n    found_any = False\n    for row_idx, params in self.active_req_ids.items():\n        req_id = params.req_id\n        if row_idx &lt; logits.size(0):\n            # Store as JSON-like string with all normalization info\n            data = f\"{log_norm_constant[row_idx]},{log_norm_constant_temp_scaled[row_idx]},{entropy[row_idx]}\"\n            pipe.rpush(req_id, data)\n            found_any = True\n        else:\n            print(f\"row_idx {row_idx} &gt;= batch size {logits.size(0)}, skipping\")\n\n    # Push values to Valkey if we found any valid ones\n    if found_any:\n        pipe.execute()\n\n    return logits\n</code></pre>"},{"location":"inference/vllm_logits_processor/#pita.inference.vllm_logits_processor.LogitsLoggingProcessor.is_argmax_invariant","title":"<code>is_argmax_invariant() -&gt; bool</code>","text":"<p>Indicate whether this processor changes which token has the highest probability.</p> <p>Returns:</p> Type Description <code>bool</code> <p>False to ensure apply() is always called, even when it doesn't change argmax.</p> Source code in <code>pita/inference/vllm_logits_processor.py</code> <pre><code>def is_argmax_invariant(self) -&gt; bool:\n    \"\"\"\n    Indicate whether this processor changes which token has the highest probability.\n\n    Returns:\n        False to ensure apply() is always called, even when it doesn't change argmax.\n    \"\"\"\n    return False  # Must be False to ensure apply() is called\n</code></pre>"},{"location":"inference/vllm_logits_processor/#pita.inference.vllm_logits_processor.LogitsLoggingProcessor.update_state","title":"<code>update_state(batch_update: Optional[BatchUpdate]) -&gt; None</code>","text":"<p>Update processor state when requests are added, removed, or moved in the batch.</p> <p>This method is called by vLLM to notify the processor of batch changes. It tracks request IDs and their associated sampling parameters.</p> <p>Parameters:</p> Name Type Description Default <code>batch_update</code> <code>Optional[BatchUpdate]</code> <p>Information about requests added, removed, or moved in the batch. Can be None if no updates occurred.</p> required Source code in <code>pita/inference/vllm_logits_processor.py</code> <pre><code>def update_state(self, batch_update: Optional[BatchUpdate]) -&gt; None:\n    \"\"\"\n    Update processor state when requests are added, removed, or moved in the batch.\n\n    This method is called by vLLM to notify the processor of batch changes. It tracks\n    request IDs and their associated sampling parameters.\n\n    Args:\n        batch_update: Information about requests added, removed, or moved in the batch.\n            Can be None if no updates occurred.\n    \"\"\"\n    self._ensure_valkey()\n\n    if batch_update is None:\n        return\n\n    for req_index, params, _, _ in batch_update.added:\n        # Debug: Check if extra_args survived the trip\n        args_str = str(params.extra_args) if params.extra_args else \"None\"\n\n        # Update the req_id map\n        if params.extra_args and \"req_id\" in params.extra_args:\n            req_id = params.extra_args[\"req_id\"]\n            self.active_req_ids[req_index] = sampling_params(\n                req_id, \n                params.extra_args.get(\"normalization_constants\", False), \n                params.temperature, \n                params.extra_args.get(\"entropy\", False), \n                params.extra_args.get(\"entropy_inference\", False), \n                params.extra_args.get(\"gradient_steps\", 0), \n                params.extra_args.get(\"learning_rate\", 0.0), \n                params.extra_args.get(\"delta\", 0.0)\n            )\n        else:\n            print(f\"WARNING: No req_id found in extra_args for req_index {req_index}. extra_args: {args_str}. Logits logging will be skipped for this request.\")\n\n    # Handle removals to keep map clean\n    for req_index in batch_update.removed:\n        if req_index in self.active_req_ids:\n            self.active_req_ids.pop(req_index)\n\n    # Handle index movements \n    for from_idx, to_idx, direction in batch_update.moved:\n        if direction == MoveDirectionality.SWAP:\n            self.active_req_ids[to_idx], self.active_req_ids[from_idx] = (\n                self.active_req_ids[from_idx], self.active_req_ids[to_idx]\n            )\n        else:\n            if from_idx in self.active_req_ids:\n                self.active_req_ids[to_idx] = self.active_req_ids[from_idx]\n                del self.active_req_ids[from_idx]\n</code></pre>"},{"location":"inference/vllm_logits_processor/#pita.inference.vllm_logits_processor.sampling_params","title":"<code>sampling_params</code>  <code>dataclass</code>","text":"<p>Sampling parameters for logits processing requests.</p> <p>Attributes:</p> Name Type Description <code>req_id</code> <code>str</code> <p>Unique identifier for the request.</p> <code>normalization_constants</code> <code>bool</code> <p>Whether to calculate normalization constants.</p> <code>temperature</code> <code>float</code> <p>Sampling temperature value.</p> <code>entropy</code> <code>bool</code> <p>Whether to calculate entropy.</p> <code>entropy_inference</code> <code>bool</code> <p>Whether entropy is used for inference decisions.</p> <code>gradient_steps</code> <code>int</code> <p>Number of gradient steps for optimization.</p> <code>learning_rate</code> <code>float</code> <p>Learning rate for optimization.</p> <code>delta</code> <code>float</code> <p>Delta value for optimization adjustments.</p> Source code in <code>pita/inference/vllm_logits_processor.py</code> <pre><code>@dataclass\nclass sampling_params:\n    \"\"\"\n    Sampling parameters for logits processing requests.\n\n    Attributes:\n        req_id: Unique identifier for the request.\n        normalization_constants: Whether to calculate normalization constants.\n        temperature: Sampling temperature value.\n        entropy: Whether to calculate entropy.\n        entropy_inference: Whether entropy is used for inference decisions.\n        gradient_steps: Number of gradient steps for optimization.\n        learning_rate: Learning rate for optimization.\n        delta: Delta value for optimization adjustments.\n    \"\"\"\n    req_id: str\n    normalization_constants: bool\n    temperature: float\n    entropy: bool\n    entropy_inference: bool\n    gradient_steps: int\n    learning_rate: float\n    delta: float\n</code></pre>"},{"location":"sampling/power_sample/","title":"Power Sample","text":""},{"location":"sampling/power_sample/#pita.sampling.power_sample.Power_Sampling","title":"<code>Power_Sampling</code>","text":"<p>Power Sampling Class that stores the parameters and methods used for power sampling.</p> <p>Attributes:</p> Name Type Description <code>block_size</code> <code>int</code> <p>How many tokens to divide the total output tokens into for power sampling. number of blocks = (sampler.sampling_params.max_tokens)/block_size. Smaller block sizes = better quality but slower</p> <code>MCMC_steps</code> <code>int</code> <p>Number of MCMC steps to perform per block. More steps = better quality but slower</p> <code>token_metric</code> <code>str</code> <p>Metric to use for token selection. Can be \"logprobs\", \"power_distribution\", \"entropy\", or \"likelihood_confidence\"</p> Source code in <code>pita/sampling/power_sample.py</code> <pre><code>class Power_Sampling:\n    \"\"\"\n    Power Sampling Class that stores the parameters and methods used for power sampling.\n\n    Attributes:\n        block_size (int): How many tokens to divide the total output tokens into for power sampling. number of blocks = (sampler.sampling_params.max_tokens)/block_size. Smaller block sizes = better quality but slower\n        MCMC_steps (int): Number of MCMC steps to perform per block. More steps = better quality but slower\n        token_metric (str): Metric to use for token selection. Can be \"logprobs\", \"power_distribution\", \"entropy\", or \"likelihood_confidence\"\n    \"\"\"\n    def __init__(\n        self, \n        block_size: int = 192, # How many tokens to divide the total output tokens into for power sampling. Smaller block sizes = better quality but slower\n        MCMC_steps: int = 8, # Number of MCMC steps to perform per block. More steps = better quality but slower\n        token_metric: str = \"power_distribution\"\n    ):\n        self.block_size = block_size\n        self.MCMC_steps = MCMC_steps\n        self.token_metric = token_metric\n\n    # TODO Implement entropy as a MCMC acceptance ratio metric\n    # TODO Implement a PRM as a MCMC acceptance ratio metric\n    # TODO Implement a separate temperature for the Power Distribution metric\n    # Power Sampling method \n    def sample(\n        self, \n        sampler: AutoregressiveSampler, \n        prompt: str,\n        logging: bool = False,\n        log_file_path: str = None\n    )-&gt; Output:\n        \"\"\"\n        Sample using power sampling.\n\n        Args:\n            sampler (AutoregressiveSampler): The sampler object containing sampling parameters and the LLM engine.\n            prompt (str): The prompt to sample from.\n            logging (bool, optional): Whether to log the sampling process. Defaults to False.\n            log_file_path (str, optional): The path to the log file. Defaults to None.\n        Returns:\n            Output (Output): The output of the sampling process.\n        \"\"\"\n        # Set the random seed for reproducibility\n        if sampler.sampling_params.seed is not None:\n            np.random.seed(sampler.sampling_params.seed)\n            random.seed(sampler.sampling_params.seed)\n\n        # Statistic Logging in a CSV\n        if(logging):\n            #create or overwrite log file\n            power_sampling_log_path = log_file_path if log_file_path is not None else f\"power_sampling_log_{time.strftime('%H%M%S_%d_%m_%Y')}.csv\"\n            with open(power_sampling_log_path, \"w\") as log_file:\n                # Extract backslash expressions for Python 3.10 compatibility\n                sampler_json = json.dumps(vars(sampler), default=str).replace('\"', '\"\"')\n                prompt_escaped = prompt.replace('\"', '\"\"')\n                log_file.write(f'\"{sampler_json}\"\\n')\n                log_file.write(f'\"{prompt_escaped}\"\\n')\n                log_file.write(\"proposed_target_distribution_sum,proposed_sampling_distribution_sum,current_target_distribution_sum,current_sampling_distribution_sum,new_target_distribution_normalized,new_sampling_distribution_normalized,acceptance_ratio,accepted,starting_index,tokens_generated,\\n\")\n\n        # Intialize arrays to store the probabilities of the current tokens\n        current_target_distribution = [] # Current list of unscaled log probabilities of the new sample. Length of block_size\n        current_sampling_distribution = [] # Current list of tokens probabilities individually scaled by temperature. Length of block_size\n\n        # New Context Window to be changed\n        context = []\n        logits = []\n        logprobs = []\n        unprocessed_log_normalization_constant = []\n        temp_processed_log_normalization_constant = []\n        entropy = []\n\n        # Number of blocks to be sampled\n        block_count = sampler.sampling_params.max_tokens // self.block_size\n        sampler_max_tokens = sampler.sampling_params.max_tokens\n        for block_idx in range(block_count):\n            # Set the max tokens for the block\n            sampler.sampling_params.max_tokens = self.block_size\n\n            # Sample the initial new tokens for the block\n            output = sampler.sample(prompt +  sampler.tokenizer.decode(context, skip_special_tokens=False))\n\n            # Calculate the log probabilities of the initial new tokens for the block\n            target_distribution = calc_token_metric(output, sampler, self.token_metric)\n            sampling_distribution = calc_token_metric(output, sampler, \"logprobs\")\n\n            # Extend the distributions\n            current_target_distribution = [*current_target_distribution, *target_distribution.tolist()]\n            current_sampling_distribution = [*current_sampling_distribution, *sampling_distribution.tolist()]\n\n            # Extend the context with the newly generated tokens\n            context.extend(output.tokens)\n            # Extend the other Output attributes along\n            logits.extend(output.top_k_logits)\n            logprobs.extend(output.top_k_logprobs)\n            unprocessed_log_normalization_constant.extend(output.unprocessed_log_normalization_constant)\n            temp_processed_log_normalization_constant.extend(output.temp_processed_log_normalization_constant)\n            entropy.extend(output.entropy)\n\n            # Log Results\n            if(logging):\n                proposed_target_distribution_sum = \"None\"\n                proposed_sampling_distribution_sum = \"None \"\n                current_target_distribution_sum = \"None\"\n                current_sampling_distribution_sum = \"None\"\n                new_target_distribution_normalized = sum(target_distribution)/len(target_distribution)\n                new_sampling_distribution_normalized = sum(sampling_distribution)/len(sampling_distribution)\n                acceptance_ratio = \"None\"\n                accepted = \"None\"\n                tokens_generated = len(output.tokens)\n                starting_index = len(context)-tokens_generated\n                # Write initial generated block data to log\n                with open(power_sampling_log_path, \"a\") as log_file:\n                    log_file.write(f\"{proposed_target_distribution_sum},{proposed_sampling_distribution_sum},{current_target_distribution_sum},{current_sampling_distribution_sum},{new_target_distribution_normalized},{new_sampling_distribution_normalized},{acceptance_ratio},{accepted},{starting_index},{tokens_generated}\\n\")\n\n            # Perform the MCMC Steps to hone in on the target distribution\n            for _ in range(self.MCMC_steps):\n                #Find a new point to start a proposal from. Generate idx tokens for the step.\n                idx = random.randint(0, len(context) - 1)\n\n                #Set the new context for the proposed block\n                context_proposed = context[:idx]\n\n                # Set the tokens to generate\n                sampler.sampling_params.max_tokens = len(context) - idx\n                #Generate proposed block of tokens\n                output = sampler.sample(prompt +  sampler.tokenizer.decode(context_proposed, skip_special_tokens=False))\n                #Find the proposed probability distributions \n                proposed_target_distribution = calc_token_metric(output, sampler, self.token_metric)\n                proposed_sampling_distribution = calc_token_metric(output, sampler, \"logprobs\")\n\n                #TODO Compare the log_acceptance_ratio summations to those calculated using calc_sequence_logprob\n                # Calculate the Metro-Hastings acceptance ratio\n                # Power Scaled Sequence Log Probability + Temperature Scaled Sequence Log Probability - Current Power Scaled Sequence Log Probability - Current Temperature Scaled Sequence Log Probability\n                log_acceptance_ratio = sum(proposed_target_distribution) + sum(current_sampling_distribution[idx:idx+len(output.tokens)]) - sum(current_target_distribution[idx:idx+len(output.tokens)]) - sum(proposed_sampling_distribution)\n\n                # Check to make sure we are comparing the correct number of elements\n                assert(len(proposed_target_distribution) == len(current_sampling_distribution[idx:idx+len(output.tokens)]) == len(current_target_distribution[idx:idx+len(output.tokens)]) == len(proposed_sampling_distribution))\n\n                # Log the logprobs and acceptance ratio\n                if(logging):\n                    proposed_target_distribution_sum = sum(proposed_target_distribution)\n                    proposed_sampling_distribution_sum = sum(proposed_sampling_distribution)\n                    current_target_distribution_sum = sum(current_target_distribution[idx:idx+len(output.tokens)])\n                    current_sampling_distribution_sum = sum(current_sampling_distribution[idx:idx+len(output.tokens)])\n\n                    # Write initial generated block data to log\n                    with open(power_sampling_log_path, \"a\") as log_file:\n                        log_file.write(f\"{proposed_target_distribution_sum},{proposed_sampling_distribution_sum},{current_target_distribution_sum},{current_sampling_distribution_sum},\")\n\n                acceptance = False\n                # Accept or reject the proposed block based on the acceptance ratio\n                if np.random.rand() &lt; np.exp(log_acceptance_ratio):\n                    # Ensure the context is updated with the accepted proposal\n                    context[idx:] = output.tokens\n                    # Replace the tail of the other Output attributes along with the context\n                    logits = safe_concat(logits, output.top_k_logits, idx)\n                    logprobs = safe_concat(logprobs, output.top_k_logprobs, idx)\n                    unprocessed_log_normalization_constant = safe_concat(\n                        unprocessed_log_normalization_constant, output.unprocessed_log_normalization_constant, idx\n                    )\n                    temp_processed_log_normalization_constant = safe_concat(\n                        temp_processed_log_normalization_constant, output.temp_processed_log_normalization_constant, idx\n                    )\n                    entropy = safe_concat(entropy, output.entropy, idx)\n\n                    # Update the logprob lists with the accepted proposal's log probabilities\n                    current_target_distribution = [*current_target_distribution[:idx], *proposed_target_distribution]\n                    current_sampling_distribution = [*current_sampling_distribution[:idx], *proposed_sampling_distribution]\n\n                    # Flag acceptance\n                    acceptance = True\n\n                # Log the new distributions and acceptance ratio\n                if(logging):\n                    current_target_distribution_norm = sum(current_target_distribution)/len(current_target_distribution)\n                    current_sampling_distribution_norm = sum(current_sampling_distribution)/len(current_sampling_distribution)\n                    acceptance_ratio = np.exp(log_acceptance_ratio)\n                    accepted = acceptance\n                    tokens_generated = len(output.tokens)\n                    starting_index = idx\n                    with open(power_sampling_log_path, \"a\") as log_file:\n                        log_file.write(f\"{current_target_distribution_norm},{current_sampling_distribution_norm},{acceptance_ratio},{accepted},{starting_index},{tokens_generated}\\n\")\n\n            # Check if an EOS token has been generated and end the process if so\n            if(sampler.tokenizer.eos_token_id in context):\n                decoded_text = sampler.tokenizer.decode(context, skip_special_tokens=False)\n                if logging:\n                    with open(power_sampling_log_path, \"a\") as log_file:\n                        # Extract backslash expression for Python 3.10 compatibility\n                        decoded_escaped = decoded_text.replace('\"', '\"\"')\n                        log_file.write(f'\"{decoded_escaped}\"\\n')\n                # Set the max_new_tokens back to the original value\n                sampler.sampling_params.max_tokens = sampler_max_tokens \n                return Output(tokens=context,top_k_logits=logits,top_k_logprobs=logprobs,unprocessed_log_normalization_constant=unprocessed_log_normalization_constant,temp_processed_log_normalization_constant=temp_processed_log_normalization_constant,entropy=entropy)\n\n\n        # EOS never found, just return the full generated context\n        decoded_text = sampler.tokenizer.decode(context, skip_special_tokens=False)\n        if logging:\n            with open(power_sampling_log_path, \"a\") as log_file:\n                # Extract backslash expression for Python 3.10 compatibility\n                decoded_escaped = decoded_text.replace('\"', '\"\"')\n                log_file.write(f'\"{decoded_escaped}\"\\n')\n        # Set the max_tokens back to the original value\n        sampler.sampling_params.max_tokens = sampler_max_tokens \n        return Output(tokens=context,top_k_logits=logits,top_k_logprobs=logprobs,unprocessed_log_normalization_constant=unprocessed_log_normalization_constant,temp_processed_log_normalization_constant=temp_processed_log_normalization_constant,entropy=entropy)\n</code></pre>"},{"location":"sampling/power_sample/#pita.sampling.power_sample.Power_Sampling.sample","title":"<code>sample(sampler: AutoregressiveSampler, prompt: str, logging: bool = False, log_file_path: str = None) -&gt; Output</code>","text":"<p>Sample using power sampling.</p> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>AutoregressiveSampler</code> <p>The sampler object containing sampling parameters and the LLM engine.</p> required <code>prompt</code> <code>str</code> <p>The prompt to sample from.</p> required <code>logging</code> <code>bool</code> <p>Whether to log the sampling process. Defaults to False.</p> <code>False</code> <code>log_file_path</code> <code>str</code> <p>The path to the log file. Defaults to None.</p> <code>None</code> <p>Returns:     Output (Output): The output of the sampling process.</p> Source code in <code>pita/sampling/power_sample.py</code> <pre><code>def sample(\n    self, \n    sampler: AutoregressiveSampler, \n    prompt: str,\n    logging: bool = False,\n    log_file_path: str = None\n)-&gt; Output:\n    \"\"\"\n    Sample using power sampling.\n\n    Args:\n        sampler (AutoregressiveSampler): The sampler object containing sampling parameters and the LLM engine.\n        prompt (str): The prompt to sample from.\n        logging (bool, optional): Whether to log the sampling process. Defaults to False.\n        log_file_path (str, optional): The path to the log file. Defaults to None.\n    Returns:\n        Output (Output): The output of the sampling process.\n    \"\"\"\n    # Set the random seed for reproducibility\n    if sampler.sampling_params.seed is not None:\n        np.random.seed(sampler.sampling_params.seed)\n        random.seed(sampler.sampling_params.seed)\n\n    # Statistic Logging in a CSV\n    if(logging):\n        #create or overwrite log file\n        power_sampling_log_path = log_file_path if log_file_path is not None else f\"power_sampling_log_{time.strftime('%H%M%S_%d_%m_%Y')}.csv\"\n        with open(power_sampling_log_path, \"w\") as log_file:\n            # Extract backslash expressions for Python 3.10 compatibility\n            sampler_json = json.dumps(vars(sampler), default=str).replace('\"', '\"\"')\n            prompt_escaped = prompt.replace('\"', '\"\"')\n            log_file.write(f'\"{sampler_json}\"\\n')\n            log_file.write(f'\"{prompt_escaped}\"\\n')\n            log_file.write(\"proposed_target_distribution_sum,proposed_sampling_distribution_sum,current_target_distribution_sum,current_sampling_distribution_sum,new_target_distribution_normalized,new_sampling_distribution_normalized,acceptance_ratio,accepted,starting_index,tokens_generated,\\n\")\n\n    # Intialize arrays to store the probabilities of the current tokens\n    current_target_distribution = [] # Current list of unscaled log probabilities of the new sample. Length of block_size\n    current_sampling_distribution = [] # Current list of tokens probabilities individually scaled by temperature. Length of block_size\n\n    # New Context Window to be changed\n    context = []\n    logits = []\n    logprobs = []\n    unprocessed_log_normalization_constant = []\n    temp_processed_log_normalization_constant = []\n    entropy = []\n\n    # Number of blocks to be sampled\n    block_count = sampler.sampling_params.max_tokens // self.block_size\n    sampler_max_tokens = sampler.sampling_params.max_tokens\n    for block_idx in range(block_count):\n        # Set the max tokens for the block\n        sampler.sampling_params.max_tokens = self.block_size\n\n        # Sample the initial new tokens for the block\n        output = sampler.sample(prompt +  sampler.tokenizer.decode(context, skip_special_tokens=False))\n\n        # Calculate the log probabilities of the initial new tokens for the block\n        target_distribution = calc_token_metric(output, sampler, self.token_metric)\n        sampling_distribution = calc_token_metric(output, sampler, \"logprobs\")\n\n        # Extend the distributions\n        current_target_distribution = [*current_target_distribution, *target_distribution.tolist()]\n        current_sampling_distribution = [*current_sampling_distribution, *sampling_distribution.tolist()]\n\n        # Extend the context with the newly generated tokens\n        context.extend(output.tokens)\n        # Extend the other Output attributes along\n        logits.extend(output.top_k_logits)\n        logprobs.extend(output.top_k_logprobs)\n        unprocessed_log_normalization_constant.extend(output.unprocessed_log_normalization_constant)\n        temp_processed_log_normalization_constant.extend(output.temp_processed_log_normalization_constant)\n        entropy.extend(output.entropy)\n\n        # Log Results\n        if(logging):\n            proposed_target_distribution_sum = \"None\"\n            proposed_sampling_distribution_sum = \"None \"\n            current_target_distribution_sum = \"None\"\n            current_sampling_distribution_sum = \"None\"\n            new_target_distribution_normalized = sum(target_distribution)/len(target_distribution)\n            new_sampling_distribution_normalized = sum(sampling_distribution)/len(sampling_distribution)\n            acceptance_ratio = \"None\"\n            accepted = \"None\"\n            tokens_generated = len(output.tokens)\n            starting_index = len(context)-tokens_generated\n            # Write initial generated block data to log\n            with open(power_sampling_log_path, \"a\") as log_file:\n                log_file.write(f\"{proposed_target_distribution_sum},{proposed_sampling_distribution_sum},{current_target_distribution_sum},{current_sampling_distribution_sum},{new_target_distribution_normalized},{new_sampling_distribution_normalized},{acceptance_ratio},{accepted},{starting_index},{tokens_generated}\\n\")\n\n        # Perform the MCMC Steps to hone in on the target distribution\n        for _ in range(self.MCMC_steps):\n            #Find a new point to start a proposal from. Generate idx tokens for the step.\n            idx = random.randint(0, len(context) - 1)\n\n            #Set the new context for the proposed block\n            context_proposed = context[:idx]\n\n            # Set the tokens to generate\n            sampler.sampling_params.max_tokens = len(context) - idx\n            #Generate proposed block of tokens\n            output = sampler.sample(prompt +  sampler.tokenizer.decode(context_proposed, skip_special_tokens=False))\n            #Find the proposed probability distributions \n            proposed_target_distribution = calc_token_metric(output, sampler, self.token_metric)\n            proposed_sampling_distribution = calc_token_metric(output, sampler, \"logprobs\")\n\n            #TODO Compare the log_acceptance_ratio summations to those calculated using calc_sequence_logprob\n            # Calculate the Metro-Hastings acceptance ratio\n            # Power Scaled Sequence Log Probability + Temperature Scaled Sequence Log Probability - Current Power Scaled Sequence Log Probability - Current Temperature Scaled Sequence Log Probability\n            log_acceptance_ratio = sum(proposed_target_distribution) + sum(current_sampling_distribution[idx:idx+len(output.tokens)]) - sum(current_target_distribution[idx:idx+len(output.tokens)]) - sum(proposed_sampling_distribution)\n\n            # Check to make sure we are comparing the correct number of elements\n            assert(len(proposed_target_distribution) == len(current_sampling_distribution[idx:idx+len(output.tokens)]) == len(current_target_distribution[idx:idx+len(output.tokens)]) == len(proposed_sampling_distribution))\n\n            # Log the logprobs and acceptance ratio\n            if(logging):\n                proposed_target_distribution_sum = sum(proposed_target_distribution)\n                proposed_sampling_distribution_sum = sum(proposed_sampling_distribution)\n                current_target_distribution_sum = sum(current_target_distribution[idx:idx+len(output.tokens)])\n                current_sampling_distribution_sum = sum(current_sampling_distribution[idx:idx+len(output.tokens)])\n\n                # Write initial generated block data to log\n                with open(power_sampling_log_path, \"a\") as log_file:\n                    log_file.write(f\"{proposed_target_distribution_sum},{proposed_sampling_distribution_sum},{current_target_distribution_sum},{current_sampling_distribution_sum},\")\n\n            acceptance = False\n            # Accept or reject the proposed block based on the acceptance ratio\n            if np.random.rand() &lt; np.exp(log_acceptance_ratio):\n                # Ensure the context is updated with the accepted proposal\n                context[idx:] = output.tokens\n                # Replace the tail of the other Output attributes along with the context\n                logits = safe_concat(logits, output.top_k_logits, idx)\n                logprobs = safe_concat(logprobs, output.top_k_logprobs, idx)\n                unprocessed_log_normalization_constant = safe_concat(\n                    unprocessed_log_normalization_constant, output.unprocessed_log_normalization_constant, idx\n                )\n                temp_processed_log_normalization_constant = safe_concat(\n                    temp_processed_log_normalization_constant, output.temp_processed_log_normalization_constant, idx\n                )\n                entropy = safe_concat(entropy, output.entropy, idx)\n\n                # Update the logprob lists with the accepted proposal's log probabilities\n                current_target_distribution = [*current_target_distribution[:idx], *proposed_target_distribution]\n                current_sampling_distribution = [*current_sampling_distribution[:idx], *proposed_sampling_distribution]\n\n                # Flag acceptance\n                acceptance = True\n\n            # Log the new distributions and acceptance ratio\n            if(logging):\n                current_target_distribution_norm = sum(current_target_distribution)/len(current_target_distribution)\n                current_sampling_distribution_norm = sum(current_sampling_distribution)/len(current_sampling_distribution)\n                acceptance_ratio = np.exp(log_acceptance_ratio)\n                accepted = acceptance\n                tokens_generated = len(output.tokens)\n                starting_index = idx\n                with open(power_sampling_log_path, \"a\") as log_file:\n                    log_file.write(f\"{current_target_distribution_norm},{current_sampling_distribution_norm},{acceptance_ratio},{accepted},{starting_index},{tokens_generated}\\n\")\n\n        # Check if an EOS token has been generated and end the process if so\n        if(sampler.tokenizer.eos_token_id in context):\n            decoded_text = sampler.tokenizer.decode(context, skip_special_tokens=False)\n            if logging:\n                with open(power_sampling_log_path, \"a\") as log_file:\n                    # Extract backslash expression for Python 3.10 compatibility\n                    decoded_escaped = decoded_text.replace('\"', '\"\"')\n                    log_file.write(f'\"{decoded_escaped}\"\\n')\n            # Set the max_new_tokens back to the original value\n            sampler.sampling_params.max_tokens = sampler_max_tokens \n            return Output(tokens=context,top_k_logits=logits,top_k_logprobs=logprobs,unprocessed_log_normalization_constant=unprocessed_log_normalization_constant,temp_processed_log_normalization_constant=temp_processed_log_normalization_constant,entropy=entropy)\n\n\n    # EOS never found, just return the full generated context\n    decoded_text = sampler.tokenizer.decode(context, skip_special_tokens=False)\n    if logging:\n        with open(power_sampling_log_path, \"a\") as log_file:\n            # Extract backslash expression for Python 3.10 compatibility\n            decoded_escaped = decoded_text.replace('\"', '\"\"')\n            log_file.write(f'\"{decoded_escaped}\"\\n')\n    # Set the max_tokens back to the original value\n    sampler.sampling_params.max_tokens = sampler_max_tokens \n    return Output(tokens=context,top_k_logits=logits,top_k_logprobs=logprobs,unprocessed_log_normalization_constant=unprocessed_log_normalization_constant,temp_processed_log_normalization_constant=temp_processed_log_normalization_constant,entropy=entropy)\n</code></pre>"},{"location":"sampling/smc/","title":"SMC","text":""},{"location":"sampling/smc/#pita.sampling.smc.Sequential_Monte_Carlo","title":"<code>Sequential_Monte_Carlo</code>","text":"<p>Sequential Monte Carlo (SMC) is a multi-particle sampling method that uses a probability metric to iteratively update a set of particles.</p> <p>Parameters:</p> Name Type Description Default <code>num_particles</code> <code>int</code> <p>The number of particles to use.</p> <code>10</code> <code>tokens_per_step</code> <code>int</code> <p>The number of tokens to generate per step.</p> <code>5</code> <code>stop_on_eos</code> <code>bool</code> <p>Whether to stop sampling when the end of the sequence is reached.</p> <code>True</code> <code>token_metric</code> <code>str</code> <p>The probability metric to use.</p> <code>'logprobs'</code> <code>aggregation</code> <code>str</code> <p>The aggregation method to use. Can be 'last', 'minimum', 'product', or 'model_aggregate'.</p> <code>'last'</code> <code>token_sampling_method</code> <code>str</code> <p>The token sampling method to use. By default, the standard token sampling method is used. However, token_sample can be used instead.</p> <code>'standard'</code> Source code in <code>pita/sampling/smc.py</code> <pre><code>class Sequential_Monte_Carlo:\n    \"\"\"\n    Sequential Monte Carlo (SMC) is a multi-particle sampling method that uses a probability metric to iteratively update a set of particles.\n\n    Args:\n        num_particles (int): The number of particles to use.\n        tokens_per_step (int): The number of tokens to generate per step.\n        stop_on_eos (bool): Whether to stop sampling when the end of the sequence is reached.\n        token_metric (str): The probability metric to use.\n        aggregation (str): The aggregation method to use. Can be 'last', 'minimum', 'product', or 'model_aggregate'.\n        token_sampling_method (str): The token sampling method to use. By default, the standard token sampling method is used. However, token_sample can be used instead. \n    \"\"\"\n    def __init__(\n        self, \n        num_particles: int = 10, \n        tokens_per_step: int = 5, \n        stop_on_eos: bool = True,\n        token_metric: str = \"logprobs\",\n        aggregation: str = \"last\",\n        token_sampling_method: str = \"standard\"\n    ):\n        self.num_particles = num_particles\n        self.tokens_per_step = tokens_per_step\n        self.stop_on_eos = stop_on_eos\n        self.token_metric = token_metric\n        self.aggregation = aggregation\n        self.token_sampling_method = token_sampling_method\n\n    def score_update(\n        self, \n        token_values: list[float],\n        token_count: int,\n        step_scores: list[float]\n    ) -&gt; float:\n        \"\"\"\n        Update the particle score and stored step score for the new token scores.\n\n        Args:\n            token_values (list[float]): All of the token values so far. Could be logprobs, power_distribution, or entropy\n            token_count (int): The number of tokens to use.\n            step_scores (list[float]): The stored step scores.\n\n        Returns:\n            float: The new particle score.\n        \"\"\"\n        if(self.token_metric == \"logprobs\" or self.token_metric == \"power_distribution\"):\n            if(self.aggregation == \"last\"):\n                step_scores.append(sum(token_values[-token_count:])/token_count)\n                return step_scores[-1]\n            elif(self.aggregation == \"minimum\"):\n                step_scores.append(sum(token_values[-token_count:])/token_count)\n                return min(step_scores)\n            elif(self.aggregation == \"product\"):\n                step_scores.append(sum(token_values[-token_count:])/token_count)\n                return -1*abs(math.prod(step_scores))\n            elif(self.aggregation == \"model_aggregate\"):\n                step_scores.append(sum(token_values[-token_count:])/token_count)\n                return sum(token_values)/len(token_values)\n            else:\n                raise ValueError(f\"Invalid aggregation method: {self.aggregation}\")\n        elif(self.token_metric == \"entropy\"):\n            # As a high entropy is less desirable, we negate the value or take the maximum value before negating\n            if(self.aggregation == \"last\"):\n                step_scores.append(-1 * sum(token_values[-token_count:])/token_count)\n                return step_scores[-1]\n            elif(self.aggregation == \"minimum\"):\n                step_scores.append(-1 * sum(token_values[-token_count:])/token_count)\n                return min(step_scores)\n            elif(self.aggregation == \"product\"):\n                step_scores.append(-1 * sum(token_values[-token_count:])/token_count)\n                return -1 * abs(math.prod(step_scores))\n            elif(self.aggregation == \"model_aggregate\"):\n                step_scores.append(-1 * sum(token_values[-token_count:])/token_count)\n                return -1 * sum(token_values)/len(token_values)\n            else:\n                raise ValueError(f\"Invalid aggregation method: {self.aggregation}\")\n        else:\n            raise ValueError(f\"Invalid token metric: {self.token_metric}\")\n    def particle_sampling(\n        self,\n        particle_scores: list[float] | np.ndarray,\n        finished: list[bool] | np.ndarray,\n    )-&gt; list[int]:\n        \"\"\"\n        Given a list of particle scores (particle_score), return a list of the new particles to use based off the softmax of the particle scores and multinomial sampling.\n        Skip any particles that have finished.\n\n        Args:\n            particle_scores (list[float]): The list of particle scores.\n            finished (list[bool]): The list of finished flags.\n\n        Returns:\n            list[int]: A list with each element being the new index of the particle to use.\n        \"\"\"\n        # Find the indices of the unfinished particles\n        unfinished_indices = np.where(np.logical_not(finished))[0]\n\n        # If all particles are finished, return the current particles\n        if len(unfinished_indices) == 0:\n            return list(range(self.num_particles))\n\n        # Exponentiate the scores of unfinished particles (softmax numerator)\n        particle_score_exp = np.exp(np.array(particle_scores)[unfinished_indices])\n\n        # Calculate the sum of exponentiated scores for normalization\n        particle_score_normalization_constant = np.sum(particle_score_exp)\n\n        # Normalize the particle scores\n        normalized_scores = particle_score_exp / particle_score_normalization_constant\n\n        # Choose the new particle with multinomial sampling skipping any finished particles\n        new_particles = np.random.choice(unfinished_indices, size=self.num_particles, p=normalized_scores)\n\n        # Make sure the finished particles are propagated forward\n        for i in range(self.num_particles):\n            if finished[i]:\n                new_particles[i] = i\n\n\n        return new_particles.tolist()\n    def update_particles(\n        self,\n        new_particles: list[int],\n        outputs: list[Output],\n        finished: list[bool],\n        token_metric_scores: list[list[float]],\n        step_scores: list[list[float]]\n    ) -&gt; None:\n        \"\"\"\n        Update the particles based on the newly SMC sampled particles by updating the outputs, token_metric_scores, and step_scores \n\n        Args:\n            new_particles (list[int]): The list of indices of the new particles to use.\n            outputs (list[Output]): The current list of outputs to be updated.\n            finished (list[bool]): The current list of finished flags to be updated.\n            token_metric_scores (list[list[float]]): The current list of token metric scores to be updated for each particle.\n            step_scores (list[list[float]]): The current list of step scores to be updated for each particle.\n\n        \"\"\"\n        # Save the particles that will be carried forward\n        # Find the unique indices in new_particles avoiding any finished particles\n        unique_indices = np.unique(new_particles)\n        for i in range(self.num_particles):\n            if finished[i]:\n                unique_indices = unique_indices[unique_indices != i]\n\n        # Save the outputs, finished, token_metric_scores, and step_scores for the unique indices in dictionaries\n        saved_outputs = {i: outputs[i] for i in unique_indices}\n        saved_finished = {i: finished[i] for i in unique_indices}\n        saved_token_metric_scores = {i: token_metric_scores[i] for i in unique_indices}\n        saved_step_scores = {i: step_scores[i] for i in unique_indices}\n\n        for i in range(self.num_particles):\n            source_idx = new_particles[i]\n            # Check to see if the particle is different\n            if source_idx != i:\n                # Copy the saved particle to the current particle\n                # Use deepcopy to avoid shared mutable state\n                outputs[i] = copy.deepcopy(saved_outputs[source_idx])\n                finished[i] = saved_finished[source_idx]\n                token_metric_scores[i] = copy.deepcopy(saved_token_metric_scores[source_idx])\n                step_scores[i] = copy.deepcopy(saved_step_scores[source_idx])\n\n    # TODO add support for a PRM token metric\n    def sample(\n        self,\n        sampler: AutoregressiveSampler,\n        prompt: str\n    ) -&gt; Output:\n        \"\"\"\n        Samples using SMC and its parameters.\n\n        Args:\n            sampler (AutoregressiveSampler): The sampler object.\n            prompt (str): The prompt to sample from.\n        Returns:\n            Output: Standard output object for the PITA library.\n        \"\"\"\n        # Check the token sampling method\n        if self.token_sampling_method == \"standard\":\n            token_sampling = sampler.sample \n        elif self.token_sampling_method == \"token_sample\":\n            token_sampling = sampler.token_sample\n        else:\n            warnings.warn(\"Invalid token sampling method. Using standard token sampling method.\")\n            token_sampling = sampler.sample\n\n        # Save the total number of tokens to generate\n        total_tokens = sampler.sampling_params.max_tokens\n        # Set the number of tokens to generate per step\n        sampler.sampling_params.max_tokens = self.tokens_per_step\n\n        # SMC Steps\n        smc_steps = total_tokens // self.tokens_per_step\n\n        # Initialize each particle with an empty Output object configured for appending\n        outputs = [\n            Output(\n                tokens=[], \n                top_k_logits=[], \n                top_k_logprobs=[],\n                unprocessed_log_normalization_constant=[],\n                temp_processed_log_normalization_constant=[],\n                entropy=[]\n            ) \n            for _ in range(self.num_particles)\n        ]\n\n        # Create a list of the token metric scores for each particle\n        token_metric_scores = [[] for _ in range(self.num_particles)]\n        step_scores = [[] for _ in range(self.num_particles)]\n        # Create a list of the current particle probability\n        particle_scores = [0 for _ in range(self.num_particles)]\n\n        # Create a list to track if a particle has finished\n        finished = [False for _ in range(self.num_particles)]\n\n        # Find the EOS token ID\n        eos_id = sampler.tokenizer.eos_token_id\n\n        # Iterate through the SMC\n        for step in range(smc_steps):\n            for particle in range(self.num_particles):\n                # If the particle has finished, skip it\n                if finished[particle]:\n                    continue\n\n                # Sample the next token\n                sample_output = token_sampling(prompt + sampler.tokenizer.decode(outputs[particle].tokens, skip_special_tokens=True))\n\n                # Append the sample output to the particle's output\n                outputs[particle].append(sample_output)\n\n                # Calculate the token metric probabilities\n                token_metric_scores[particle].extend(\n                    np.ravel(calc_token_metric(sample_output, sampler, self.token_metric)).tolist()\n                )\n\n                # Calculate the current particle probability\n                particle_scores[particle] = self.score_update(token_metric_scores[particle], len(sample_output.tokens), step_scores[particle]) \n\n                # Check if the particle has finished \n                if self.stop_on_eos and eos_id in sample_output.tokens:\n                    finished[particle] = True\n\n            # Check if all particles are finished; if so, skip resampling and stop\n            if np.all(finished):\n                break\n\n            # Find the new list of particles to use\n            new_particles = self.particle_sampling(particle_scores, finished)\n            # Update the particles if not finished\n            self.update_particles(new_particles, outputs, finished, token_metric_scores, step_scores)\n\n        # Greedily select the best particle\n        best_particle = np.argmax(particle_scores)\n\n        # Restore the sampling parameters\n        sampler.sampling_params.max_tokens = total_tokens\n\n        # Return the best particle\n        return outputs[best_particle]\n</code></pre>"},{"location":"sampling/smc/#pita.sampling.smc.Sequential_Monte_Carlo.particle_sampling","title":"<code>particle_sampling(particle_scores: list[float] | np.ndarray, finished: list[bool] | np.ndarray) -&gt; list[int]</code>","text":"<p>Given a list of particle scores (particle_score), return a list of the new particles to use based off the softmax of the particle scores and multinomial sampling. Skip any particles that have finished.</p> <p>Parameters:</p> Name Type Description Default <code>particle_scores</code> <code>list[float]</code> <p>The list of particle scores.</p> required <code>finished</code> <code>list[bool]</code> <p>The list of finished flags.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>list[int]: A list with each element being the new index of the particle to use.</p> Source code in <code>pita/sampling/smc.py</code> <pre><code>def particle_sampling(\n    self,\n    particle_scores: list[float] | np.ndarray,\n    finished: list[bool] | np.ndarray,\n)-&gt; list[int]:\n    \"\"\"\n    Given a list of particle scores (particle_score), return a list of the new particles to use based off the softmax of the particle scores and multinomial sampling.\n    Skip any particles that have finished.\n\n    Args:\n        particle_scores (list[float]): The list of particle scores.\n        finished (list[bool]): The list of finished flags.\n\n    Returns:\n        list[int]: A list with each element being the new index of the particle to use.\n    \"\"\"\n    # Find the indices of the unfinished particles\n    unfinished_indices = np.where(np.logical_not(finished))[0]\n\n    # If all particles are finished, return the current particles\n    if len(unfinished_indices) == 0:\n        return list(range(self.num_particles))\n\n    # Exponentiate the scores of unfinished particles (softmax numerator)\n    particle_score_exp = np.exp(np.array(particle_scores)[unfinished_indices])\n\n    # Calculate the sum of exponentiated scores for normalization\n    particle_score_normalization_constant = np.sum(particle_score_exp)\n\n    # Normalize the particle scores\n    normalized_scores = particle_score_exp / particle_score_normalization_constant\n\n    # Choose the new particle with multinomial sampling skipping any finished particles\n    new_particles = np.random.choice(unfinished_indices, size=self.num_particles, p=normalized_scores)\n\n    # Make sure the finished particles are propagated forward\n    for i in range(self.num_particles):\n        if finished[i]:\n            new_particles[i] = i\n\n\n    return new_particles.tolist()\n</code></pre>"},{"location":"sampling/smc/#pita.sampling.smc.Sequential_Monte_Carlo.sample","title":"<code>sample(sampler: AutoregressiveSampler, prompt: str) -&gt; Output</code>","text":"<p>Samples using SMC and its parameters.</p> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>AutoregressiveSampler</code> <p>The sampler object.</p> required <code>prompt</code> <code>str</code> <p>The prompt to sample from.</p> required <p>Returns:     Output: Standard output object for the PITA library.</p> Source code in <code>pita/sampling/smc.py</code> <pre><code>def sample(\n    self,\n    sampler: AutoregressiveSampler,\n    prompt: str\n) -&gt; Output:\n    \"\"\"\n    Samples using SMC and its parameters.\n\n    Args:\n        sampler (AutoregressiveSampler): The sampler object.\n        prompt (str): The prompt to sample from.\n    Returns:\n        Output: Standard output object for the PITA library.\n    \"\"\"\n    # Check the token sampling method\n    if self.token_sampling_method == \"standard\":\n        token_sampling = sampler.sample \n    elif self.token_sampling_method == \"token_sample\":\n        token_sampling = sampler.token_sample\n    else:\n        warnings.warn(\"Invalid token sampling method. Using standard token sampling method.\")\n        token_sampling = sampler.sample\n\n    # Save the total number of tokens to generate\n    total_tokens = sampler.sampling_params.max_tokens\n    # Set the number of tokens to generate per step\n    sampler.sampling_params.max_tokens = self.tokens_per_step\n\n    # SMC Steps\n    smc_steps = total_tokens // self.tokens_per_step\n\n    # Initialize each particle with an empty Output object configured for appending\n    outputs = [\n        Output(\n            tokens=[], \n            top_k_logits=[], \n            top_k_logprobs=[],\n            unprocessed_log_normalization_constant=[],\n            temp_processed_log_normalization_constant=[],\n            entropy=[]\n        ) \n        for _ in range(self.num_particles)\n    ]\n\n    # Create a list of the token metric scores for each particle\n    token_metric_scores = [[] for _ in range(self.num_particles)]\n    step_scores = [[] for _ in range(self.num_particles)]\n    # Create a list of the current particle probability\n    particle_scores = [0 for _ in range(self.num_particles)]\n\n    # Create a list to track if a particle has finished\n    finished = [False for _ in range(self.num_particles)]\n\n    # Find the EOS token ID\n    eos_id = sampler.tokenizer.eos_token_id\n\n    # Iterate through the SMC\n    for step in range(smc_steps):\n        for particle in range(self.num_particles):\n            # If the particle has finished, skip it\n            if finished[particle]:\n                continue\n\n            # Sample the next token\n            sample_output = token_sampling(prompt + sampler.tokenizer.decode(outputs[particle].tokens, skip_special_tokens=True))\n\n            # Append the sample output to the particle's output\n            outputs[particle].append(sample_output)\n\n            # Calculate the token metric probabilities\n            token_metric_scores[particle].extend(\n                np.ravel(calc_token_metric(sample_output, sampler, self.token_metric)).tolist()\n            )\n\n            # Calculate the current particle probability\n            particle_scores[particle] = self.score_update(token_metric_scores[particle], len(sample_output.tokens), step_scores[particle]) \n\n            # Check if the particle has finished \n            if self.stop_on_eos and eos_id in sample_output.tokens:\n                finished[particle] = True\n\n        # Check if all particles are finished; if so, skip resampling and stop\n        if np.all(finished):\n            break\n\n        # Find the new list of particles to use\n        new_particles = self.particle_sampling(particle_scores, finished)\n        # Update the particles if not finished\n        self.update_particles(new_particles, outputs, finished, token_metric_scores, step_scores)\n\n    # Greedily select the best particle\n    best_particle = np.argmax(particle_scores)\n\n    # Restore the sampling parameters\n    sampler.sampling_params.max_tokens = total_tokens\n\n    # Return the best particle\n    return outputs[best_particle]\n</code></pre>"},{"location":"sampling/smc/#pita.sampling.smc.Sequential_Monte_Carlo.score_update","title":"<code>score_update(token_values: list[float], token_count: int, step_scores: list[float]) -&gt; float</code>","text":"<p>Update the particle score and stored step score for the new token scores.</p> <p>Parameters:</p> Name Type Description Default <code>token_values</code> <code>list[float]</code> <p>All of the token values so far. Could be logprobs, power_distribution, or entropy</p> required <code>token_count</code> <code>int</code> <p>The number of tokens to use.</p> required <code>step_scores</code> <code>list[float]</code> <p>The stored step scores.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The new particle score.</p> Source code in <code>pita/sampling/smc.py</code> <pre><code>def score_update(\n    self, \n    token_values: list[float],\n    token_count: int,\n    step_scores: list[float]\n) -&gt; float:\n    \"\"\"\n    Update the particle score and stored step score for the new token scores.\n\n    Args:\n        token_values (list[float]): All of the token values so far. Could be logprobs, power_distribution, or entropy\n        token_count (int): The number of tokens to use.\n        step_scores (list[float]): The stored step scores.\n\n    Returns:\n        float: The new particle score.\n    \"\"\"\n    if(self.token_metric == \"logprobs\" or self.token_metric == \"power_distribution\"):\n        if(self.aggregation == \"last\"):\n            step_scores.append(sum(token_values[-token_count:])/token_count)\n            return step_scores[-1]\n        elif(self.aggregation == \"minimum\"):\n            step_scores.append(sum(token_values[-token_count:])/token_count)\n            return min(step_scores)\n        elif(self.aggregation == \"product\"):\n            step_scores.append(sum(token_values[-token_count:])/token_count)\n            return -1*abs(math.prod(step_scores))\n        elif(self.aggregation == \"model_aggregate\"):\n            step_scores.append(sum(token_values[-token_count:])/token_count)\n            return sum(token_values)/len(token_values)\n        else:\n            raise ValueError(f\"Invalid aggregation method: {self.aggregation}\")\n    elif(self.token_metric == \"entropy\"):\n        # As a high entropy is less desirable, we negate the value or take the maximum value before negating\n        if(self.aggregation == \"last\"):\n            step_scores.append(-1 * sum(token_values[-token_count:])/token_count)\n            return step_scores[-1]\n        elif(self.aggregation == \"minimum\"):\n            step_scores.append(-1 * sum(token_values[-token_count:])/token_count)\n            return min(step_scores)\n        elif(self.aggregation == \"product\"):\n            step_scores.append(-1 * sum(token_values[-token_count:])/token_count)\n            return -1 * abs(math.prod(step_scores))\n        elif(self.aggregation == \"model_aggregate\"):\n            step_scores.append(-1 * sum(token_values[-token_count:])/token_count)\n            return -1 * sum(token_values)/len(token_values)\n        else:\n            raise ValueError(f\"Invalid aggregation method: {self.aggregation}\")\n    else:\n        raise ValueError(f\"Invalid token metric: {self.token_metric}\")\n</code></pre>"},{"location":"sampling/smc/#pita.sampling.smc.Sequential_Monte_Carlo.update_particles","title":"<code>update_particles(new_particles: list[int], outputs: list[Output], finished: list[bool], token_metric_scores: list[list[float]], step_scores: list[list[float]]) -&gt; None</code>","text":"<p>Update the particles based on the newly SMC sampled particles by updating the outputs, token_metric_scores, and step_scores </p> <p>Parameters:</p> Name Type Description Default <code>new_particles</code> <code>list[int]</code> <p>The list of indices of the new particles to use.</p> required <code>outputs</code> <code>list[Output]</code> <p>The current list of outputs to be updated.</p> required <code>finished</code> <code>list[bool]</code> <p>The current list of finished flags to be updated.</p> required <code>token_metric_scores</code> <code>list[list[float]]</code> <p>The current list of token metric scores to be updated for each particle.</p> required <code>step_scores</code> <code>list[list[float]]</code> <p>The current list of step scores to be updated for each particle.</p> required Source code in <code>pita/sampling/smc.py</code> <pre><code>def update_particles(\n    self,\n    new_particles: list[int],\n    outputs: list[Output],\n    finished: list[bool],\n    token_metric_scores: list[list[float]],\n    step_scores: list[list[float]]\n) -&gt; None:\n    \"\"\"\n    Update the particles based on the newly SMC sampled particles by updating the outputs, token_metric_scores, and step_scores \n\n    Args:\n        new_particles (list[int]): The list of indices of the new particles to use.\n        outputs (list[Output]): The current list of outputs to be updated.\n        finished (list[bool]): The current list of finished flags to be updated.\n        token_metric_scores (list[list[float]]): The current list of token metric scores to be updated for each particle.\n        step_scores (list[list[float]]): The current list of step scores to be updated for each particle.\n\n    \"\"\"\n    # Save the particles that will be carried forward\n    # Find the unique indices in new_particles avoiding any finished particles\n    unique_indices = np.unique(new_particles)\n    for i in range(self.num_particles):\n        if finished[i]:\n            unique_indices = unique_indices[unique_indices != i]\n\n    # Save the outputs, finished, token_metric_scores, and step_scores for the unique indices in dictionaries\n    saved_outputs = {i: outputs[i] for i in unique_indices}\n    saved_finished = {i: finished[i] for i in unique_indices}\n    saved_token_metric_scores = {i: token_metric_scores[i] for i in unique_indices}\n    saved_step_scores = {i: step_scores[i] for i in unique_indices}\n\n    for i in range(self.num_particles):\n        source_idx = new_particles[i]\n        # Check to see if the particle is different\n        if source_idx != i:\n            # Copy the saved particle to the current particle\n            # Use deepcopy to avoid shared mutable state\n            outputs[i] = copy.deepcopy(saved_outputs[source_idx])\n            finished[i] = saved_finished[source_idx]\n            token_metric_scores[i] = copy.deepcopy(saved_token_metric_scores[source_idx])\n            step_scores[i] = copy.deepcopy(saved_step_scores[source_idx])\n</code></pre>"},{"location":"utils/benchmarking_utils/","title":"Benchmarking","text":""},{"location":"utils/benchmarking_utils/#pita.utils.benchmarking_utils.benchmark_sampling","title":"<code>benchmark_sampling(llm: AutoregressiveSampler, system_message: str, question_list: list[str], answer_list: list[str], enable_thinking: bool, chat_template: bool, sampling_techniques: list[bool], max_questions: int = 0, output_file_name: str = 'math500_power_sampling_results.csv', **kwargs: Any) -&gt; None</code>","text":"<p>Benchmark different sampling techniques on a dataset of math problems.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>AutoregressiveSampler</code> <p>The AutoregressiveSampler instance to use for generation.</p> required <code>system_message</code> <code>str</code> <p>The system message to include in prompts.</p> required <code>question_list</code> <code>list[str]</code> <p>List of formatted questions to benchmark.</p> required <code>answer_list</code> <code>list[str]</code> <p>List of correct answers corresponding to the questions.</p> required <code>enable_thinking</code> <code>bool</code> <p>Whether to enable thinking mode in chat templates.</p> required <code>chat_template</code> <code>bool</code> <p>Whether to use chat template formatting for prompts.</p> required <code>sampling_techniques</code> <code>list[bool]</code> <p>List of booleans indicating which sampling techniques to use. [0]: Naive sampling (temperature=1.0) [1]: Low temperature sampling [2]: Power sampling (MCMC)</p> required <code>max_questions</code> <code>int</code> <p>Maximum number of questions to process. 0 means process all.</p> <code>0</code> <code>output_file_name</code> <code>str</code> <p>Path to the output CSV file for results.</p> <code>'math500_power_sampling_results.csv'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to sampling functions. log_file_path: Base directory for logging individual question results.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None. Results are written to the output CSV file.</p> Source code in <code>pita/utils/benchmarking_utils.py</code> <pre><code>def benchmark_sampling(\n    llm: AutoregressiveSampler,\n    system_message: str,\n    question_list: list[str],\n    answer_list: list[str],\n    enable_thinking: bool,\n    chat_template: bool,\n    sampling_techniques: list[bool],\n    max_questions: int = 0,\n    output_file_name: str = \"math500_power_sampling_results.csv\",\n    **kwargs: Any\n) -&gt; None:\n    \"\"\"\n    Benchmark different sampling techniques on a dataset of math problems.\n\n    Args:\n        llm: The AutoregressiveSampler instance to use for generation.\n        system_message: The system message to include in prompts.\n        question_list: List of formatted questions to benchmark.\n        answer_list: List of correct answers corresponding to the questions.\n        enable_thinking: Whether to enable thinking mode in chat templates.\n        chat_template: Whether to use chat template formatting for prompts.\n        sampling_techniques: List of booleans indicating which sampling techniques to use.\n            [0]: Naive sampling (temperature=1.0)\n            [1]: Low temperature sampling\n            [2]: Power sampling (MCMC)\n        max_questions: Maximum number of questions to process. 0 means process all.\n        output_file_name: Path to the output CSV file for results.\n        **kwargs: Additional keyword arguments passed to sampling functions.\n            log_file_path: Base directory for logging individual question results.\n\n    Returns:\n        None. Results are written to the output CSV file.\n    \"\"\"\n    # Store results\n    results = []    \n    os.makedirs(os.path.dirname(output_file_name), exist_ok=True)\n    output_file = open(output_file_name, \"w\")\n\n    # Create a template for the log file path, defaulting to the output file directory if not provided\n    log_base_dir = kwargs.get(\"log_file_path\", os.path.dirname(output_file_name) or \".\")\n    log_file_path_template = os.path.join(log_base_dir, \"question_{}.csv\")\n\n    # Iterate over the dataset\n    for question_index, question in enumerate(question_list):\n\n        # Break if we have reached the max number of questions to ask\n        if(max_questions == question_index and max_questions != 0):\n            break\n\n        #Retrive the dataset answer\n        answer = answer_list[question_index]\n\n        # Prepare prompt based on whether LLM has chat template or not\n        if chat_template:\n            formatted_prompt = tokenizer_chat_template(llm.tokenizer, enable_thinking, system_message, question)\n        else:\n            formatted_prompt = system_message +  question\n\n        # Store the prompt and answers in the results csv\n        result_row = {\n            \"question\": formatted_prompt,\n            \"correct_answer\": answer\n        }\n\n        # Generate a response using the sampler\n        if(sampling_techniques[2]): # Power Sampling\n            #Add the question number to the kwargs log_file_path\n            kwargs[\"log_file_path\"] = log_file_path_template.format(question_index)\n\n            #Time how long it takes to get a response\n            start_time = time.time()\n\n            # Send the prompt to the sliding window power sampling function\n            output = llm.token_sample(formatted_prompt, **kwargs)\n\n            # Find the end time of the power sampling\n            end_time = time.time()\n\n            # Parse the answer\n            power_sampling_answer = parse_answer(llm.tokenizer.decode(output.tokens, skip_special_tokens=False))\n\n            # Save the results\n            result_row[\"mcmc_completion\"] = llm.tokenizer.decode(output.tokens, skip_special_tokens=False)\n            result_row[\"mcmc_output_token_count\"] = len(output.tokens)\n            result_row[\"mcmc_time_to_solution\"] = end_time - start_time\n            result_row[\"mcmc_answer\"] = power_sampling_answer\n\n        # Generate a response with just low temperature sampling\n        if(sampling_techniques[1]): # Low Temperature Sampling\n            #Time how long it takes to get a response\n            start_time = time.time()\n\n            # Prompt the LLM and get the output/answer\n            output = llm.sample(formatted_prompt)\n\n            # Find the end time of the low temperature sampling\n            end_time = time.time()\n\n            # Parse the answer\n            low_temp_sampling_answer = parse_answer(llm.tokenizer.decode(output.tokens, skip_special_tokens=False))\n\n            # Save the results\n            result_row[\"naive_completion\"] = llm.tokenizer.decode(output.tokens, skip_special_tokens=False)\n            result_row[\"naive_sampling_output_token_count\"] = len(output.tokens)\n            result_row[\"naive_sampling_time_to_solution\"] = end_time - start_time\n            result_row[\"naive_answer\"] = low_temp_sampling_answer\n\n        if(sampling_techniques[0]): # Naive Sampling\n            # Save and change the temperature to 1.0 for naive sampling\n            saved_temperature = llm.sampling_params.temperature\n            llm.sampling_params.temperature = 1.0\n\n            #Time how long it takes to get a response\n            start_time = time.time()\n\n            # Prompt the LLM and get the output/answer\n            output = llm.sample(formatted_prompt)\n\n            # Find the end time of the naive sampling\n            end_time = time.time()\n\n            # Parse the answer\n            std_sampling_answer = parse_answer(llm.tokenizer.decode(output.tokens, skip_special_tokens=False))\n            # Save the results\n            result_row[\"std_completion\"] = llm.tokenizer.decode(output.tokens, skip_special_tokens=False)\n            result_row[\"std_sampling_output_token_count\"] = len(output.tokens)\n            result_row[\"std_sampling_time_to_solution\"] = end_time - start_time\n            result_row[\"std_answer\"] = std_sampling_answer\n\n            # Set the temperature back to original\n            llm.sampling_params.temperature = saved_temperature\n\n        # Write the question and final answer to the output file\n        results.append(result_row)\n        # Write to CSV after each iteration, only write header for first row\n        df = pd.DataFrame([result_row])\n        df.to_csv(output_file, index=False, header=(question_index==0))\n        output_file.flush()\n        os.fsync(output_file.fileno())\n</code></pre>"},{"location":"utils/benchmarking_utils/#pita.utils.benchmarking_utils.format_dataset","title":"<code>format_dataset(dataset: datasets.Dataset, pre_question: str, post_question: str) -&gt; tuple[list[str], list[str]]</code>","text":"<p>Format a dataset by adding pre and post question templates to each problem.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset containing problems and answers.</p> required <code>pre_question</code> <code>str</code> <p>Text to prepend before each problem.</p> required <code>post_question</code> <code>str</code> <p>Text to append after each problem.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A tuple of (question_list, answer_list) where question_list contains</p> <code>list[str]</code> <p>formatted questions and answer_list contains corresponding answers.</p> Source code in <code>pita/utils/benchmarking_utils.py</code> <pre><code>def format_dataset(\n    dataset: datasets.Dataset,\n    pre_question: str,\n    post_question: str\n) -&gt; tuple[list[str], list[str]]:\n    \"\"\"\n    Format a dataset by adding pre and post question templates to each problem.\n\n    Args:\n        dataset: The dataset containing problems and answers.\n        pre_question: Text to prepend before each problem.\n        post_question: Text to append after each problem.\n\n    Returns:\n        A tuple of (question_list, answer_list) where question_list contains\n        formatted questions and answer_list contains corresponding answers.\n    \"\"\"\n    # Lists to store the questions and answers\n    question_list = []\n    answer_list = []\n\n    # Iterate through the dataset and format each question\n    for(dataset_index, data) in enumerate(dataset):\n        # Extract the problem and answer from the dataset\n        problem = data[\"problem\"]\n        answer = data[\"answer\"]\n\n        # Format the question with pre and post templates\n        formatted_question = pre_question + problem + post_question\n\n        # Store back in dataset\n        question_list.append(formatted_question)\n        answer_list.append(answer)\n\n    return question_list, answer_list\n</code></pre>"},{"location":"utils/benchmarking_utils/#pita.utils.benchmarking_utils.load_benchmark","title":"<code>load_benchmark(dataset_name: str) -&gt; tuple[str, list[str], list[str]]</code>","text":"<p>Load a benchmark dataset by name and return formatted questions and answers.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>Name of the dataset to load. Supported values are \"MATH500\" and \"AIME\".</p> required <p>Returns:</p> Type Description <code>str</code> <p>A tuple of (system_message, question_list, answer_list) where:</p> <code>list[str]</code> <ul> <li>system_message: The system message to use for the chat template</li> </ul> <code>list[str]</code> <ul> <li>question_list: List of formatted questions</li> </ul> <code>tuple[str, list[str], list[str]]</code> <ul> <li>answer_list: List of corresponding answers</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the dataset_name is not supported.</p> Source code in <code>pita/utils/benchmarking_utils.py</code> <pre><code>def load_benchmark(\n    dataset_name: str\n) -&gt; tuple[str, list[str], list[str]]:\n    \"\"\"\n    Load a benchmark dataset by name and return formatted questions and answers.\n\n    Args:\n        dataset_name: Name of the dataset to load. Supported values are \"MATH500\" and \"AIME\".\n\n    Returns:\n        A tuple of (system_message, question_list, answer_list) where:\n        - system_message: The system message to use for the chat template\n        - question_list: List of formatted questions\n        - answer_list: List of corresponding answers\n\n    Raises:\n        ValueError: If the dataset_name is not supported.\n    \"\"\"\n    # Load either the MATH500 or AIME dataset\n    if(dataset_name == \"MATH500\"):\n        # Load the Math500 dataset\n        dataset = datasets.load_dataset(\"HuggingFaceH4/MATH-500\")[\"test\"]\n        # Convert all keys to lowercase\n        dataset = dataset.map(lambda x: {k.lower(): v for k, v in x.items()})\n        # convert answers to a string\n        dataset = dataset.cast_column('answer', datasets.Value('string'))\n\n        # Create the system message, pre, and post question templates\n        system_message = MATH_SYSTEM_MESSAGE\n        pre_question = MATH_PRE_QUESTION \n        post_question = MATH_ANSWER_FORMAT\n\n    elif(dataset_name == \"AIME\"):\n        #Load both parts of the AIME tests and concatenate them\n        dataset = datasets.concatenate_datasets([datasets.load_dataset(\"opencompass/AIME2025\", \"AIME2025-I\")[\"test\"], \n                                                datasets.load_dataset(\"opencompass/AIME2025\", \"AIME2025-II\")[\"test\"]])\n        # Convert all keys to lowercase\n        dataset = dataset.map(lambda x: {k.lower(): v for k, v in x.items()})\n        # convert answers to a string\n        dataset = dataset.cast_column('answer', datasets.Value('string'))\n        # convert the question column name to \"problem\"\n        dataset = dataset.rename_column(\"question\", \"problem\")\n\n        # Create the system message, pre, and post question templates\n        system_message = MATH_SYSTEM_MESSAGE\n        pre_question = MATH_PRE_QUESTION\n        post_question = MATH_ANSWER_FORMAT\n\n    else: \n        raise ValueError(f\"Dataset {dataset_name} not supported for benchmarking.\")\n\n    # Format the dataset and return the system message, question list, and answer list\n    question_list, answer_list = format_dataset(dataset, pre_question, post_question)\n    return system_message, question_list, answer_list\n</code></pre>"},{"location":"utils/benchmarking_utils/#pita.utils.benchmarking_utils.tokenizer_chat_template","title":"<code>tokenizer_chat_template(tokenizer: AutoTokenizer, enable_thinking: bool, system_message: str, user_message: str) -&gt; str</code>","text":"<p>Format messages for chat models using the tokenizer's chat template.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>AutoTokenizer</code> <p>The AutoTokenizer instance to use for formatting.</p> required <code>enable_thinking</code> <code>bool</code> <p>Whether to enable thinking mode in the chat template.</p> required <code>system_message</code> <code>str</code> <p>The system message content to include.</p> required <code>user_message</code> <code>str</code> <p>The user message content to include.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The formatted prompt string ready for the model.</p> Source code in <code>pita/utils/benchmarking_utils.py</code> <pre><code>def tokenizer_chat_template(\n    tokenizer: AutoTokenizer,\n    enable_thinking: bool,\n    system_message: str,\n    user_message: str,\n) -&gt; str:\n    \"\"\"\n    Format messages for chat models using the tokenizer's chat template.\n\n    Args:\n        tokenizer: The AutoTokenizer instance to use for formatting.\n        enable_thinking: Whether to enable thinking mode in the chat template.\n        system_message: The system message content to include.\n        user_message: The user message content to include.\n\n    Returns:\n        The formatted prompt string ready for the model.\n    \"\"\"\n\n    # Create the message format for apply_chat_template function\n    messages = [\n        {\n            \"role\": \"system\",\n            # Crucial for benchmarks: explicitly ask for reasoning and boxed format\n            \"content\": system_message\n        },\n        {\n            \"role\": \"user\",\n            \"content\": user_message\n        }\n    ]\n\n    # Apply the chat template to create the final prompt\n    prompt = tokenizer.apply_chat_template(\n        messages,\n        tokenize = False,\n        add_generation_prompt = True,\n        enable_thinking = enable_thinking\n    )\n\n    return prompt\n</code></pre>"},{"location":"utils/constants/","title":"Constants","text":"<p>Valkey configuration constants for the PITA package.</p> <p>This module defines Valkey connection parameters that can be overridden via environment variables. Valkey is used for inter-process communication between the main process and logits processors during inference.</p> Environment Variables <p>VALKEY_HOST: Valkey server hostname (default: 'localhost') VALKEY_PORT: Valkey server port (default: 6379) VALKEY_PASSWORD: Valkey authentication password (default: None)</p>"},{"location":"utils/parse_utils/","title":"Parse Utils","text":""},{"location":"utils/parse_utils/#pita.utils.parse_utils.last_boxed_only","title":"<code>last_boxed_only(sample: Tuple[str, str]) -&gt; Optional[Tuple[str, str]]</code>","text":"<p>Filter a (question, answer) sample to keep only the last \\boxed{} or \\fbox{} element.</p> <p>Given a (q,a) sample, filter the answers so that they only contain the last \\boxed{...} or \\fbox{...} element</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>Tuple[str, str]</code> <p>A tuple of (question, answer) strings.</p> required <p>Returns:</p> Type Description <code>Optional[Tuple[str, str]]</code> <p>A tuple of (question, filtered_answer) where filtered_answer contains only</p> <code>Optional[Tuple[str, str]]</code> <p>the last boxed element, or None if no boxed element is found.</p> Source code in <code>pita/utils/parse_utils.py</code> <pre><code>def last_boxed_only(sample: Tuple[str, str]) -&gt; Optional[Tuple[str, str]]:\n    \"\"\"\n    Filter a (question, answer) sample to keep only the last \\\\boxed{} or \\\\fbox{} element.\n\n    Given a (q,a) sample, filter the answers so that they only contain\n    the last \\\\boxed{...} or \\\\fbox{...} element\n\n    Args:\n        sample: A tuple of (question, answer) strings.\n\n    Returns:\n        A tuple of (question, filtered_answer) where filtered_answer contains only\n        the last boxed element, or None if no boxed element is found.\n    \"\"\"\n    q, a = sample\n    a = last_boxed_only_string(a)\n    if a == None:\n        return None\n    return (q, a)\n</code></pre>"},{"location":"utils/parse_utils/#pita.utils.parse_utils.last_boxed_only_string","title":"<code>last_boxed_only_string(string: str) -&gt; Optional[str]</code>","text":"<p>Extract the last \\boxed{} or \\fbox{} expression with proper brace matching.</p> <p>This function handles nested braces correctly by tracking open and closed braces to find the complete expression.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>A string potentially containing \\boxed{} or \\fbox{} expressions.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The last complete \\boxed{} or \\fbox{} expression (including the command</p> <code>Optional[str]</code> <p>and braces), or None if no such expression is found or braces don't match.</p> Source code in <code>pita/utils/parse_utils.py</code> <pre><code>def last_boxed_only_string(string: str) -&gt; Optional[str]:\n    \"\"\"\n    Extract the last \\\\boxed{} or \\\\fbox{} expression with proper brace matching.\n\n    This function handles nested braces correctly by tracking open and closed braces\n    to find the complete expression.\n\n    Args:\n        string: A string potentially containing \\\\boxed{} or \\\\fbox{} expressions.\n\n    Returns:\n        The last complete \\\\boxed{} or \\\\fbox{} expression (including the command\n        and braces), or None if no such expression is found or braces don't match.\n    \"\"\"\n    idx = string.rfind(\"\\\\boxed\")\n    if idx &lt; 0:\n        idx = string.rfind(\"\\\\fbox\")\n        if idx &lt; 0:\n            return None\n\n    i = idx\n    right_brace_idx = None\n    num_left_braces_open = 0\n    while i &lt; len(string):\n        if string[i] == \"{\":\n            num_left_braces_open += 1\n        if string[i] == \"}\":\n            num_left_braces_open -= 1\n            if num_left_braces_open == 0:\n                right_brace_idx = i\n                break\n        i += 1\n\n    if right_brace_idx == None:\n        retval = None\n    else:\n        retval = string[idx:right_brace_idx + 1]\n\n    return retval\n</code></pre>"},{"location":"utils/parse_utils/#pita.utils.parse_utils.parse_answer","title":"<code>parse_answer(input_str: Optional[str]) -&gt; Optional[str]</code>","text":"<p>Parse and extract the final answer from a mathematical solution.</p> <p>This function finds the last \\boxed{} expression in the answer string and extracts the content from within the braces.</p> <p>Parameters:</p> Name Type Description Default <code>input_str</code> <code>Optional[str]</code> <p>A string containing a mathematical solution, potentially with one or more \\boxed{} expressions, or None.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The extracted answer content (without the \\boxed{} wrapper), or None</p> <code>Optional[str]</code> <p>if no boxed answer is found or the input is None.</p> Source code in <code>pita/utils/parse_utils.py</code> <pre><code>def parse_answer(input_str: Optional[str]) -&gt; Optional[str]:\n    \"\"\"\n    Parse and extract the final answer from a mathematical solution.\n\n    This function finds the last \\\\boxed{} expression in the answer string and\n    extracts the content from within the braces.\n\n    Args:\n        input_str: A string containing a mathematical solution, potentially with\n            one or more \\\\boxed{} expressions, or None.\n\n    Returns:\n        The extracted answer content (without the \\\\boxed{} wrapper), or None\n        if no boxed answer is found or the input is None.\n    \"\"\"\n    if input_str is None:\n        return None\n    last_boxed = last_boxed_only_string(input_str)\n    if last_boxed is None:\n        return None\n    return remove_boxed(last_boxed)\n</code></pre>"},{"location":"utils/parse_utils/#pita.utils.parse_utils.remove_boxed","title":"<code>remove_boxed(s: str) -&gt; Optional[str]</code>","text":"<p>Remove the LaTeX \\boxed{} wrapper from a string.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>A string that should start with \"\\boxed{\" and end with \"}\".</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The content inside the \\boxed{} wrapper, or None if the string doesn't</p> <code>Optional[str]</code> <p>have the expected format.</p> Source code in <code>pita/utils/parse_utils.py</code> <pre><code>def remove_boxed(s: str) -&gt; Optional[str]:\n    \"\"\"\n    Remove the LaTeX \\\\boxed{} wrapper from a string.\n\n    Args:\n        s: A string that should start with \"\\\\boxed{\" and end with \"}\".\n\n    Returns:\n        The content inside the \\\\boxed{} wrapper, or None if the string doesn't\n        have the expected format.\n    \"\"\"\n    left = \"\\\\boxed{\"\n    try:\n        assert s[:len(left)] == left\n        assert s[-1] == \"}\"\n        return s[len(left):-1]\n    except (AssertionError, IndexError, TypeError):\n        return None\n</code></pre>"},{"location":"utils/system_utils/","title":"System Utils","text":""},{"location":"utils/system_utils/#pita.utils.system_utils.detect_model_type","title":"<code>detect_model_type(model: str) -&gt; Optional[str]</code>","text":"<p>Detect the model file type from a Hugging Face model repository.</p> <p>This function attempts to determine whether a Hugging Face model repository contains GGUF or safetensors files by querying the repository file list.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier, typically in the format \"owner/repo\" for Hugging Face repos.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>A string indicating the detected model type (\"gguf\" or \"safetensors\"), or None</p> <code>Optional[str]</code> <p>if the type could not be determined or if the huggingface_hub library is not available.</p> Source code in <code>pita/utils/system_utils.py</code> <pre><code>def detect_model_type(model: str) -&gt; Optional[str]:\n    \"\"\"\n    Detect the model file type from a Hugging Face model repository.\n\n    This function attempts to determine whether a Hugging Face model repository\n    contains GGUF or safetensors files by querying the repository file list.\n\n    Args:\n        model: Model identifier, typically in the format \"owner/repo\" for Hugging Face repos.\n\n    Returns:\n        A string indicating the detected model type (\"gguf\" or \"safetensors\"), or None\n        if the type could not be determined or if the huggingface_hub library is not available.\n    \"\"\"\n    # Determine the model type (GGUF, safetensors, etc.) for Hugging Face repos\n    detected_dtype = None\n\n    # Try to detect model file type using the Hugging Face hub when the model looks\n    # like a repo id (owner/repo) and the huggingface_hub package is available.\n    if '/' in str(model) and not model.startswith(\"model/\"):\n        try:\n            from huggingface_hub import HfApi\n\n            api = HfApi()\n            try:\n                files = api.list_repo_files(repo_id=model)\n            except Exception:\n                files = []\n\n            # Check for common filetypes in the repo\n            files_lower = [f.lower() for f in files]\n            if any(f.endswith('.gguf') for f in files_lower):\n                detected_dtype = 'gguf'\n            elif any(f.endswith('.safetensors') for f in files_lower):\n                detected_dtype = 'safetensors'\n            else:\n                print(\"Warning: Could not determine model type.\")\n\n        except Exception:\n            # huggingface_hub not installed or network error \u2014 leave dtype as passed\n            print(\"Warning: Could not import huggingface_hub or access model repo to detect model type.\")\n\n    return detected_dtype\n</code></pre>"},{"location":"utils/system_utils/#pita.utils.system_utils.get_gpu_vram_usage_mb","title":"<code>get_gpu_vram_usage_mb() -&gt; Optional[int]</code>","text":"<p>Get the current VRAM usage (in MiB) across all NVIDIA GPUs.</p> <p>This function uses nvidia-smi to query current GPU memory usage and returns the sum across all GPUs if multiple are present.</p> <p>Returns:</p> Type Description <code>Optional[int]</code> <p>Total current VRAM usage in MiB across all GPUs, or None if nvidia-smi</p> <code>Optional[int]</code> <p>is not available or an error occurs.</p> Source code in <code>pita/utils/system_utils.py</code> <pre><code>def get_gpu_vram_usage_mb() -&gt; Optional[int]:\n    \"\"\"\n    Get the current VRAM usage (in MiB) across all NVIDIA GPUs.\n\n    This function uses nvidia-smi to query current GPU memory usage and returns\n    the sum across all GPUs if multiple are present.\n\n    Returns:\n        Total current VRAM usage in MiB across all GPUs, or None if nvidia-smi\n        is not available or an error occurs.\n    \"\"\"\n    try:\n        result = subprocess.run(\n            ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits'],\n            capture_output=True,\n            text=True,\n            check=True\n        )\n        # Returns usage for all GPUs, sum them or take the first one\n        vram_values = [int(x.strip()) for x in result.stdout.strip().split('\\n') if x.strip()]\n        return sum(vram_values)  # Total across all GPUs, or use vram_values[0] for first GPU\n    except (subprocess.CalledProcessError, FileNotFoundError, ValueError) as e:\n        print(f\"Warning: Could not get VRAM usage from nvidia-smi: {e}\")\n        return None\n</code></pre>"},{"location":"utils/system_utils/#pita.utils.system_utils.get_total_vram","title":"<code>get_total_vram() -&gt; Union[int, str]</code>","text":"<p>Get the total VRAM (in MiB) of the primary GPU on the system.</p> <p>This function attempts to detect VRAM across different platforms and GPU types: - NVIDIA GPUs: Uses nvidia-smi (Windows &amp; Linux) - AMD GPUs: Uses ROCm-smi (Linux) - Windows Generic: Uses PowerShell WMI queries</p> <p>Returns:</p> Type Description <code>Union[int, str]</code> <p>Total VRAM in MiB (int) if successfully detected, or an error message (str)</p> <code>Union[int, str]</code> <p>if detection fails or drivers are not installed.</p> Source code in <code>pita/utils/system_utils.py</code> <pre><code>def get_total_vram() -&gt; Union[int, str]:\n    \"\"\"\n    Get the total VRAM (in MiB) of the primary GPU on the system.\n\n    This function attempts to detect VRAM across different platforms and GPU types:\n    - NVIDIA GPUs: Uses nvidia-smi (Windows &amp; Linux)\n    - AMD GPUs: Uses ROCm-smi (Linux)\n    - Windows Generic: Uses PowerShell WMI queries\n\n    Returns:\n        Total VRAM in MiB (int) if successfully detected, or an error message (str)\n        if detection fails or drivers are not installed.\n    \"\"\"\n    os_name = platform.system()\n\n    # --- NVIDIA (Windows &amp; Linux) ---\n    if shutil.which(\"nvidia-smi\"):\n        try:\n            # Change 'memory.used' to 'memory.total'\n            result = subprocess.check_output(\n                [\"nvidia-smi\", \"--query-gpu=memory.total\", \"--format=csv,noheader,nounits\"],\n                encoding=\"utf-8\"\n            )\n            return int(result.strip())\n        except Exception as e:\n            return f\"Error reading Nvidia SMI: {e}\"\n\n    # --- WINDOWS (AMD, Intel, &amp; Generic) ---\n    if os_name == \"Windows\":\n        try:\n            # We use WMI (Win32_VideoController) to get the 'AdapterRAM'\n            # We sort by size to find the largest GPU (ignoring small iGPUs if a dGPU exists)\n            cmd = 'Get-CimInstance Win32_VideoController | Sort-Object -Property AdapterRAM -Descending | Select-Object -First 1 -ExpandProperty AdapterRAM'\n\n            result = subprocess.check_output(\n                [\"powershell\", \"-Command\", cmd],\n                encoding=\"utf-8\"\n            )\n\n            # Windows returns Bytes. Convert to MiB.\n            vram_bytes = float(result.strip())\n            vram_mib = int(vram_bytes / 1024 / 1024)\n            return vram_mib\n        except Exception:\n            pass\n\n    # --- LINUX (AMD ROCm) ---\n    if shutil.which(\"rocm-smi\"):\n        try:\n            # Use --showmeminfo vram and parse for 'Total'\n            result = subprocess.check_output(\n                [\"rocm-smi\", \"--showmeminfo\", \"vram\"], encoding=\"utf-8\"\n            )\n            # Use regex to find \"VRAM Total Memory (B): &lt;numbers&gt;\"\n            match = re.search(r\"VRAM Total Memory \\(B\\):\\s+(\\d+)\", result)\n            if match:\n                vram_bytes = int(match.group(1))\n                vram_mib = vram_bytes // (1024 * 1024)\n                return vram_mib\n        except Exception as e:\n            return f\"Error reading ROCm SMI: {e}\"\n\n    return \"Could not detect Total VRAM. Ensure drivers are installed.\"\n</code></pre>"},{"location":"utils/valkey_manager/","title":"Valkey Manager","text":"<p>Manager for starting and stopping a Valkey server subprocess.</p> <p>This module provides the ValkeyManager class that automatically manages a Valkey server instance for use by the PITA package. It can detect existing Valkey instances and start a new one if needed.</p>"},{"location":"utils/valkey_manager/#pita.utils.valkey_manager.ValkeyManager","title":"<code>ValkeyManager</code>","text":"<p>Manager for starting and stopping a Valkey server subprocess.</p> <p>This class provides utilities to automatically manage a Valkey server instance for use by the PITA package. It can detect existing Valkey instances and start a new one if needed.</p> <p>Attributes:</p> Name Type Description <code>_process</code> <code>Optional[Popen]</code> <p>The subprocess.Popen instance for the managed Valkey server, or None.</p> Source code in <code>pita/utils/valkey_manager.py</code> <pre><code>class ValkeyManager:\n    \"\"\"\n    Manager for starting and stopping a Valkey server subprocess.\n\n    This class provides utilities to automatically manage a Valkey server instance\n    for use by the PITA package. It can detect existing Valkey instances and start\n    a new one if needed.\n\n    Attributes:\n        _process: The subprocess.Popen instance for the managed Valkey server, or None.\n    \"\"\"\n    _process: Optional[subprocess.Popen] = None\n\n    @classmethod\n    def start(cls) -&gt; None:\n        \"\"\"Starts the Valkey server if it is not already running.\"\"\"\n        # Check if we already started it\n        if cls._process is not None:\n            return\n\n        # Check if already running system-wide or by another process\n        if cls._is_valkey_running():\n            return\n\n        valkey_executable = cls._find_valkey_executable()\n\n        if not valkey_executable:\n            print(\"Warning: valkey-server executable not found. Please ensure it is installed.\")\n            return\n\n        print(f\"Starting valkey-server on port {VALKEY_PORT}...\")\n        try:\n            # Start valkey-server as a subprocess\n            # We use --daemonize no so we can control the subprocess lifecycle easily\n            cls._process = subprocess.Popen(\n                [valkey_executable, '--port', str(VALKEY_PORT), '--save', '', '--appendonly', 'no'],\n                stdout=subprocess.DEVNULL,\n                stderr=subprocess.DEVNULL\n            )\n\n            # Wait a moment to ensure it starts\n            time.sleep(0.5)\n            if cls._process.poll() is not None:\n                print(\"Failed to start valkey-server subprocess.\")\n                cls._process = None\n            else:\n                print(\"Valkey server started successfully.\")\n                # Register cleanup on exit\n                atexit.register(cls.stop)\n\n        except Exception as e:\n            print(f\"Failed to start valkey-server: {e}\")\n\n    @classmethod\n    def stop(cls) -&gt; None:\n        \"\"\"\n        Stop the Valkey server if it was started by this manager.\n\n        This method attempts to gracefully terminate the Valkey process, waiting up to\n        2 seconds before forcefully killing it if necessary.\n        \"\"\"\n        if cls._process:\n            print(\"Stopping valkey-server...\")\n            cls._process.terminate()\n            try:\n                cls._process.wait(timeout=2)\n            except subprocess.TimeoutExpired:\n                cls._process.kill()\n            cls._process = None\n\n    @classmethod\n    def _is_valkey_running(cls) -&gt; bool:\n        \"\"\"\n        Check if a Valkey or Redis server process is currently running on the system.\n\n        Since Valkey and Redis are protocol-compatible, we check for both server types\n        to avoid starting a new instance when an existing compatible server is running.\n\n        Returns:\n            True if a valkey-server or redis-server process is found, False otherwise.\n        \"\"\"\n        for proc in psutil.process_iter(['name']):\n            try:\n                proc_name = proc.info['name']\n                if 'valkey-server' in proc_name or 'redis-server' in proc_name:\n                    # Could add stricter check for port/args, but simplified for now\n                    return True\n            except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n                # Process may have terminated or be inaccessible; safely skip it and continue scanning\n                pass\n        return False\n\n    @classmethod\n    def _find_valkey_executable(cls) -&gt; Optional[str]:\n        \"\"\"\n        Find the valkey-server executable in various system locations.\n\n        This method searches for valkey-server in the system PATH, conda environments,\n        and relative to the Python executable.\n\n        Returns:\n            The absolute path to the valkey-server executable, or None if not found.\n        \"\"\"\n        import sys\n        executable = shutil.which('valkey-server')\n        if executable:\n            return executable\n\n        # Fallback to checking conda env bin if not in PATH\n        conda_prefix = os.environ.get('CONDA_PREFIX')\n        if conda_prefix:\n            candidate = os.path.join(conda_prefix, 'bin', 'valkey-server')\n            if os.path.exists(candidate):\n                return candidate\n\n        # Fallback relative to sys.executable (common in conda envs)\n        # sys.executable is .../bin/python, so we look in .../bin/valkey-server\n        bin_dir = os.path.dirname(sys.executable)\n        candidate = os.path.join(bin_dir, 'valkey-server')\n        if os.path.exists(candidate):\n            return candidate\n\n        return None\n</code></pre>"},{"location":"utils/valkey_manager/#pita.utils.valkey_manager.ValkeyManager.start","title":"<code>start() -&gt; None</code>  <code>classmethod</code>","text":"<p>Starts the Valkey server if it is not already running.</p> Source code in <code>pita/utils/valkey_manager.py</code> <pre><code>@classmethod\ndef start(cls) -&gt; None:\n    \"\"\"Starts the Valkey server if it is not already running.\"\"\"\n    # Check if we already started it\n    if cls._process is not None:\n        return\n\n    # Check if already running system-wide or by another process\n    if cls._is_valkey_running():\n        return\n\n    valkey_executable = cls._find_valkey_executable()\n\n    if not valkey_executable:\n        print(\"Warning: valkey-server executable not found. Please ensure it is installed.\")\n        return\n\n    print(f\"Starting valkey-server on port {VALKEY_PORT}...\")\n    try:\n        # Start valkey-server as a subprocess\n        # We use --daemonize no so we can control the subprocess lifecycle easily\n        cls._process = subprocess.Popen(\n            [valkey_executable, '--port', str(VALKEY_PORT), '--save', '', '--appendonly', 'no'],\n            stdout=subprocess.DEVNULL,\n            stderr=subprocess.DEVNULL\n        )\n\n        # Wait a moment to ensure it starts\n        time.sleep(0.5)\n        if cls._process.poll() is not None:\n            print(\"Failed to start valkey-server subprocess.\")\n            cls._process = None\n        else:\n            print(\"Valkey server started successfully.\")\n            # Register cleanup on exit\n            atexit.register(cls.stop)\n\n    except Exception as e:\n        print(f\"Failed to start valkey-server: {e}\")\n</code></pre>"},{"location":"utils/valkey_manager/#pita.utils.valkey_manager.ValkeyManager.stop","title":"<code>stop() -&gt; None</code>  <code>classmethod</code>","text":"<p>Stop the Valkey server if it was started by this manager.</p> <p>This method attempts to gracefully terminate the Valkey process, waiting up to 2 seconds before forcefully killing it if necessary.</p> Source code in <code>pita/utils/valkey_manager.py</code> <pre><code>@classmethod\ndef stop(cls) -&gt; None:\n    \"\"\"\n    Stop the Valkey server if it was started by this manager.\n\n    This method attempts to gracefully terminate the Valkey process, waiting up to\n    2 seconds before forcefully killing it if necessary.\n    \"\"\"\n    if cls._process:\n        print(\"Stopping valkey-server...\")\n        cls._process.terminate()\n        try:\n            cls._process.wait(timeout=2)\n        except subprocess.TimeoutExpired:\n            cls._process.kill()\n        cls._process = None\n</code></pre>"},{"location":"utils/valkey_manager/#overview","title":"Overview","text":"<p>The <code>valkey_manager</code> module provides utilities for managing Valkey connections and data storage, primarily used for inter-process communication between the main inference process and logits processors in vLLM and TensorRT backends.</p>"},{"location":"utils/valkey_manager/#use-cases","title":"Use Cases","text":"<p>Valkey is used in PITA for:</p> <ul> <li>Logits Processor Communication: vLLM and TensorRT run logits processors in separate threads. These processors calculate normalization constants and entropy values, storing them in Valkey for retrieval by the main process.</li> <li>Temporary Data Storage: Metrics are stored temporarily and automatically cleaned up after retrieval.</li> <li>Multi-threaded Coordination: Enables safe data sharing between the inference engine threads and the main application.</li> </ul>"},{"location":"utils/valkey_manager/#configuration","title":"Configuration","text":"<p>Valkey connection parameters are configured via environment variables (see constants):</p> <ul> <li><code>VALKEY_HOST</code>: Valkey server hostname (default: \"localhost\")</li> <li><code>VALKEY_PORT</code>: Valkey server port (default: 6379)</li> <li><code>VALKEY_DB</code>: Valkey database number (default: 0)</li> </ul>"},{"location":"utils/valkey_manager/#related-components","title":"Related Components","text":"<ul> <li>vLLM Logits Processor: Uses Valkey to store computed metrics</li> <li>TensorRT Backend: Uses Valkey for logits processing</li> <li>Constants: Valkey configuration constants</li> </ul>"},{"location":"utils/valkey_manager/#requirements","title":"Requirements","text":"<p>Valkey is required for the following backends when <code>logits_processor=True</code>:</p> <ul> <li>vLLM backend with entropy or power distribution metrics</li> <li>TensorRT backend with entropy or power distribution metrics</li> </ul> <p>Install Valkey:</p> <pre><code># Ubuntu/Debian\nsudo apt-get install valkey-server\n\n# macOS\nbrew install valkey\n\n# Or via pip for Python client\npip install valkey\n</code></pre> <p>Start Valkey server:</p> <pre><code>valkey-server\n</code></pre>"},{"location":"utils/valkey_manager/#example-usage","title":"Example Usage","text":"<p>The Valkey manager is typically used internally by the logits processors, but you can access it directly if needed:</p> <pre><code>from pita.utils.valkey_manager import ValkeyManager\nfrom pita.utils.constants import VALKEY_HOST, VALKEY_PORT, VALKEY_DB\n\n# Initialize Valkey connection (usually done internally)\nvalkey_client = ValkeyManager(\n    host=VALKEY_HOST,\n    port=VALKEY_PORT,\n    db=VALKEY_DB\n)\n\n# The logits processors use Valkey to store metrics\n# These are automatically retrieved and cleaned up by the sampler\n</code></pre> <p>For most users, Valkey integration is transparent - simply ensure Valkey is running when using vLLM or TensorRT backends with <code>logits_processor=True</code>.</p>"},{"location":"utils/grading_utils/math/eval_math/","title":"Eval Math","text":""},{"location":"utils/grading_utils/math/eval_math/#pita.utils.grading_utils.math.eval_math.eval_math","title":"<code>eval_math(fname: str) -&gt; Tuple[int, int, int, int, int]</code>","text":"<p>Evaluate math answers from a CSV file using different sampling methods.</p> <p>Reads a CSV file containing answers from different sampling strategies and grades them against the correct answers.</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>Path to the CSV file containing answers and correct solutions.</p> required <p>Returns:</p> Type Description <code>int</code> <p>A tuple containing (naive_sampling_correct, low_temp_sampling_correct,</p> <code>int</code> <p>power_sampling_sliding_window_correct, power_sampling_correct, total),</p> <code>int</code> <p>where each value represents the count of correct answers for that method</p> <code>int</code> <p>and total is the number of questions evaluated.</p> Source code in <code>pita/utils/grading_utils/math/eval_math.py</code> <pre><code>def eval_math(fname: str) -&gt; Tuple[int, int, int, int, int]:\n    \"\"\"\n    Evaluate math answers from a CSV file using different sampling methods.\n\n    Reads a CSV file containing answers from different sampling strategies\n    and grades them against the correct answers.\n\n    Args:\n        fname: Path to the CSV file containing answers and correct solutions.\n\n    Returns:\n        A tuple containing (naive_sampling_correct, low_temp_sampling_correct,\n        power_sampling_sliding_window_correct, power_sampling_correct, total),\n        where each value represents the count of correct answers for that method\n        and total is the number of questions evaluated.\n    \"\"\"\n    print(fname)\n    df = pd.read_csv(fname)\n    naive_sampling_correct = 0\n    low_temp_sampling_correct = 0\n    power_sampling_sliding_window_correct = 0\n    power_sampling_correct = 0\n    total = len(df)\n\n    for i in range(total):\n        naive_sampling_correct += safe_grade(df[\"naive_sampling_answer\"][i], df[\"correct_answer\"][i])\n        low_temp_sampling_correct += safe_grade(df[\"low_temp_sampling_answer\"][i], df[\"correct_answer\"][i])\n        power_sampling_sliding_window_correct += safe_grade(df[\"power_sampling_windowed_answer\"][i], df[\"correct_answer\"][i])\n        power_sampling_correct += safe_grade(df[\"power_sampling_answer\"][i], df[\"correct_answer\"][i])\n\n\n    return naive_sampling_correct, low_temp_sampling_correct, power_sampling_sliding_window_correct, power_sampling_correct, total\n</code></pre>"},{"location":"utils/grading_utils/math/eval_math/#pita.utils.grading_utils.math.eval_math.math_results","title":"<code>math_results(fnames: List[str]) -&gt; Dict[str, float]</code>","text":"<p>Compute and display aggregate math results across multiple CSV files.</p> <p>Evaluates answers from multiple CSV files using different sampling strategies and computes accuracy metrics for each strategy.</p> <p>Parameters:</p> Name Type Description Default <code>fnames</code> <code>List[str]</code> <p>List of paths to CSV files containing answers and correct solutions.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>A dictionary containing accuracy metrics for each sampling strategy:</p> <code>Dict[str, float]</code> <ul> <li>naive_sampling_acc: Accuracy of naive sampling method.</li> </ul> <code>Dict[str, float]</code> <ul> <li>low_temp_sampling_acc: Accuracy of low temperature sampling method.</li> </ul> <code>Dict[str, float]</code> <ul> <li>power_sampling_sliding_window_acc: Accuracy of power sampling with sliding window.</li> </ul> <code>Dict[str, float]</code> <ul> <li>power_sampling_acc: Accuracy of power sampling method.</li> </ul> Source code in <code>pita/utils/grading_utils/math/eval_math.py</code> <pre><code>def math_results(fnames: List[str]) -&gt; Dict[str, float]:\n    \"\"\"\n    Compute and display aggregate math results across multiple CSV files.\n\n    Evaluates answers from multiple CSV files using different sampling strategies\n    and computes accuracy metrics for each strategy.\n\n    Args:\n        fnames: List of paths to CSV files containing answers and correct solutions.\n\n    Returns:\n        A dictionary containing accuracy metrics for each sampling strategy:\n        - naive_sampling_acc: Accuracy of naive sampling method.\n        - low_temp_sampling_acc: Accuracy of low temperature sampling method.\n        - power_sampling_sliding_window_acc: Accuracy of power sampling with sliding window.\n        - power_sampling_acc: Accuracy of power sampling method.\n    \"\"\"\n    naive_sampling_total = 0\n    low_temp_sampling_total = 0\n    power_sampling_sliding_window_total = 0\n    power_sampling_total = 0\n    total = 0\n\n    for fname in fnames:\n        naive, low_temp, power_sliding, power, n = eval_math(fname)\n        naive_sampling_total += naive\n        low_temp_sampling_total += low_temp\n        power_sampling_sliding_window_total += power_sliding\n        power_sampling_total += power\n        total += n\n\n    denom = max(total, 1)\n    naive_sampling_acc = naive_sampling_total / denom\n    low_temp_sampling_acc = low_temp_sampling_total / denom\n    power_sampling_sliding_window_acc = power_sampling_sliding_window_total / denom\n    power_sampling_acc = power_sampling_total / denom\n\n    print(f\"Files evaluated: {len(fnames)}\")\n    print(f\"Total questions: {total}\")\n    print(f\"Naive sampling accuracy:  {naive_sampling_acc:.3f}\")\n    print(f\"Low temperature sampling accuracy:  {low_temp_sampling_acc:.3f}\")\n    print(f\"Power sampling sliding window accuracy:  {power_sampling_sliding_window_acc:.3f}\")\n    print(f\"Power Sampling accuracy:  {power_sampling_acc:.3f}\")\n\n    return {\n        \"naive_sampling_acc\": naive_sampling_acc,\n        \"low_temp_sampling_acc\": low_temp_sampling_acc,\n        \"power_sampling_sliding_window_acc\": power_sampling_sliding_window_acc,\n        \"power_sampling_acc\": power_sampling_acc,\n    }\n</code></pre>"},{"location":"utils/grading_utils/math/eval_math/#pita.utils.grading_utils.math.eval_math.safe_grade","title":"<code>safe_grade(ans: str, correct_ans: str) -&gt; int</code>","text":"<p>Safely grade an answer against the correct answer.</p> <p>Attempts to grade the given answer using the grade_answer function. Returns 0 if any exception occurs during grading.</p> <p>Parameters:</p> Name Type Description Default <code>ans</code> <code>str</code> <p>The student's answer to grade.</p> required <code>correct_ans</code> <code>str</code> <p>The correct answer to compare against.</p> required <p>Returns:</p> Type Description <code>int</code> <p>1 if the answer is correct, 0 if incorrect or if an exception occurred.</p> Source code in <code>pita/utils/grading_utils/math/eval_math.py</code> <pre><code>def safe_grade(ans: str, correct_ans: str) -&gt; int:\n    \"\"\"\n    Safely grade an answer against the correct answer.\n\n    Attempts to grade the given answer using the grade_answer function.\n    Returns 0 if any exception occurs during grading.\n\n    Args:\n        ans: The student's answer to grade.\n        correct_ans: The correct answer to compare against.\n\n    Returns:\n        1 if the answer is correct, 0 if incorrect or if an exception occurred.\n    \"\"\"\n    try:\n        return int(grade_answer(ans, correct_ans))\n    except Exception:\n        return 0\n</code></pre>"},{"location":"utils/grading_utils/math/math_grader/","title":"Math Grader","text":"<p>Answer checker API that uses sympy to simplify expressions and check for equality.</p> <p>Call grade_answer(given_answer: str, ground_truth: str).</p>"},{"location":"utils/grading_utils/math/math_grader/#pita.utils.grading_utils.math.math_grader.are_equal_under_sympy","title":"<code>are_equal_under_sympy(ground_truth_normalized: str, given_normalized: str) -&gt; bool</code>","text":"<p>Check if two expressions are mathematically equivalent using sympy.</p> <p>Subtracts the two expressions and simplifies the result. If the simplified difference equals zero, the expressions are equivalent.</p> <p>Parameters:</p> Name Type Description Default <code>ground_truth_normalized</code> <code>str</code> <p>The normalized ground truth expression.</p> required <code>given_normalized</code> <code>str</code> <p>The normalized given expression to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the expressions are mathematically equivalent, False otherwise.</p> Source code in <code>pita/utils/grading_utils/math/math_grader.py</code> <pre><code>def are_equal_under_sympy(ground_truth_normalized: str, given_normalized: str) -&gt; bool:\n    \"\"\"\n    Check if two expressions are mathematically equivalent using sympy.\n\n    Subtracts the two expressions and simplifies the result. If the\n    simplified difference equals zero, the expressions are equivalent.\n\n    Args:\n        ground_truth_normalized: The normalized ground truth expression.\n        given_normalized: The normalized given expression to check.\n\n    Returns:\n        True if the expressions are mathematically equivalent, False otherwise.\n    \"\"\"\n    are_equal = False\n    try:\n        expr = f\"({ground_truth_normalized})-({given_normalized})\"\n        if should_allow_eval(expr):\n            sympy_diff = _sympy_parse(expr)\n            simplified = sympy.simplify(sympy_diff)\n            if simplified == 0:\n                are_equal = True\n    except:\n        pass\n    return are_equal\n</code></pre>"},{"location":"utils/grading_utils/math/math_grader/#pita.utils.grading_utils.math.math_grader.count_unknown_letters_in_expr","title":"<code>count_unknown_letters_in_expr(expr: str) -&gt; int</code>","text":"<p>Count the number of unknown letters in an expression.</p> <p>Removes known mathematical function names (sqrt, frac) and counts the remaining alphabetic characters.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>A mathematical expression string.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of distinct unknown alphabetic characters in the expression.</p> Source code in <code>pita/utils/grading_utils/math/math_grader.py</code> <pre><code>def count_unknown_letters_in_expr(expr: str) -&gt; int:\n    \"\"\"\n    Count the number of unknown letters in an expression.\n\n    Removes known mathematical function names (sqrt, frac) and counts\n    the remaining alphabetic characters.\n\n    Args:\n        expr: A mathematical expression string.\n\n    Returns:\n        The number of distinct unknown alphabetic characters in the expression.\n    \"\"\"\n    expr = expr.replace(\"sqrt\", \"\")\n    expr = expr.replace(\"frac\", \"\")\n    letters_in_expr = set([x for x in expr if x.isalpha()])\n    return len(letters_in_expr)\n</code></pre>"},{"location":"utils/grading_utils/math/math_grader/#pita.utils.grading_utils.math.math_grader.grade_answer","title":"<code>grade_answer(given_answer: str, ground_truth: str) -&gt; bool</code>","text":"<p>Grade a given answer against the ground truth.</p> <p>The answer will be considered correct if: (a) it normalizes to the same string as the ground truth answer OR (b) sympy can simplify the difference between the expressions to 0</p> <p>Special handling for: - Tuples and intervals (must match structure and order) - Fractions (must be in reduced form) - Integers (must be exact matches, no decimal equivalents)</p> <p>Parameters:</p> Name Type Description Default <code>given_answer</code> <code>str</code> <p>The answer to grade.</p> required <code>ground_truth</code> <code>str</code> <p>The correct answer to compare against.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the answer is correct, False otherwise.</p> Source code in <code>pita/utils/grading_utils/math/math_grader.py</code> <pre><code>def grade_answer(given_answer: str, ground_truth: str) -&gt; bool:\n    \"\"\"\n    Grade a given answer against the ground truth.\n\n    The answer will be considered correct if:\n    (a) it normalizes to the same string as the ground truth answer\n    OR\n    (b) sympy can simplify the difference between the expressions to 0\n\n    Special handling for:\n    - Tuples and intervals (must match structure and order)\n    - Fractions (must be in reduced form)\n    - Integers (must be exact matches, no decimal equivalents)\n\n    Args:\n        given_answer: The answer to grade.\n        ground_truth: The correct answer to compare against.\n\n    Returns:\n        True if the answer is correct, False otherwise.\n    \"\"\"\n    if given_answer is None:\n        return False\n\n    ground_truth_normalized_mathd = math_normalize.normalize_answer(ground_truth)\n    given_answer_normalized_mathd = math_normalize.normalize_answer(given_answer)\n\n    # be at least as lenient as mathd\n    if ground_truth_normalized_mathd == given_answer_normalized_mathd:\n        return True\n\n    ground_truth_normalized = _normalize(ground_truth)\n    given_normalized = _normalize(given_answer)\n\n    if ground_truth_normalized is None:\n        return False\n\n    if ground_truth_normalized == given_normalized:\n        return True\n\n    if len(given_normalized) == 0:\n        return False\n\n    ground_truth_elems = split_tuple(ground_truth_normalized)\n    given_elems = split_tuple(given_normalized)\n\n    if len(ground_truth_elems) &gt; 1 and (\n        ground_truth_normalized[0] != given_normalized[0]\n        or ground_truth_normalized[-1] != given_normalized[-1]\n    ):\n        is_correct = False\n    elif len(ground_truth_elems) != len(given_elems):\n        is_correct = False\n    else:\n        for ground_truth_elem, given_elem in zip(ground_truth_elems, given_elems):\n            if _is_frac(ground_truth_elem) and _is_frac(given_elem):\n                # if fractions aren't reduced, then shouldn't be marked as correct\n                # so, we don't want to allow sympy.simplify in this case\n                is_correct = ground_truth_elem == given_elem\n            elif _str_is_int(ground_truth_elem) != _str_is_int(given_elem):\n                # if the ground truth answer is an integer, we require the given answer to be a strict match (no sympy.simplify)\n                is_correct = False\n            else:\n                is_correct = are_equal_under_sympy(ground_truth_elem, given_elem)\n            if not is_correct:\n                break\n\n    return is_correct\n</code></pre>"},{"location":"utils/grading_utils/math/math_grader/#pita.utils.grading_utils.math.math_grader.should_allow_eval","title":"<code>should_allow_eval(expr: str) -&gt; bool</code>","text":"<p>Determine if an expression is safe to evaluate with sympy.</p> <p>Checks for potentially problematic patterns that might cause sympy to hang or fail, including too many unknown variables and known bad patterns.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>A mathematical expression string.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the expression is safe to evaluate, False otherwise.</p> Source code in <code>pita/utils/grading_utils/math/math_grader.py</code> <pre><code>def should_allow_eval(expr: str) -&gt; bool:\n    \"\"\"\n    Determine if an expression is safe to evaluate with sympy.\n\n    Checks for potentially problematic patterns that might cause sympy to hang\n    or fail, including too many unknown variables and known bad patterns.\n\n    Args:\n        expr: A mathematical expression string.\n\n    Returns:\n        True if the expression is safe to evaluate, False otherwise.\n    \"\"\"\n    # we don't want to try parsing unknown text or functions of more than two variables\n    if count_unknown_letters_in_expr(expr) &gt; 2:\n        return False\n\n    for bad_string in BAD_SUBSTRINGS:\n        if bad_string in expr:\n            return False\n\n    for bad_regex in BAD_REGEXES:\n        if re.search(bad_regex, expr) is not None:\n            return False\n\n    return True\n</code></pre>"},{"location":"utils/grading_utils/math/math_grader/#pita.utils.grading_utils.math.math_grader.split_tuple","title":"<code>split_tuple(expr: str) -&gt; List[str]</code>","text":"<p>Split the elements in a tuple or interval.</p> <p>Handles well-formatted commas in large numbers while splitting tuple elements. Recognizes tuples by their bracketing characters.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>A string representing a tuple, interval, or single value.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of string elements. If the expression is a tuple/interval,</p> <code>List[str]</code> <p>returns the individual elements; otherwise returns a list containing</p> <code>List[str]</code> <p>the original expression.</p> Source code in <code>pita/utils/grading_utils/math/math_grader.py</code> <pre><code>def split_tuple(expr: str) -&gt; List[str]:\n    \"\"\"\n    Split the elements in a tuple or interval.\n\n    Handles well-formatted commas in large numbers while splitting tuple\n    elements. Recognizes tuples by their bracketing characters.\n\n    Args:\n        expr: A string representing a tuple, interval, or single value.\n\n    Returns:\n        A list of string elements. If the expression is a tuple/interval,\n        returns the individual elements; otherwise returns a list containing\n        the original expression.\n    \"\"\"\n    expr = _strip_properly_formatted_commas(expr)\n    if len(expr) == 0:\n        return []\n    if (\n        len(expr) &gt; 2\n        and expr[0] in TUPLE_CHARS\n        and expr[-1] in TUPLE_CHARS\n        and all([ch not in expr[1:-1] for ch in TUPLE_CHARS])\n    ):\n        elems = [elem.strip() for elem in expr[1:-1].split(\",\")]\n    else:\n        elems = [expr]\n    return elems\n</code></pre>"},{"location":"utils/grading_utils/math/math_normalize/","title":"Math Normalize","text":"<p>This logic is largely copied from the Hendrycks' MATH release (math_equivalence).</p>"},{"location":"utils/grading_utils/math/parse_utils/","title":"Parse Utils","text":""},{"location":"utils/grading_utils/math/parse_utils/#pita.utils.grading_utils.math.parse_utils.last_boxed_only","title":"<code>last_boxed_only(sample: Tuple[str, str]) -&gt; Optional[Tuple[str, str]]</code>","text":"<p>Extract only the last boxed answer from a question-answer sample.</p> <p>Given a (q,a) sample, filter the answers so that they only contain the last \\boxed{...} or \\fbox{...} element.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>Tuple[str, str]</code> <p>A tuple containing (question, answer) strings.</p> required <p>Returns:</p> Type Description <code>Optional[Tuple[str, str]]</code> <p>A tuple containing (question, last_boxed_answer), or None if no</p> <code>Optional[Tuple[str, str]]</code> <p>boxed element is found in the answer.</p> Source code in <code>pita/utils/grading_utils/math/parse_utils.py</code> <pre><code>def last_boxed_only(sample: Tuple[str, str]) -&gt; Optional[Tuple[str, str]]:\n    \"\"\"\n    Extract only the last boxed answer from a question-answer sample.\n\n    Given a (q,a) sample, filter the answers so that they only contain\n    the last \\\\boxed{...} or \\\\fbox{...} element.\n\n    Args:\n        sample: A tuple containing (question, answer) strings.\n\n    Returns:\n        A tuple containing (question, last_boxed_answer), or None if no\n        boxed element is found in the answer.\n    \"\"\"\n    q, a = sample\n    a = last_boxed_only_string(a)\n    if a == None:\n        return None\n    return (q, a)\n</code></pre>"},{"location":"utils/grading_utils/math/parse_utils/#pita.utils.grading_utils.math.parse_utils.last_boxed_only_string","title":"<code>last_boxed_only_string(string: str) -&gt; Optional[str]</code>","text":"<p>Extract the last boxed answer from a string.</p> <p>Finds the last occurrence of \\boxed{...} or \\fbox{...} in the string and extracts the complete boxed expression, properly handling nested braces.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>A string potentially containing LaTeX boxed commands.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The last complete boxed expression (including the \\boxed{} wrapper),</p> <code>Optional[str]</code> <p>or None if no boxed element is found or if braces are unbalanced.</p> Source code in <code>pita/utils/grading_utils/math/parse_utils.py</code> <pre><code>def last_boxed_only_string(string: str) -&gt; Optional[str]:\n    \"\"\"\n    Extract the last boxed answer from a string.\n\n    Finds the last occurrence of \\\\boxed{...} or \\\\fbox{...} in the string\n    and extracts the complete boxed expression, properly handling nested braces.\n\n    Args:\n        string: A string potentially containing LaTeX boxed commands.\n\n    Returns:\n        The last complete boxed expression (including the \\\\boxed{} wrapper),\n        or None if no boxed element is found or if braces are unbalanced.\n    \"\"\"\n    idx = string.rfind(\"\\\\boxed\")\n    if idx &lt; 0:\n        idx = string.rfind(\"\\\\fbox\")\n        if idx &lt; 0:\n            return None\n\n    i = idx\n    right_brace_idx = None\n    num_left_braces_open = 0\n    while i &lt; len(string):\n        if string[i] == \"{\":\n            num_left_braces_open += 1\n        if string[i] == \"}\":\n            num_left_braces_open -= 1\n            if num_left_braces_open == 0:\n                right_brace_idx = i\n                break\n        i += 1\n\n    if right_brace_idx == None:\n        retval = None\n    else:\n        retval = string[idx:right_brace_idx + 1]\n\n    return retval\n</code></pre>"},{"location":"utils/grading_utils/math/parse_utils/#pita.utils.grading_utils.math.parse_utils.parse_answer","title":"<code>parse_answer(input_str: str) -&gt; Optional[str]</code>","text":"<p>Parse the final answer from a LaTeX string.</p> <p>Extracts the content of the last \\boxed{...} or \\fbox{...} command in the input string, removing the wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>input_str</code> <code>str</code> <p>A string containing LaTeX formatted mathematical content, potentially with multiple boxed expressions.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The content of the last boxed expression without the wrapper,</p> <code>Optional[str]</code> <p>or None if no boxed element is found.</p> Source code in <code>pita/utils/grading_utils/math/parse_utils.py</code> <pre><code>def parse_answer(input_str: str) -&gt; Optional[str]:\n    \"\"\"\n    Parse the final answer from a LaTeX string.\n\n    Extracts the content of the last \\\\boxed{...} or \\\\fbox{...} command\n    in the input string, removing the wrapper.\n\n    Args:\n        input_str: A string containing LaTeX formatted mathematical content,\n            potentially with multiple boxed expressions.\n\n    Returns:\n        The content of the last boxed expression without the wrapper,\n        or None if no boxed element is found.\n    \"\"\"\n    return remove_boxed(last_boxed_only_string(input_str))\n</code></pre>"},{"location":"utils/grading_utils/math/parse_utils/#pita.utils.grading_utils.math.parse_utils.remove_boxed","title":"<code>remove_boxed(s: str) -&gt; Optional[str]</code>","text":"<p>Remove the LaTeX boxed wrapper from a string.</p> <p>Extracts the content from within a \\boxed{...} LaTeX command.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>A string containing a LaTeX \\boxed{} command.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The content within the boxed command, or None if the string</p> <code>Optional[str]</code> <p>doesn't match the expected format.</p> Source code in <code>pita/utils/grading_utils/math/parse_utils.py</code> <pre><code>def remove_boxed(s: str) -&gt; Optional[str]:\n    \"\"\"\n    Remove the LaTeX boxed wrapper from a string.\n\n    Extracts the content from within a \\\\boxed{...} LaTeX command.\n\n    Args:\n        s: A string containing a LaTeX \\\\boxed{} command.\n\n    Returns:\n        The content within the boxed command, or None if the string\n        doesn't match the expected format.\n    \"\"\"\n    left = \"\\\\boxed{\"\n    try:\n        assert s[:len(left)] == left\n        assert s[-1] == \"}\"\n        return s[len(left):-1]\n    except:\n        return None\n</code></pre>"}]}