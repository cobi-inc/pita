loading file vocab.json from cache at /home/wilgmoy/.cache/huggingface/hub/models--Qwen--Qwen3-30B-A3B-GPTQ-Int4/snapshots/9b534e4318b7ebc3c961a839f13eb18b1833f441/vocab.json
loading file merges.txt from cache at /home/wilgmoy/.cache/huggingface/hub/models--Qwen--Qwen3-30B-A3B-GPTQ-Int4/snapshots/9b534e4318b7ebc3c961a839f13eb18b1833f441/merges.txt
loading file tokenizer.json from cache at /home/wilgmoy/.cache/huggingface/hub/models--Qwen--Qwen3-30B-A3B-GPTQ-Int4/snapshots/9b534e4318b7ebc3c961a839f13eb18b1833f441/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /home/wilgmoy/.cache/huggingface/hub/models--Qwen--Qwen3-30B-A3B-GPTQ-Int4/snapshots/9b534e4318b7ebc3c961a839f13eb18b1833f441/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading configuration file config.json from cache at /home/wilgmoy/.cache/huggingface/hub/models--Qwen--Qwen3-30B-A3B-GPTQ-Int4/snapshots/9b534e4318b7ebc3c961a839f13eb18b1833f441/config.json
Model config Qwen3MoeConfig {
  "architectures": [
    "Qwen3MoeForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "decoder_sparse_step": 1,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 6144,
  "max_position_embeddings": 40960,
  "max_window_layers": 48,
  "mlp_only_layers": [],
  "model_type": "qwen3_moe",
  "moe_intermediate_size": 768,
  "norm_topk_prob": true,
  "num_attention_heads": 32,
  "num_experts": 128,
  "num_experts_per_tok": 8,
  "num_hidden_layers": 48,
  "num_key_value_heads": 4,
  "output_router_logits": false,
  "quantization_config": {
    "bits": 4,
    "checkpoint_format": "gptq",
    "damp_percent": 0.01,
    "desc_act": false,
    "group_size": 128,
    "model_file_base_name": null,
    "model_name_or_path": null,
    "quant_method": "gptq",
    "static_groups": false,
    "sym": true,
    "true_sequential": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "router_aux_loss_coef": 0.001,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

You have activated exllama backend. Note that you can get better inference speed using exllamav2 kernel by setting `exllama_config`.

qwen3_32
cuda
20
dataset done
Loading tokenizer from Qwen/Qwen3-30B-A3B-GPTQ-Int4...
Loading model from Qwen/Qwen3-30B-A3B-GPTQ-Int4...
Note: GPTQ MoE models with 128 experts take 10-20 minutes to initialize.
The process will appear stalled but is working. GPU memory will increase once complete.


[33mWARN[0m  Python >= 3.13T (free-threading) version detected but GIL is not disabled due to manual override or `regex` package compatibility which can be ignored. Please disable GIL via env `PYTHON_GIL=0`.

[33mWARN[0m  Python GIL is enabled: Multi-gpu quant acceleration for MoE models is sub-optimal and multi-core accelerated cpu packing is also disabled. We recommend Python >= 3.13.3t with Pytorch > 2.8 for mult-gpu quantization and multi-cpu packing with env `PYTHON_GIL=0`.

[33mWARN[0m  Feature `utils/Perplexity` requires python GIL or Python >= 3.13.3T (T for Threading-Free edition of Python) plus Torch 2.8. Feature is currently skipped/disabled.

[32mINFO[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.

[32mINFO[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.          
/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  self.setter(val)
Loading the model in `torch.float16`. To overwrite it, set `dtype` manually.
loading weights file model.safetensors from cache at /home/wilgmoy/.cache/huggingface/hub/models--Qwen--Qwen3-30B-A3B-GPTQ-Int4/snapshots/9b534e4318b7ebc3c961a839f13eb18b1833f441/model.safetensors
Instantiating Qwen3MoeForCausalLM model under default dtype torch.float16.
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}


[32mINFO[0m   Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`          
`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Some weights of the model checkpoint at Qwen/Qwen3-30B-A3B-GPTQ-Int4 were not used when initializing Qwen3MoeForCausalLM: ['model.layers.0.mlp.gate.weight', 'model.layers.1.mlp.gate.weight', 'model.layers.10.mlp.gate.weight', 'model.layers.11.mlp.gate.weight', 'model.layers.12.mlp.gate.weight', 'model.layers.13.mlp.gate.weight', 'model.layers.14.mlp.gate.weight', 'model.layers.15.mlp.gate.weight', 'model.layers.16.mlp.gate.weight', 'model.layers.17.mlp.gate.weight', 'model.layers.18.mlp.gate.weight', 'model.layers.19.mlp.gate.weight', 'model.layers.2.mlp.gate.weight', 'model.layers.20.mlp.gate.weight', 'model.layers.21.mlp.gate.weight', 'model.layers.22.mlp.gate.weight', 'model.layers.23.mlp.gate.weight', 'model.layers.24.mlp.gate.weight', 'model.layers.25.mlp.gate.weight', 'model.layers.26.mlp.gate.weight', 'model.layers.27.mlp.gate.weight', 'model.layers.28.mlp.gate.weight', 'model.layers.29.mlp.gate.weight', 'model.layers.3.mlp.gate.weight', 'model.layers.30.mlp.gate.weight', 'model.layers.31.mlp.gate.weight', 'model.layers.32.mlp.gate.weight', 'model.layers.33.mlp.gate.weight', 'model.layers.34.mlp.gate.weight', 'model.layers.35.mlp.gate.weight', 'model.layers.36.mlp.gate.weight', 'model.layers.37.mlp.gate.weight', 'model.layers.38.mlp.gate.weight', 'model.layers.39.mlp.gate.weight', 'model.layers.4.mlp.gate.weight', 'model.layers.40.mlp.gate.weight', 'model.layers.41.mlp.gate.weight', 'model.layers.42.mlp.gate.weight', 'model.layers.43.mlp.gate.weight', 'model.layers.44.mlp.gate.weight', 'model.layers.45.mlp.gate.weight', 'model.layers.46.mlp.gate.weight', 'model.layers.47.mlp.gate.weight', 'model.layers.5.mlp.gate.weight', 'model.layers.6.mlp.gate.weight', 'model.layers.7.mlp.gate.weight', 'model.layers.8.mlp.gate.weight', 'model.layers.9.mlp.gate.weight']
- This IS expected if you are initializing Qwen3MoeForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Qwen3MoeForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Qwen3MoeForCausalLM were not initialized from the model checkpoint at Qwen/Qwen3-30B-A3B-GPTQ-Int4 and are newly initialized: ['model.layers.0.mlp.gate.g_idx', 'model.layers.0.mlp.gate.qweight', 'model.layers.0.mlp.gate.qzeros', 'model.layers.0.mlp.gate.scales', 'model.layers.1.mlp.gate.g_idx', 'model.layers.1.mlp.gate.qweight', 'model.layers.1.mlp.gate.qzeros', 'model.layers.1.mlp.gate.scales', 'model.layers.10.mlp.gate.g_idx', 'model.layers.10.mlp.gate.qweight', 'model.layers.10.mlp.gate.qzeros', 'model.layers.10.mlp.gate.scales', 'model.layers.11.mlp.gate.g_idx', 'model.layers.11.mlp.gate.qweight', 'model.layers.11.mlp.gate.qzeros', 'model.layers.11.mlp.gate.scales', 'model.layers.12.mlp.gate.g_idx', 'model.layers.12.mlp.gate.qweight', 'model.layers.12.mlp.gate.qzeros', 'model.layers.12.mlp.gate.scales', 'model.layers.13.mlp.gate.g_idx', 'model.layers.13.mlp.gate.qweight', 'model.layers.13.mlp.gate.qzeros', 'model.layers.13.mlp.gate.scales', 'model.layers.14.mlp.gate.g_idx', 'model.layers.14.mlp.gate.qweight', 'model.layers.14.mlp.gate.qzeros', 'model.layers.14.mlp.gate.scales', 'model.layers.15.mlp.gate.g_idx', 'model.layers.15.mlp.gate.qweight', 'model.layers.15.mlp.gate.qzeros', 'model.layers.15.mlp.gate.scales', 'model.layers.16.mlp.gate.g_idx', 'model.layers.16.mlp.gate.qweight', 'model.layers.16.mlp.gate.qzeros', 'model.layers.16.mlp.gate.scales', 'model.layers.17.mlp.gate.g_idx', 'model.layers.17.mlp.gate.qweight', 'model.layers.17.mlp.gate.qzeros', 'model.layers.17.mlp.gate.scales', 'model.layers.18.mlp.gate.g_idx', 'model.layers.18.mlp.gate.qweight', 'model.layers.18.mlp.gate.qzeros', 'model.layers.18.mlp.gate.scales', 'model.layers.19.mlp.gate.g_idx', 'model.layers.19.mlp.gate.qweight', 'model.layers.19.mlp.gate.qzeros', 'model.layers.19.mlp.gate.scales', 'model.layers.2.mlp.gate.g_idx', 'model.layers.2.mlp.gate.qweight', 'model.layers.2.mlp.gate.qzeros', 'model.layers.2.mlp.gate.scales', 'model.layers.20.mlp.gate.g_idx', 'model.layers.20.mlp.gate.qweight', 'model.layers.20.mlp.gate.qzeros', 'model.layers.20.mlp.gate.scales', 'model.layers.21.mlp.gate.g_idx', 'model.layers.21.mlp.gate.qweight', 'model.layers.21.mlp.gate.qzeros', 'model.layers.21.mlp.gate.scales', 'model.layers.22.mlp.gate.g_idx', 'model.layers.22.mlp.gate.qweight', 'model.layers.22.mlp.gate.qzeros', 'model.layers.22.mlp.gate.scales', 'model.layers.23.mlp.gate.g_idx', 'model.layers.23.mlp.gate.qweight', 'model.layers.23.mlp.gate.qzeros', 'model.layers.23.mlp.gate.scales', 'model.layers.24.mlp.gate.g_idx', 'model.layers.24.mlp.gate.qweight', 'model.layers.24.mlp.gate.qzeros', 'model.layers.24.mlp.gate.scales', 'model.layers.25.mlp.gate.g_idx', 'model.layers.25.mlp.gate.qweight', 'model.layers.25.mlp.gate.qzeros', 'model.layers.25.mlp.gate.scales', 'model.layers.26.mlp.gate.g_idx', 'model.layers.26.mlp.gate.qweight', 'model.layers.26.mlp.gate.qzeros', 'model.layers.26.mlp.gate.scales', 'model.layers.27.mlp.gate.g_idx', 'model.layers.27.mlp.gate.qweight', 'model.layers.27.mlp.gate.qzeros', 'model.layers.27.mlp.gate.scales', 'model.layers.28.mlp.gate.g_idx', 'model.layers.28.mlp.gate.qweight', 'model.layers.28.mlp.gate.qzeros', 'model.layers.28.mlp.gate.scales', 'model.layers.29.mlp.gate.g_idx', 'model.layers.29.mlp.gate.qweight', 'model.layers.29.mlp.gate.qzeros', 'model.layers.29.mlp.gate.scales', 'model.layers.3.mlp.gate.g_idx', 'model.layers.3.mlp.gate.qweight', 'model.layers.3.mlp.gate.qzeros', 'model.layers.3.mlp.gate.scales', 'model.layers.30.mlp.gate.g_idx', 'model.layers.30.mlp.gate.qweight', 'model.layers.30.mlp.gate.qzeros', 'model.layers.30.mlp.gate.scales', 'model.layers.31.mlp.gate.g_idx', 'model.layers.31.mlp.gate.qweight', 'model.layers.31.mlp.gate.qzeros', 'model.layers.31.mlp.gate.scales', 'model.layers.32.mlp.gate.g_idx', 'model.layers.32.mlp.gate.qweight', 'model.layers.32.mlp.gate.qzeros', 'model.layers.32.mlp.gate.scales', 'model.layers.33.mlp.gate.g_idx', 'model.layers.33.mlp.gate.qweight', 'model.layers.33.mlp.gate.qzeros', 'model.layers.33.mlp.gate.scales', 'model.layers.34.mlp.gate.g_idx', 'model.layers.34.mlp.gate.qweight', 'model.layers.34.mlp.gate.qzeros', 'model.layers.34.mlp.gate.scales', 'model.layers.35.mlp.gate.g_idx', 'model.layers.35.mlp.gate.qweight', 'model.layers.35.mlp.gate.qzeros', 'model.layers.35.mlp.gate.scales', 'model.layers.36.mlp.gate.g_idx', 'model.layers.36.mlp.gate.qweight', 'model.layers.36.mlp.gate.qzeros', 'model.layers.36.mlp.gate.scales', 'model.layers.37.mlp.gate.g_idx', 'model.layers.37.mlp.gate.qweight', 'model.layers.37.mlp.gate.qzeros', 'model.layers.37.mlp.gate.scales', 'model.layers.38.mlp.gate.g_idx', 'model.layers.38.mlp.gate.qweight', 'model.layers.38.mlp.gate.qzeros', 'model.layers.38.mlp.gate.scales', 'model.layers.39.mlp.gate.g_idx', 'model.layers.39.mlp.gate.qweight', 'model.layers.39.mlp.gate.qzeros', 'model.layers.39.mlp.gate.scales', 'model.layers.4.mlp.gate.g_idx', 'model.layers.4.mlp.gate.qweight', 'model.layers.4.mlp.gate.qzeros', 'model.layers.4.mlp.gate.scales', 'model.layers.40.mlp.gate.g_idx', 'model.layers.40.mlp.gate.qweight', 'model.layers.40.mlp.gate.qzeros', 'model.layers.40.mlp.gate.scales', 'model.layers.41.mlp.gate.g_idx', 'model.layers.41.mlp.gate.qweight', 'model.layers.41.mlp.gate.qzeros', 'model.layers.41.mlp.gate.scales', 'model.layers.42.mlp.gate.g_idx', 'model.layers.42.mlp.gate.qweight', 'model.layers.42.mlp.gate.qzeros', 'model.layers.42.mlp.gate.scales', 'model.layers.43.mlp.gate.g_idx', 'model.layers.43.mlp.gate.qweight', 'model.layers.43.mlp.gate.qzeros', 'model.layers.43.mlp.gate.scales', 'model.layers.44.mlp.gate.g_idx', 'model.layers.44.mlp.gate.qweight', 'model.layers.44.mlp.gate.qzeros', 'model.layers.44.mlp.gate.scales', 'model.layers.45.mlp.gate.g_idx', 'model.layers.45.mlp.gate.qweight', 'model.layers.45.mlp.gate.qzeros', 'model.layers.45.mlp.gate.scales', 'model.layers.46.mlp.gate.g_idx', 'model.layers.46.mlp.gate.qweight', 'model.layers.46.mlp.gate.qzeros', 'model.layers.46.mlp.gate.scales', 'model.layers.47.mlp.gate.g_idx', 'model.layers.47.mlp.gate.qweight', 'model.layers.47.mlp.gate.qzeros', 'model.layers.47.mlp.gate.scales', 'model.layers.5.mlp.gate.g_idx', 'model.layers.5.mlp.gate.qweight', 'model.layers.5.mlp.gate.qzeros', 'model.layers.5.mlp.gate.scales', 'model.layers.6.mlp.gate.g_idx', 'model.layers.6.mlp.gate.qweight', 'model.layers.6.mlp.gate.qzeros', 'model.layers.6.mlp.gate.scales', 'model.layers.7.mlp.gate.g_idx', 'model.layers.7.mlp.gate.qweight', 'model.layers.7.mlp.gate.qzeros', 'model.layers.7.mlp.gate.scales', 'model.layers.8.mlp.gate.g_idx', 'model.layers.8.mlp.gate.qweight', 'model.layers.8.mlp.gate.qzeros', 'model.layers.8.mlp.gate.scales', 'model.layers.9.mlp.gate.g_idx', 'model.layers.9.mlp.gate.qweight', 'model.layers.9.mlp.gate.qzeros', 'model.layers.9.mlp.gate.scales']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file generation_config.json from cache at /home/wilgmoy/.cache/huggingface/hub/models--Qwen--Qwen3-30B-A3B-GPTQ-Int4/snapshots/9b534e4318b7ebc3c961a839f13eb18b1833f441/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.6,
  "top_k": 20,
  "top_p": 0.95
}

Could not locate the custom_generate/generate.py inside Qwen/Qwen3-30B-A3B-GPTQ-Int4.
[32mINFO[0m  Format: Converting `checkpoint_format` from `FORMAT.GPTQ` to internal `FORMAT.GPTQ_V2`.
[32mINFO[0m  Format: Converting GPTQ v1 to v2                                         
[32mINFO[0m  Format: Conversion complete: 0.12637805938720703s                        
[32mINFO[0m  Optimize: `TritonV2QuantLinear` compilation triggered.                   
loaded models
Benchmark on MATH: 0it [00:00, ?it/s]Benchmark on MATH: 2it [00:01,  1.25it/s]
Can you solve the following math problem? If $f(x) = \frac{3x-2}{x-2}$, what is the value of $f(-2) +f(-1)+f(0)$? Express your answer as a common fraction. Please reason step by step, and put your final answer within \boxed{{}}.
\frac{14}{3}
Traceback (most recent call last):
  File "/home/wilgmoy/training-free-reasoning/./llm_experiments/power_samp.py", line 483, in <module>
    naive_temp_output = hf_model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=3072, return_dict_in_generate=True, output_scores=True, temperature = temp)
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/transformers/generation/utils.py", line 2564, in generate
    result = decoding_method(
        self,
    ...<5 lines>...
        **model_kwargs,
    )
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/transformers/generation/utils.py", line 2784, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/transformers/utils/generic.py", line 918, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/transformers/models/qwen3_moe/modeling_qwen3_moe.py", line 650, in forward
    outputs: MoeModelOutputWithPast = self.model(
                                      ~~~~~~~~~~^
        input_ids=input_ids,
        ^^^^^^^^^^^^^^^^^^^^
    ...<7 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/transformers/utils/generic.py", line 1064, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/transformers/models/qwen3_moe/modeling_qwen3_moe.py", line 487, in forward
    hidden_states = decoder_layer(
        hidden_states,
    ...<6 lines>...
        **kwargs,
    )
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/transformers/models/qwen3_moe/modeling_qwen3_moe.py", line 359, in forward
    hidden_states = self.mlp(hidden_states)
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/transformers/models/qwen3_moe/modeling_qwen3_moe.py", line 231, in forward
    router_logits = self.gate(hidden_states)
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/gptqmodel/nn_modules/qlinear/tritonv2.py", line 155, in forward
    out = QuantLinearFunction.apply(
          ~~~~~~~~~~~~~~~~~~~~~~~~~^
        x.reshape(-1, x.shape[-1]),
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        self.maxq,
        ^^^^^^^^^^
    ).reshape(out_shape)
    ^
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/torch/autograd/function.py", line 581, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/torch/amp/autocast_mode.py", line 527, in decorate_fwd
    return fwd(*args, **kwargs)
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/gptqmodel/nn_modules/triton_utils/dequant.py", line 180, in forward
    output = quant_matmul(input, qweight, scales, qzeros, g_idx, bits, pack_bits, maxq)
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/gptqmodel/nn_modules/triton_utils/dequant.py", line 171, in quant_matmul
    W = dequant(input.dtype, qweight, scales, qzeros, g_idx, bits, pack_bits, maxq)
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/gptqmodel/nn_modules/triton_utils/dequant.py", line 153, in dequant
    dequant_kernel[grid](
    ~~~~~~~~~~~~~~~~~~~~^
        g_idx,
        ^^^^^^
    ...<10 lines>...
        num_groups=num_groups,
        ^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/triton/runtime/jit.py", line 419, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/triton/runtime/autotuner.py", line 250, in run
    ret = self.fn.run(
        *args,
        **kwargs,
        **config.all_kwargs(),
    )
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/triton/runtime/jit.py", line 733, in run
    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/triton/runtime/jit.py", line 861, in _do_compile
    kernel = self.compile(src, target=target, options=options.__dict__)
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/triton/compiler/compiler.py", line 300, in compile
    module = src.make_ir(target, options, codegen_fns, module_map, context)
  File "/home/wilgmoy/miniconda3/envs/power_sampling_pytorch/lib/python3.13/site-packages/triton/compiler/compiler.py", line 80, in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
                       module_map=module_map)
triton.compiler.errors.CompilationError: at 32:16:
    col_idx = x_index % out_features

    pack_scale: tl.constexpr = pack_bits // bits
    qzero_ncols: tl.constexpr = out_features // pack_scale

    # Load group indices
    g_idx = tl.load(g_idx_ptr + row_idx, mask=xmask, eviction_policy="evict_last")
    groups = tl.where(g_idx < 0, g_idx + num_groups, g_idx)

    # Load scales
    scales = tl.cast(
        tl.load(scales_ptr + (col_idx + out_features * groups), mask=xmask, eviction_policy="evict_last"),
                ^
IncompatibleTypeErrorImpl('invalid operands of type pointer<fp16> and triton.language.float16')
[W1017 00:12:26.277150684 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
