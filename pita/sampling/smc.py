# Math Libraries
import numpy as np

# Custom Libraries
from pita.inference.LLM_backend import AutoregressiveSampler
from pita.sampling.token_metrics import process_top_k_probs, low_temp_logprobs, power_sampling_logprobs

# SMC Class
class Sequential_Monte_Carlo:
    def __init__(
        self, 
        num_particles: int = 10, 
        tokens_per_step: int = 5, 
        stop_on_eos: bool = True,
        token_metric: str = "logprobs",
        aggregation: str = "last"
    ):
        self.num_particles = num_particles
        self.tokens_per_step = tokens_per_step
        self.stop_on_eos = stop_on_eos
        self.token_metric = token_metric
        self.aggregation = aggregation

    # TODO implement the sample function
    def sample(
        self,
        sampler: AutoregressiveSampler,
        prompt: str
    ) -> str:
        """
        Samples using SMC and its parameters.

        Args:
            sampler (AutoregressiveSampler): The sampler object.
            prompt (str): The prompt to sample from.
        Returns:
            str: Response to the prompt from the SMC sampling.
        """
        pass

# TODO Remove this function as it will be incorperated into the LLM_backend.py
# Enable SMC Sampling Function
# Take in the default parameters for SMC sampling and set them in the sampler object
def enable_smc_sampling(
    sampler: AutoregressiveSampler, 
    num_particles: int = 10, 
    tokens_per_step: int = 5, 
    stop_on_eos: bool = True,
    token_metric: str = "logprobs",
    aggregation: str = "last"
) -> None:
    # Check if the sampler is initialized
    if(sampler is None):
        raise ValueError("Sampler must be initialized before enabling SMC sampling.")
    
    sampler.smc_sampling_params = Sequential_Monte_Carlo_Params(
        num_particles=num_particles,
        tokens_per_step=tokens_per_step,    
        stop_on_eos=stop_on_eos
    )

# TODO Determine if this function is really needed anymore
# Use a standard temperature affected log probability metric to perform SMC sampling
# Input Variables
# sampler: The sampler object
# tokens: The list of tokens generated by the sampler
# top_k_logits: The list of logits for the top k tokens
# top_k_logprobs: The list of log probabilities for the top k tokens
# unprocessed_normalization_constant: The unprocessed normalization constant from the sample() function
# temp_processed_normalization_constant: The temperature processed normalization constant sample() function
# Returns: The list of log probabilities for each token in the generated sequence
def standard_log_probability_metric(
    sampler: AutoregressiveSampler,
    tokens: list[int], 
    top_k_logits: list[float], 
    top_k_logprobs: list[float], 
    unprocessed_normalization_constant: list[float], 
    temp_processed_normalization_constant: list[float]
) -> list[float]:
    # Want to return a list of log probabilities for each token in the generated sequence
    # Check if the top_k_logprobs is None
    if(top_k_logprobs is None and top_k_logits is not None):
        logits_list = process_top_k_probs(top_k_logits, tokens)
        probability_list = low_temp_logprobs(logits_list, temp_processed_normalization_constant, sampler.sampling_params.temperature)
    elif(top_k_logprobs is not None):
        probability_list = process_top_k_probs(top_k_logprobs, tokens)
    else:
        raise ValueError("top_k_logprobs and top_k_logits must both not be None.")
    
    return probability_list

# TODO Determine if this function is really needed anymore
# Use the power sampling log probability metric to perform the SMC sampling
# Input Variables
# sampler: The sampler object
# tokens: The list of tokens generated by the sampler
# top_k_logits: The list of logits for the top k tokens
# top_k_logprobs: The list of log probabilities for the top k tokens
# unprocessed_normalization_constant: The unprocessed normalization constant from the sample() function
# temp_processed_normalization_constant: The temperature processed normalization constant sample() function
# Returns: The list of log probabilities for each token in the generated sequence
def power_sampling_logprobability_metric(
    sampler: AutoregressiveSampler,
    tokens: list[int], 
    top_k_logits: list[float], 
    top_k_logprobs: list[float], 
    unprocessed_normalization_constant: list[float], 
    temp_processed_normalization_constant: list[float],
) -> list[float]:
    # Check if you have the logits
    if(top_k_logits is not None):
        # Find the power sampling log probabilities from the logits and unprocessed normalization constant
        logits_list = process_top_k_probs(top_k_logits, tokens)
        probability_list = power_sampling_logprobs(logits_list, unprocessed_normalization_constant, sampler.sampling_params.temperature)
    else: 
        raise ValueError("top_k_logits must be provided if the Power Sampled Log Probability Metric is used.")
    
    return probability_list

# Use negative entropy as a comparison metric for the SMC sampling

# TODO incorperate this function into the Sequential_Monte_Carlo class
# Perform SMC Sampling Function based on the log probabilites of the tokens generated
def sequential_monte_carlo(
    sampler: AutoregressiveSampler, 
    prompt: str
) -> str:

        # Initialize particles with the prompt 
        particles = [prompt for _ in range(sampler.smc_sampling_params.num_particles)]

        # cumulative log weights for each particle
        cum_logit_weights = np.zeros(sampler.smc_sampling_params.num_particles, dtype=float)

        # finished flags per particle
        finished = np.zeros(sampler.smc_sampling_params.num_particles, dtype=bool)
        eos_id = getattr(sampler.tokenizer, "eos_token_id", None)

        total_tokens_generated = 0

        while total_tokens_generated < sampler.sampling_params.max_tokens:
            # determine how many tokens to generate in this step
            max_new_tokens = min(sampler.smc_sampling_params.tokens_per_step, sampler.sampling_params.max_tokens - total_tokens_generated)
            print("Max New Tokens this step:", max_new_tokens)
            for i in range(sampler.smc_sampling_params.num_particles):
                if finished[i]:
                    continue # skip sampling for finished particles 
                context = particles[i]
                tokens, top_k_logits, top_k_logprobs, unprocessed_normalization_constant, temp_processed_normalization_constant = sampler.sample(context, max_new_tokens) # get tokens and their logprobs

                # tokens expected as list of token ids
                # If EOS detection enabled, truncate and mark finished
                if sampler.smc_sampling_params.stop_on_eos and eos_id is not None:
                    if eos_id in tokens:
                        idx = tokens.index(eos_id) + 1
                        tokens = tokens[:idx]
                        finished[i] = True

                # append generated tokens 
                tokens = sampler.tokenizer.decode(tokens)
                particles[i] = context + tokens 

                # calculate the comparison metric
                # for each particle generated block find cumulative logit sum
                block_logits = float(np.sum(token_logits)) if len(token_logits) > 0 else 0.0
                cum_logit_weights[i] += block_logits # accumulate from previous block generation 
            
            # Normalize cumulative logit weights to prevent numerical issues
            # TO DO: Do we need to normalize over token count to prevent bias towards longer sequences?
            max_cum_logit_weight = np.max(cum_logit_weights)
            weight = np.exp(cum_logit_weights - max_cum_logit_weight)
            normalized_weights = weight / np.sum(weight)

            # Resample indices according to normalized weights (multinomial resampling)
            # Indices creates an 1 x num_particles array of selected particle indices
            indices = np.random.choice(
                sampler.smc_sampling_params.num_particles, 
                size=sampler.smc_sampling_params.num_particles, 
                p=normalized_weights
            )
            # Set the new particle array according to resampled indices
            particles = [particles[idx] for idx in indices]
            # Assign the cum_logit_weights to the corresponding cum_log_weights of the resampled particles
            cum_logit_weights = cum_logit_weights[indices]
            # Update finished flags
            finished = finished[indices]

            # if all particles finished, we can stop early
            if finished.all():
                break

            total_tokens_generated += max_new_tokens

        # Greedy select the best particle by cumulative logit weight and return decoded text
        # Find the index of the particle with the highest cumulative logit weight
        best_idx = int(np.argmax(cum_logit_weights))

        # Decode the best particle token IDs to text and return it to the user
        best_particle_response = particles[best_idx]

        return best_particle_response